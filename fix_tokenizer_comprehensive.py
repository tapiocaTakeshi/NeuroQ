#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NeuroQ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å•é¡Œã®åŒ…æ‹¬çš„ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
=================================================

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™:
1. sentencepieceã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
2. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
3. vocab_sizeã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
4. neuroquantum_layered.pyã®ä¿®æ­£ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½œæˆ
5. handler.pyã®ä¿®æ­£ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½œæˆ
"""

import os
import sys
import subprocess

def check_and_install_sentencepiece():
    """sentencepieceã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print("=" * 70)
    print("ğŸ“¦ Step 1: sentencepieceã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª")
    print("=" * 70)

    try:
        import sentencepiece as spm
        print("âœ… sentencepieceã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™")
        return True
    except ImportError:
        print("âŒ sentencepieceãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’é–‹å§‹ã—ã¾ã™...")

        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "sentencepiece"])
            print("âœ… sentencepieceã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸ")
            return True
        except Exception as e:
            print(f"âŒ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False

def verify_tokenizer_files():
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼"""
    print("\n" + "=" * 70)
    print("ğŸ” Step 2: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼")
    print("=" * 70)

    try:
        import sentencepiece as spm
    except ImportError:
        print("âŒ sentencepieceãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“")
        return None

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    tokenizer_paths = [
        "/home/user/NeuroQ/neuroq-runpod/neuroq_tokenizer.model",
        "/home/user/NeuroQ/neuroq_tokenizer_8k.model",
        "/home/user/NeuroQ/neuroq_tokenizer.model",
    ]

    results = {}
    for path in tokenizer_paths:
        if os.path.exists(path):
            try:
                sp = spm.SentencePieceProcessor()
                sp.load(path)
                vocab_size = sp.get_piece_size()
                results[path] = {
                    "exists": True,
                    "vocab_size": vocab_size,
                    "valid": True
                }
                print(f"âœ… {path}")
                print(f"   èªå½™ã‚µã‚¤ã‚º: {vocab_size:,}")
            except Exception as e:
                results[path] = {
                    "exists": True,
                    "vocab_size": None,
                    "valid": False,
                    "error": str(e)
                }
                print(f"âŒ {path}")
                print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            results[path] = {
                "exists": False,
                "vocab_size": None,
                "valid": False
            }
            print(f"âŒ {path} (ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“)")

    return results

def create_vocab_checker():
    """vocab_sizeæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ"""
    print("\n" + "=" * 70)
    print("ğŸ”§ Step 3: vocab_sizeæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ")
    print("=" * 70)

    checker_code = '''#!/usr/bin/env python3
"""
NeuroQ vocab_size æ•´åˆæ€§ãƒã‚§ãƒƒã‚«ãƒ¼
================================
ãƒ¢ãƒ‡ãƒ«ã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®vocab_sizeãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
"""

import torch
import os
import sys

# neuroquantum_layered ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.insert(0, os.path.dirname(__file__))

try:
    from neuroquantum_layered import NeuroQuantumAI, NeuroQuantumTokenizer
    import sentencepiece as spm

    print("=" * 70)
    print("ğŸ” NeuroQ vocab_size æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")
    print("=" * 70)

    # 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®vocab_sizeã‚’ç¢ºèª
    print("\\n1ï¸âƒ£ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®vocab_size:")
    print("-" * 70)

    tokenizer_paths = [
        "neuroq_tokenizer.model",
        "../neuroq_tokenizer.model",
        "neuroq_tokenizer_8k.model",
        "../neuroq_tokenizer_8k.model",
    ]

    tokenizer_vocab_size = None
    tokenizer_path = None

    for path in tokenizer_paths:
        if os.path.exists(path):
            try:
                sp = spm.SentencePieceProcessor()
                sp.load(path)
                tokenizer_vocab_size = sp.get_piece_size()
                tokenizer_path = path
                print(f"   âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«: {path}")
                print(f"   ğŸ“Š èªå½™ã‚µã‚¤ã‚º: {tokenizer_vocab_size:,}")
                break
            except Exception as e:
                print(f"   âŒ {path}: {e}")

    if tokenizer_vocab_size is None:
        print("   âŒ æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    # 2. NeuroQuantumAIã‚’åˆæœŸåŒ–ã—ã¦ç¢ºèª
    print("\\n2ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«ã®vocab_size:")
    print("-" * 70)

    ai = NeuroQuantumAI(embed_dim=64, num_heads=2, num_layers=2)

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼ˆè»½é‡ï¼‰
    sample_texts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é©æ–°çš„ã§ã™ã€‚",
        "äººå·¥çŸ¥èƒ½ãŒæœªæ¥ã‚’å¤‰ãˆã¾ã™ã€‚",
    ] * 10

    ai.train(sample_texts, epochs=1, seq_len=16)

    # vocab_sizeã‚’ç¢ºèª
    print(f"   ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿéš›ã®vocab_size: {ai.tokenizer.actual_vocab_size:,}")
    print(f"   ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®vocab_size: {ai.tokenizer.vocab_size:,}")
    print(f"   ğŸ“Š ãƒ¢ãƒ‡ãƒ«config.vocab_size: {ai.config.vocab_size:,}")
    print(f"   ğŸ“Š Embeddingå±¤ã®num_embeddings: {ai.model.text_embedding.num_embeddings:,}")
    print(f"   ğŸ“Š LM Headå‡ºåŠ›æ¬¡å…ƒ: {ai.model.output_head.out_features:,}")

    # 3. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    print("\\n3ï¸âƒ£ æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯:")
    print("-" * 70)

    vocab_sizes = {
        "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼(actual)": ai.tokenizer.actual_vocab_size,
        "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼(è¨­å®š)": ai.tokenizer.vocab_size,
        "ãƒ¢ãƒ‡ãƒ«Config": ai.config.vocab_size,
        "Embeddingå±¤": ai.model.text_embedding.num_embeddings,
        "LM Head": ai.model.output_head.out_features,
    }

    all_match = len(set(vocab_sizes.values())) == 1

    if all_match:
        print(f"   âœ… ã™ã¹ã¦ã®vocab_sizeãŒä¸€è‡´ã—ã¦ã„ã¾ã™: {list(vocab_sizes.values())[0]:,}")
    else:
        print("   âŒ vocab_sizeã«ä¸ä¸€è‡´ãŒã‚ã‚Šã¾ã™:")
        for name, size in vocab_sizes.items():
            print(f"      {name}: {size:,}")

    print("\\n" + "=" * 70)
    if all_match:
        print("âœ… æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: åˆæ ¼")
    else:
        print("âŒ æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: ä¸åˆæ ¼")
    print("=" * 70)

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
'''

    output_path = "/home/user/NeuroQ/neuroq-runpod/check_vocab_consistency.py"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(checker_code)

    os.chmod(output_path, 0o755)
    print(f"âœ… æ•´åˆæ€§ãƒã‚§ãƒƒã‚«ãƒ¼ã‚’ä½œæˆã—ã¾ã—ãŸ: {output_path}")
    return output_path

def create_dockerfile_fix():
    """Dockerfileã«sentencepieceã‚’è¿½åŠ ã™ã‚‹ä¿®æ­£æ¡ˆã‚’æç¤º"""
    print("\n" + "=" * 70)
    print("ğŸ³ Step 4: Dockerfileä¿®æ­£æ¡ˆ")
    print("=" * 70)

    dockerfile_path = "/home/user/NeuroQ/neuroq-runpod/Dockerfile"

    if os.path.exists(dockerfile_path):
        print(f"DockerfileãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {dockerfile_path}")
        print("\nğŸ“ ä»¥ä¸‹ã®è¡Œã‚’RUNå‘½ä»¤ã«è¿½åŠ ã—ã¦ãã ã•ã„:")
        print("-" * 70)
        print("RUN pip install sentencepiece")
        print("-" * 70)
        print("\nå®Œå…¨ãªä¾‹:")
        print("RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\")
        print("    && pip install runpod sentencepiece")
    else:
        print(f"âŒ DockerfileãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dockerfile_path}")

def create_requirements_txt():
    """requirements.txtã‚’ä½œæˆ"""
    print("\n" + "=" * 70)
    print("ğŸ“„ Step 5: requirements.txtä½œæˆ")
    print("=" * 70)

    requirements = """# NeuroQ Dependencies
torch>=2.1.0
sentencepiece>=0.1.99
numpy>=1.24.0
runpod>=1.3.0
"""

    output_path = "/home/user/NeuroQ/neuroq-runpod/requirements.txt"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(requirements)

    print(f"âœ… requirements.txtã‚’ä½œæˆã—ã¾ã—ãŸ: {output_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 70)
    print("ğŸš€ NeuroQ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å•é¡Œ åŒ…æ‹¬çš„ä¿®æ­£")
    print("=" * 70)

    # Step 1: sentencepieceã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    if not check_and_install_sentencepiece():
        print("\nâŒ sentencepieceã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")
        print("   æ‰‹å‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: pip install sentencepiece")
        return

    # Step 2: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
    results = verify_tokenizer_files()

    # Step 3: æ•´åˆæ€§ãƒã‚§ãƒƒã‚«ãƒ¼ä½œæˆ
    checker_path = create_vocab_checker()

    # Step 4: Dockerfileä¿®æ­£æ¡ˆ
    create_dockerfile_fix()

    # Step 5: requirements.txtä½œæˆ
    create_requirements_txt()

    print("\n" + "=" * 70)
    print("âœ… ä¿®æ­£å®Œäº†")
    print("=" * 70)
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ:")
    print(f"   cd /home/user/NeuroQ/neuroq-runpod && python3 check_vocab_consistency.py")
    print("\n2. Dockerfileã‚’ä¿®æ­£ã—ã¦ sentencepiece ã‚’è¿½åŠ ")
    print("\n3. Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’å†ãƒ“ãƒ«ãƒ‰")
    print("\n4. RunPodã§å‹•ä½œç¢ºèª")

if __name__ == "__main__":
    main()
