#!/usr/bin/env python3
"""
QBNN ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ€ãƒ¼
======================
QBNNã‚’ä½¿ã£ãŸè¿·è·¯ã®çµŒè·¯æ¢ç´¢AI
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random
import math
import time
from typing import List, Tuple, Optional
from collections import deque


# ========================================
# QBNN Layer
# ========================================

class QBNNLayer(nn.Module):
    """Quantum-Bit Neural Network Layer"""
    
    def __init__(self, input_dim: int, output_dim: int, 
                 lambda_min: float = 0.2, lambda_max: float = 0.5):
        super().__init__()
        self.lambda_min = lambda_min
        self.lambda_max = lambda_max
        
        self.W = nn.Linear(input_dim, output_dim)
        self.J = nn.Parameter(torch.randn(input_dim, output_dim) * 0.02)
        self.lambda_base = nn.Parameter(torch.tensor(0.5))
        self.layer_norm = nn.LayerNorm(output_dim)
        self.call_count = 0
    
    def forward(self, h_prev: torch.Tensor) -> torch.Tensor:
        s_prev = torch.tanh(h_prev)
        h_tilde = self.W(h_prev)
        s_raw = torch.tanh(h_tilde)
        
        delta = torch.einsum('...i,ij,...j->...j', s_prev, self.J, s_raw)
        
        lambda_normalized = torch.sigmoid(self.lambda_base)
        if not self.training:
            phase = self.call_count * 0.2
            dynamic_factor = 0.5 + 0.5 * math.sin(phase)
            self.call_count += 1
        else:
            dynamic_factor = 0.5
        
        lambda_range = self.lambda_max - self.lambda_min
        lambda_eff = self.lambda_min + lambda_range * (lambda_normalized * 0.7 + dynamic_factor * 0.3)
        
        h_hat = h_tilde + lambda_eff * delta
        output = self.layer_norm(h_hat)
        return F.gelu(output)


# ========================================
# QBNN ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ€ãƒ¼
# ========================================

class QBNNPathfinder(nn.Module):
    """QBNNãƒ™ãƒ¼ã‚¹ã®çµŒè·¯æ¢ç´¢AI"""
    
    def __init__(self, grid_size: int = 10, hidden_dim: int = 128):
        super().__init__()
        self.grid_size = grid_size
        self.hidden_dim = hidden_dim
        
        # å…¥åŠ›: ç¾åœ¨ä½ç½®(2) + ã‚´ãƒ¼ãƒ«ä½ç½®(2) + å‘¨å›²8æ–¹å‘ã®å£æƒ…å ±(8) + è·é›¢æƒ…å ±(1) = 13
        self.input_dim = 13
        # å‡ºåŠ›: 4æ–¹å‘ã®ç§»å‹•ç¢ºç‡ (ä¸Š, ä¸‹, å·¦, å³)
        self.output_dim = 4
        
        # QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.qbnn1 = QBNNLayer(self.input_dim, hidden_dim)
        self.qbnn2 = QBNNLayer(hidden_dim, hidden_dim)
        self.qbnn3 = QBNNLayer(hidden_dim, hidden_dim // 2)
        
        # å‡ºåŠ›å±¤
        self.output = nn.Linear(hidden_dim // 2, self.output_dim)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        x = self.qbnn1(state)
        x = self.qbnn2(x)
        x = self.qbnn3(x)
        return self.output(x)
    
    def get_action(self, state: torch.Tensor, valid_actions: List[int], 
                   temperature: float = 0.3) -> int:
        """æ¬¡ã®è¡Œå‹•ã‚’é¸æŠ"""
        self.eval()
        with torch.no_grad():
            logits = self.forward(state.unsqueeze(0))[0]
            
            # ç„¡åŠ¹ãªè¡Œå‹•ã‚’ãƒã‚¹ã‚¯
            mask = torch.full((4,), float('-inf'))
            for a in valid_actions:
                mask[a] = 0
            logits = logits + mask
            
            # æ¸©åº¦ä»˜ãã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
            probs = F.softmax(logits / temperature, dim=0)
            
            # é‡å­çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if random.random() < 0.1:  # 10%æ¢ç´¢
                return random.choice(valid_actions)
            
            return torch.argmax(probs).item()


# ========================================
# è¿·è·¯ã‚¯ãƒ©ã‚¹
# ========================================

class Maze:
    """è¿·è·¯ç”Ÿæˆã¨ç®¡ç†"""
    
    # æ–¹å‘: 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³
    DIRECTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]
    DIR_NAMES = ['â†‘', 'â†“', 'â†', 'â†’']
    
    def __init__(self, size: int = 10):
        self.size = size
        self.grid = None
        self.start = None
        self.goal = None
        self.path = []
        self.visited = set()
    
    def generate(self, wall_density: float = 0.25):
        """è¿·è·¯ç”Ÿæˆ"""
        self.grid = [[0] * self.size for _ in range(self.size)]
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«å£ã‚’é…ç½®
        for i in range(self.size):
            for j in range(self.size):
                if random.random() < wall_density:
                    self.grid[i][j] = 1  # å£
        
        # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«ã‚’è¨­å®š
        self.start = (0, 0)
        self.goal = (self.size - 1, self.size - 1)
        
        # ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã‚´ãƒ¼ãƒ«ã¯å¿…ãšç©ºã«
        self.grid[self.start[0]][self.start[1]] = 0
        self.grid[self.goal[0]][self.goal[1]] = 0
        
        # çµŒè·¯ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        if not self._has_path():
            self.generate(wall_density)  # å†ç”Ÿæˆ
        
        self.path = []
        self.visited = set()
    
    def _has_path(self) -> bool:
        """BFSã§çµŒè·¯å­˜åœ¨ç¢ºèª"""
        queue = deque([self.start])
        visited = {self.start}
        
        while queue:
            r, c = queue.popleft()
            if (r, c) == self.goal:
                return True
            
            for dr, dc in self.DIRECTIONS:
                nr, nc = r + dr, c + dc
                if (0 <= nr < self.size and 0 <= nc < self.size and 
                    self.grid[nr][nc] == 0 and (nr, nc) not in visited):
                    visited.add((nr, nc))
                    queue.append((nr, nc))
        
        return False
    
    def get_shortest_path(self) -> List[Tuple[int, int]]:
        """BFSã§æœ€çŸ­çµŒè·¯ã‚’å–å¾—"""
        queue = deque([(self.start, [self.start])])
        visited = {self.start}
        
        while queue:
            (r, c), path = queue.popleft()
            if (r, c) == self.goal:
                return path
            
            for dr, dc in self.DIRECTIONS:
                nr, nc = r + dr, c + dc
                if (0 <= nr < self.size and 0 <= nc < self.size and 
                    self.grid[nr][nc] == 0 and (nr, nc) not in visited):
                    visited.add((nr, nc))
                    queue.append(((nr, nc), path + [(nr, nc)]))
        
        return []
    
    def get_valid_actions(self, pos: Tuple[int, int]) -> List[int]:
        """æœ‰åŠ¹ãªè¡Œå‹•ãƒªã‚¹ãƒˆ"""
        r, c = pos
        valid = []
        for i, (dr, dc) in enumerate(self.DIRECTIONS):
            nr, nc = r + dr, c + dc
            if 0 <= nr < self.size and 0 <= nc < self.size and self.grid[nr][nc] == 0:
                valid.append(i)
        return valid
    
    def get_state(self, pos: Tuple[int, int]) -> torch.Tensor:
        """ç¾åœ¨çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
        r, c = pos
        gr, gc = self.goal
        
        # ä½ç½®æƒ…å ±ï¼ˆæ­£è¦åŒ–ï¼‰
        state = [
            r / self.size,
            c / self.size,
            gr / self.size,
            gc / self.size,
        ]
        
        # å‘¨å›²8æ–¹å‘ã®å£æƒ…å ±
        for dr in [-1, 0, 1]:
            for dc in [-1, 0, 1]:
                if dr == 0 and dc == 0:
                    continue
                nr, nc = r + dr, c + dc
                if 0 <= nr < self.size and 0 <= nc < self.size:
                    state.append(float(self.grid[nr][nc]))
                else:
                    state.append(1.0)  # ç¯„å›²å¤–ã¯å£æ‰±ã„
        
        # ã‚´ãƒ¼ãƒ«ã¾ã§ã®è·é›¢ï¼ˆæ­£è¦åŒ–ï¼‰
        dist = abs(gr - r) + abs(gc - c)
        state.append(dist / (2 * self.size))
        
        return torch.tensor(state, dtype=torch.float32)
    
    def display(self, current_pos: Optional[Tuple[int, int]] = None, 
                path: Optional[List[Tuple[int, int]]] = None):
        """è¿·è·¯è¡¨ç¤º"""
        path_set = set(path) if path else set()
        
        print("\n  " + " ".join([str(i % 10) for i in range(self.size)]))
        for i in range(self.size):
            row = f"{i % 10} "
            for j in range(self.size):
                if current_pos and (i, j) == current_pos:
                    row += "ğŸ”µ"
                elif (i, j) == self.start:
                    row += "ğŸŸ¢"
                elif (i, j) == self.goal:
                    row += "ğŸ”´"
                elif (i, j) in path_set:
                    row += "ğŸŸ¡"
                elif self.grid[i][j] == 1:
                    row += "â¬›"
                else:
                    row += "â¬œ"
            print(row)
        print()


# ========================================
# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
# ========================================

class PathfinderTrainer:
    """QBNNãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ€ãƒ¼ã®è¨“ç·´"""
    
    def __init__(self, pathfinder: QBNNPathfinder, grid_size: int = 10):
        self.pathfinder = pathfinder
        self.grid_size = grid_size
        self.optimizer = torch.optim.Adam(pathfinder.parameters(), lr=0.005)
    
    def generate_episode(self, maze: Maze) -> List[Tuple]:
        """1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµŒé¨“ã‚’ç”Ÿæˆ"""
        pos = maze.start
        trajectory = []
        visited = {pos}
        max_steps = maze.size * maze.size
        
        for _ in range(max_steps):
            state = maze.get_state(pos)
            valid_actions = maze.get_valid_actions(pos)
            
            if not valid_actions:
                break
            
            # Îµ-greedy
            if random.random() < 0.3:
                action = random.choice(valid_actions)
            else:
                action = self.pathfinder.get_action(state, valid_actions)
            
            # ç§»å‹•
            dr, dc = Maze.DIRECTIONS[action]
            new_pos = (pos[0] + dr, pos[1] + dc)
            
            # å ±é…¬è¨ˆç®—
            if new_pos == maze.goal:
                reward = 10.0
            elif new_pos in visited:
                reward = -1.0  # å†è¨ªå•ãƒšãƒŠãƒ«ãƒ†ã‚£
            else:
                # ã‚´ãƒ¼ãƒ«ã«è¿‘ã¥ã„ãŸã‚‰å ±é…¬
                old_dist = abs(maze.goal[0] - pos[0]) + abs(maze.goal[1] - pos[1])
                new_dist = abs(maze.goal[0] - new_pos[0]) + abs(maze.goal[1] - new_pos[1])
                reward = (old_dist - new_dist) * 0.5
            
            trajectory.append((state, action, reward, valid_actions))
            
            visited.add(new_pos)
            pos = new_pos
            
            if pos == maze.goal:
                break
        
        return trajectory
    
    def train(self, epochs: int = 100, episodes_per_epoch: int = 50):
        """è¨“ç·´"""
        print("\nğŸ—ºï¸ QBNN ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ€ãƒ¼è¨“ç·´")
        print("=" * 50)
        
        best_success_rate = 0
        
        for epoch in range(epochs):
            maze = Maze(self.grid_size)
            maze.generate(wall_density=0.2)
            
            all_trajectories = []
            successes = 0
            
            for _ in range(episodes_per_epoch):
                trajectory = self.generate_episode(maze)
                all_trajectories.append(trajectory)
                
                # æˆåŠŸåˆ¤å®šï¼ˆæœ€å¾Œã«ã‚´ãƒ¼ãƒ«ã«åˆ°é”ï¼‰
                if trajectory and trajectory[-1][2] == 10.0:
                    successes += 1
            
            # å­¦ç¿’
            total_loss = 0
            for trajectory in all_trajectories:
                if not trajectory:
                    continue
                
                # Returnè¨ˆç®—ï¼ˆå‰²å¼•ç´¯ç©å ±é…¬ï¼‰
                returns = []
                G = 0
                gamma = 0.99
                for _, _, reward, _ in reversed(trajectory):
                    G = reward + gamma * G
                    returns.insert(0, G)
                
                returns = torch.tensor(returns, dtype=torch.float32)
                if len(returns) > 1:
                    returns = (returns - returns.mean()) / (returns.std() + 1e-8)
                
                # Policy Gradient
                for (state, action, _, valid_actions), ret in zip(trajectory, returns):
                    self.optimizer.zero_grad()
                    
                    logits = self.pathfinder(state.unsqueeze(0))[0]
                    
                    # ãƒã‚¹ã‚¯
                    mask = torch.full((4,), float('-inf'))
                    for a in valid_actions:
                        mask[a] = 0
                    logits = logits + mask
                    
                    log_probs = F.log_softmax(logits, dim=0)
                    loss = -log_probs[action] * ret
                    
                    loss.backward()
                    self.optimizer.step()
                    
                    total_loss += loss.item()
            
            success_rate = successes / episodes_per_epoch * 100
            
            if success_rate > best_success_rate:
                best_success_rate = success_rate
            
            if (epoch + 1) % 20 == 0 or epoch == 0:
                avg_loss = total_loss / max(sum(len(t) for t in all_trajectories), 1)
                print(f"   Epoch {epoch+1:3d}/{epochs}: Loss={avg_loss:.4f}, æˆåŠŸç‡={success_rate:.1f}%")
        
        print(f"\nâœ… è¨“ç·´å®Œäº†ï¼ æœ€é«˜æˆåŠŸç‡: {best_success_rate:.1f}%")


# ========================================
# ãƒ‡ãƒ¢
# ========================================

def demo_pathfinding():
    """ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒ‡ãƒ¢"""
    print("=" * 60)
    print("ğŸ—ºï¸ QBNN ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ€ãƒ¼ ãƒ‡ãƒ¢")
    print("=" * 60)
    
    grid_size = 8
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    pathfinder = QBNNPathfinder(grid_size=grid_size, hidden_dim=64)
    
    # è¨“ç·´
    trainer = PathfinderTrainer(pathfinder, grid_size=grid_size)
    trainer.train(epochs=100, episodes_per_epoch=30)
    
    # ãƒ†ã‚¹ãƒˆ
    print("\n" + "=" * 50)
    print("ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 50)
    
    for test_num in range(3):
        print(f"\n--- ãƒ†ã‚¹ãƒˆ {test_num + 1} ---")
        
        maze = Maze(grid_size)
        maze.generate(wall_density=0.2)
        
        # æœ€çŸ­çµŒè·¯ï¼ˆBFSï¼‰
        shortest = maze.get_shortest_path()
        print(f"ğŸ“ æœ€çŸ­çµŒè·¯é•·: {len(shortest) - 1} ã‚¹ãƒ†ãƒƒãƒ—")
        
        # QBNNçµŒè·¯
        pos = maze.start
        qbnn_path = [pos]
        visited = {pos}
        max_steps = grid_size * grid_size
        
        start_time = time.time()
        
        for step in range(max_steps):
            if pos == maze.goal:
                break
            
            state = maze.get_state(pos)
            valid_actions = maze.get_valid_actions(pos)
            
            if not valid_actions:
                print("   âŒ è¡Œãæ­¢ã¾ã‚Šï¼")
                break
            
            action = pathfinder.get_action(state, valid_actions, temperature=0.2)
            dr, dc = Maze.DIRECTIONS[action]
            pos = (pos[0] + dr, pos[1] + dc)
            qbnn_path.append(pos)
            
            if pos in visited:
                pass  # å†è¨ªå•
            visited.add(pos)
        
        elapsed = time.time() - start_time
        
        if pos == maze.goal:
            print(f"âœ… QBNNçµŒè·¯é•·: {len(qbnn_path) - 1} ã‚¹ãƒ†ãƒƒãƒ—")
            efficiency = (len(shortest) - 1) / (len(qbnn_path) - 1) * 100 if len(qbnn_path) > 1 else 0
            print(f"ğŸ“Š åŠ¹ç‡: {efficiency:.1f}%")
        else:
            print(f"âŒ ã‚´ãƒ¼ãƒ«æœªåˆ°é” ({len(qbnn_path) - 1} ã‚¹ãƒ†ãƒƒãƒ—ã§ä¸­æ–­)")
        
        print(f"â±ï¸ æ¢ç´¢æ™‚é–“: {elapsed*1000:.2f}ms")
        
        # è¿·è·¯è¡¨ç¤º
        print("\næœ€çŸ­çµŒè·¯:")
        maze.display(path=shortest)
        
        print("QBNNçµŒè·¯:")
        maze.display(path=qbnn_path)


def interactive_demo():
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢"""
    print("=" * 60)
    print("ğŸ—ºï¸ QBNN ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ€ãƒ¼ - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    print("=" * 60)
    
    grid_size = 8
    
    pathfinder = QBNNPathfinder(grid_size=grid_size, hidden_dim=64)
    trainer = PathfinderTrainer(pathfinder, grid_size=grid_size)
    trainer.train(epochs=80, episodes_per_epoch=40)
    
    while True:
        print("\næ–°ã—ã„è¿·è·¯ã‚’ç”Ÿæˆã—ã¾ã™...")
        maze = Maze(grid_size)
        maze.generate(wall_density=0.25)
        
        print("\nğŸŸ¢ = ã‚¹ã‚¿ãƒ¼ãƒˆ, ğŸ”´ = ã‚´ãƒ¼ãƒ«, â¬› = å£")
        maze.display()
        
        input("Enterã§ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°é–‹å§‹...")
        
        pos = maze.start
        path = [pos]
        step = 0
        
        while pos != maze.goal and step < grid_size * grid_size:
            state = maze.get_state(pos)
            valid_actions = maze.get_valid_actions(pos)
            
            if not valid_actions:
                print("è¡Œãæ­¢ã¾ã‚Šï¼")
                break
            
            action = pathfinder.get_action(state, valid_actions, temperature=0.2)
            dr, dc = Maze.DIRECTIONS[action]
            pos = (pos[0] + dr, pos[1] + dc)
            path.append(pos)
            step += 1
            
            print(f"\rã‚¹ãƒ†ãƒƒãƒ— {step}: {Maze.DIR_NAMES[action]} â†’ ({pos[0]},{pos[1]})", end="")
            time.sleep(0.1)
        
        print()
        
        if pos == maze.goal:
            print(f"\nğŸ‰ ã‚´ãƒ¼ãƒ«åˆ°é”ï¼ {len(path)-1}ã‚¹ãƒ†ãƒƒãƒ—")
        else:
            print(f"\nâŒ å¤±æ•—")
        
        # æœ€çŸ­çµŒè·¯ã¨æ¯”è¼ƒ
        shortest = maze.get_shortest_path()
        print(f"ğŸ“ æœ€çŸ­: {len(shortest)-1}ã‚¹ãƒ†ãƒƒãƒ—")
        
        maze.display(path=path)
        
        cont = input("\nç¶šã‘ã¾ã™ã‹? (y/n): ").strip().lower()
        if cont != 'y':
            break


if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ—ºï¸ QBNN ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ€ãƒ¼")
    print("=" * 60)
    print("\né¸æŠ:")
    print("  1. ãƒ‡ãƒ¢ï¼ˆè‡ªå‹•ãƒ†ã‚¹ãƒˆï¼‰")
    print("  2. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–")
    
    choice = input("\né¸æŠ (1/2): ").strip()
    
    if choice == '2':
        interactive_demo()
    else:
        demo_pathfinding()

