#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—      â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—     â•‘
â•‘   â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘     â•‘
â•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘     â•‘
â•‘   â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘     â•‘
â•‘   â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•  â•šâ•â•â–€â–€â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•     â•‘
â•‘                                                                               â•‘
â•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                                                                   â•‘
â•‘   â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—                                                                  â•‘
â•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘                                                                  â•‘
â•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘                                                                  â•‘
â•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•                                                                  â•‘
â•‘    â•šâ•â•â•â•â•â•                                                                   â•‘
â•‘                                                                               â•‘
â•‘   neuroQ Brain: è„³å‹æ•£åœ¨QBNNã«ã‚ˆã‚‹ç”ŸæˆAI                                      â•‘
â•‘   ç‹¬è‡ªã®é‡å­ã‚‚ã¤ã‚Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ç”ŸæˆAI                          â•‘
â•‘                                                                               â•‘
â•‘   å‚ç…§å…ƒ: qbnn_brain.py                                                       â•‘
â•‘   - QBNNBrain: ç´”ç²‹Pythonç‰ˆã®è„³å‹æ•£åœ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯                             â•‘
â•‘   - QBNNBrainTorch: PyTorchç‰ˆã®è„³å‹æ•£åœ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯                          â•‘
â•‘   - QuantumNeuron: å˜ä¸€é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³                                   â•‘
â•‘                                                                               â•‘
â•‘   ç‰¹å¾´:                                                                       â•‘
â•‘   - å„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒç‹¬ç«‹ã—ãŸé‡å­ãƒ“ãƒƒãƒˆï¼ˆAPQBï¼‰                                   â•‘
â•‘   - ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é–“ã®æ¥ç¶šã¯ã‚°ãƒ©ãƒ•æ§‹é€ ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ï¼‰                                 â•‘
â•‘   - æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ä¿¡å·ãŒä¼æ’­                                                   â•‘
â•‘   - é‡å­ã‚‚ã¤ã‚ŒãŒä»»æ„ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é–“ã§ç™ºç”Ÿ                                       â•‘
â•‘   - å‹•çš„å…¥å‡ºåŠ›ï¼ˆæœ¬ç‰©ã®è„³ã®ã‚ˆã†ã«å…¥åŠ›/å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå¤‰åŒ–ï¼‰                     â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import random
import os
from typing import List, Dict, Tuple, Optional
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

# SentencePieceï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ç”¨ï¼‰
try:
    import sentencepiece as spm
    SENTENCEPIECE_AVAILABLE = True
except ImportError:
    SENTENCEPIECE_AVAILABLE = False
    warnings.warn("sentencepieceãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install sentencepiece ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç”¨ï¼‰
try:
    from transformers import (
        GPT2Config,
        GPT2Attention,
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    # è­¦å‘Šã¯è¡¨ç¤ºã—ãªã„ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œå¯èƒ½ãªãŸã‚ï¼‰

# OpenAI APIï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    warnings.warn("OpenAI APIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚openai>=1.0.0ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")

# ========================================
# qbnn_brain.py ã‹ã‚‰ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ========================================
try:
    from qbnn_brain import (
        QuantumNeuron,      # å˜ä¸€é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³
        QBNNBrain,          # ç´”ç²‹Pythonç‰ˆè„³å‹QBNN
        QBNNBrainTorch,     # PyTorchç‰ˆè„³å‹QBNN
    )
    QBNN_BRAIN_AVAILABLE = True
    print("âœ… qbnn_brain.py ã‹ã‚‰ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError:
    QBNN_BRAIN_AVAILABLE = False
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãªã®ã§è­¦å‘Šã‚’è¡¨ç¤ºã—ãªã„ï¼ˆå†…è”µã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§å‹•ä½œã—ã¾ã™ï¼‰


print("=" * 70)
print("ğŸ§ âš›ï¸ ãƒ‹ãƒ¥ãƒ¼ãƒ­Q Brain")
print("   è„³å‹æ•£åœ¨é‡å­ãƒ“ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ç”ŸæˆAI")
print("   (qbnn_brain.py ãƒ™ãƒ¼ã‚¹)")
print("=" * 70)


# ========================================
# APQBï¼ˆèª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆï¼‰- å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ========================================

class APQB:
    """
    APQBç†è«–ã®ã‚³ã‚¢
    
    qbnn_brain.py ã® QuantumNeuron ã¨åŒã˜ç†è«–ã«åŸºã¥ã:
    - Î¸: å†…éƒ¨è§’åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - r = cos(2Î¸): ç›¸é–¢ä¿‚æ•°
    - T = |sin(2Î¸)|: æ¸©åº¦ï¼ˆã‚†ã‚‰ãï¼‰
    """
    
    @staticmethod
    def theta_to_r(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ ç›¸é–¢ä¿‚æ•° r = cos(2Î¸)"""
        return torch.cos(2 * theta)
    
    @staticmethod
    def theta_to_T(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ æ¸©åº¦ T = |sin(2Î¸)|"""
        return torch.abs(torch.sin(2 * theta))
    
    @staticmethod
    def measure(theta: torch.Tensor) -> torch.Tensor:
        """é‡å­æ¸¬å®šï¼ˆç¢ºç‡çš„ã«0 or 1ï¼‰"""
        prob_1 = torch.sin(theta) ** 2
        return (torch.rand_like(prob_1) < prob_1).float()


# ========================================
# OpenAI Embedding ãƒ©ãƒƒãƒ‘ãƒ¼
# ========================================

class OpenAIEmbeddingWrapper:
    """
    OpenAI Embedding API ãƒ©ãƒƒãƒ‘ãƒ¼
    
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥OpenAI APIã«é€ä¿¡ã—ã¦ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—
    """
    
    def __init__(self, api_key: Optional[str] = None, model: str = "text-embedding-3-large", dimensions: Optional[int] = None):
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI APIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚openai>=1.0.0ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã™ã‚‹ã‹ã€api_keyå¼•æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        
        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒ
        if dimensions is not None:
            self.embed_dim = dimensions
            self.dimensions = dimensions
        elif "ada-002" in model:
            self.embed_dim = 1536
            self.dimensions = None
        elif "embedding-3-large" in model:
            self.embed_dim = 3072  # text-embedding-3-largeã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒ
            self.dimensions = None
        elif "embedding-3-small" in model:
            self.embed_dim = 1536  # text-embedding-3-smallã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒ
            self.dimensions = None
        else:
            self.embed_dim = 3072  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            self.dimensions = None
    
    def get_embeddings(self, texts: List[str], batch_size: int = 100) -> np.ndarray:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‹ã‚‰ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—
        
        Args:
            texts: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆAPIåˆ¶é™ã‚’è€ƒæ…®ï¼‰
        
        Returns:
            (N, embed_dim) ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°é…åˆ—
        """
        all_embeddings = []
        
        # ãƒãƒƒãƒå‡¦ç†
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            try:
                # dimensionsãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šå¯èƒ½ï¼ˆtext-embedding-3-largeç­‰ã§ä½¿ç”¨ï¼‰
                params = {
                    "model": self.model,
                    "input": batch
                }
                if self.dimensions is not None:
                    params["dimensions"] = self.dimensions
                
                response = self.client.embeddings.create(**params)
                
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
            except Exception as e:
                raise RuntimeError(f"OpenAI APIã‚¨ãƒ©ãƒ¼: {e}")
        
        return np.array(all_embeddings)


# ========================================
# è„³å‹é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤
# ========================================

class BrainQuantumLayer(nn.Module):
    """
    è„³å‹æ•£åœ¨é‡å­ãƒ“ãƒƒãƒˆå±¤
    
    qbnn_brain.py ã® QBNNBrainTorch ã‚’åŸºç›¤ã¨ã—ã¦ä½¿ç”¨å¯èƒ½
    
    - ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒãƒãƒ©ãƒãƒ©ã«æ¥ç¶š
    - ã‚¹ãƒ‘ãƒ¼ã‚¹ãªã‚°ãƒ©ãƒ•æ§‹é€ 
    - æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ä¿¡å·ä¼æ’­
    - å‹•çš„å…¥å‡ºåŠ›å¯¾å¿œï¼ˆQBNN_BRAIN_AVAILABLEæ™‚ï¼‰
    """
    
    def __init__(self, num_neurons: int, input_dim: int, output_dim: int,
                 connection_density: float = 0.25, lambda_entangle: float = 0.35,
                 use_qbnn_brain: bool = True):
        super().__init__()
        
        self.num_neurons = num_neurons
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.use_qbnn_brain = use_qbnn_brain and QBNN_BRAIN_AVAILABLE
        
        if self.use_qbnn_brain:
            # qbnn_brain.py ã® QBNNBrainTorch ã‚’ä½¿ç”¨
            self.qbnn_core = QBNNBrainTorch(
                num_neurons=num_neurons,
                max_input_size=input_dim,
                max_output_size=output_dim,
                connection_density=connection_density
            )
            self.qbnn_core.lambda_entangle = nn.Parameter(torch.tensor(lambda_entangle))
            
            # QBNNBrainTorchã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‚ç…§
            self.theta = self.qbnn_core.theta
            self.connection_mask = self.qbnn_core.connection_mask
            self.weights = self.qbnn_core.weights
            self.J = self.qbnn_core.J
            self.lambda_entangle = self.qbnn_core.lambda_entangle
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå†…è”µå®Ÿè£…
            self.lambda_entangle = nn.Parameter(torch.tensor(lambda_entangle))
            
            # å…¥åŠ›å°„å½±
            self.input_proj = nn.Linear(input_dim, num_neurons)
            
            # å„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            self.theta = nn.Parameter(torch.rand(num_neurons) * 1.0 + 0.25)
            
            # æ¥ç¶šãƒã‚¹ã‚¯ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ï¼‰
            mask = torch.rand(num_neurons, num_neurons) < connection_density
            mask.fill_diagonal_(False)  # è‡ªå·±æ¥ç¶šãªã—
            self.register_buffer('connection_mask', mask.float())
            
            # é‡ã¿è¡Œåˆ—
            self.weights = nn.Parameter(torch.randn(num_neurons, num_neurons) * 0.3)
            
            # ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ« J
            self.J = nn.Parameter(torch.randn(num_neurons, num_neurons) * 0.1)
            
            # å‡ºåŠ›å°„å½±
            self.output_proj = nn.Linear(num_neurons, output_dim)
    
    def get_r(self) -> torch.Tensor:
        if self.use_qbnn_brain:
            return self.qbnn_core.get_r()
        return APQB.theta_to_r(self.theta)
    
    def get_T(self) -> torch.Tensor:
        if self.use_qbnn_brain:
            return self.qbnn_core.get_T()
        return APQB.theta_to_T(self.theta)
    
    def forward(self, x: torch.Tensor, time_steps: int = 3, 
                dynamic_selection: bool = False) -> torch.Tensor:
        """
        å‰å‘ãä¼æ’­
        
        qbnn_brain.py ã® QBNNBrainTorch ã‚’ä½¿ç”¨å¯èƒ½
        
        Args:
            x: (batch, seq, input_dim) or (batch, input_dim)
            time_steps: ä¼æ’­ã‚¹ãƒ†ãƒƒãƒ—æ•°
            dynamic_selection: å‹•çš„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é¸æŠã‚’ä½¿ç”¨ã™ã‚‹ã‹
        
        Returns:
            (batch, seq, output_dim) or (batch, output_dim)
        """
        # å…¥åŠ›å½¢çŠ¶ã‚’ä¿æŒ
        original_shape = x.shape
        if len(original_shape) == 3:
            batch, seq, _ = x.shape
            x = x.view(batch * seq, -1)
        else:
            batch = x.size(0)
            seq = None
        
        if self.use_qbnn_brain:
            # qbnn_brain.py ã® QBNNBrainTorch ã‚’ä½¿ç”¨
            output, in_neurons, out_neurons = self.qbnn_core(
                x, 
                input_size=self.input_dim,
                output_size=self.output_dim,
                time_steps=time_steps,
                dynamic_selection=dynamic_selection
            )
            
            # å…ƒã®å½¢çŠ¶ã«æˆ»ã™
            if seq is not None:
                output = output.view(batch, seq, -1)
            
            return output
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå†…è”µå®Ÿè£…
            # å…¥åŠ›ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«å°„å½±
            state = self.input_proj(x)  # (batch*seq, num_neurons)
            
            # æœ‰åŠ¹ãªé‡ã¿ï¼ˆãƒã‚¹ã‚¯é©ç”¨ï¼‰
            effective_weights = self.weights * self.connection_mask
            
            # æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ä¼æ’­
            for t in range(time_steps):
                # é€šå¸¸ã®ä¿¡å·ä¼æ’­
                signal = torch.matmul(state, effective_weights)
                
                # é‡å­ã‚‚ã¤ã‚Œè£œæ­£
                s = torch.tanh(state)  # æ­£è¦åŒ– [-1, 1]
                
                # ã‚‚ã¤ã‚Œè¨ˆç®—ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
                J_masked = self.J * self.connection_mask
                delta = torch.einsum('bi,ij,bj->bj', s, J_masked, s)
                
                # æœ‰åŠ¹å…¥åŠ›
                effective_input = signal + self.lambda_entangle * delta
                
                # é‡å­ã‚†ã‚‰ãã‚’è¿½åŠ 
                T = self.get_T()
                noise = torch.randn_like(state) * T.unsqueeze(0) * 0.1
                
                state = torch.tanh(effective_input + noise)
            
            # å‡ºåŠ›å°„å½±
            output = self.output_proj(state)
            
            # å…ƒã®å½¢çŠ¶ã«æˆ»ã™
            if seq is not None:
                output = output.view(batch, seq, -1)
            
            return output
    
    def get_quantum_stats(self) -> Dict:
        """é‡å­çµ±è¨ˆã‚’å–å¾—"""
        with torch.no_grad():
            if self.use_qbnn_brain:
                # qbnn_brain.py ã®æƒ…å ±ã‚’å–å¾—
                info = self.qbnn_core.get_quantum_info()
                return {
                    'theta_mean': info['theta_mean'],
                    'r_mean': info['r_mean'],
                    'T_mean': info['T_mean'],
                    'lambda': info['lambda'],
                    'connections': info['connections'],
                    'sensitivity_mean': info.get('sensitivity_mean', 0),
                    'output_tendency_mean': info.get('output_tendency_mean', 0),
                    'source': 'qbnn_brain.py',
                }
            else:
                return {
                    'theta_mean': self.theta.mean().item(),
                    'r_mean': self.get_r().mean().item(),
                    'T_mean': self.get_T().mean().item(),
                    'lambda': self.lambda_entangle.item(),
                    'connections': self.connection_mask.sum().item(),
                    'source': 'builtin',
                }


# ========================================
# è„³å‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
# ========================================

class BrainQuantumAttention(nn.Module):
    """
    è„³å‹é‡å­ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆtransformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ™ãƒ¼ã‚¹ + QBNNæ‹¡å¼µï¼‰
    
    transformersã®GPT2Attentionã‚’ãƒ™ãƒ¼ã‚¹ã«ã€è„³å‹é‡å­ã‚‚ã¤ã‚Œè£œæ­£ã‚’è¿½åŠ 
    """
    
    def __init__(self, embed_dim: int, num_heads: int = 4, 
                 num_neurons: int = 32, dropout: float = 0.1,
                 max_positions: int = 1024):
        super().__init__()
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        # transformersã®GPT2Attentionã‚’ä½¿ç”¨
        if TRANSFORMERS_AVAILABLE:
            config = GPT2Config(
                n_embd=embed_dim,
                n_head=num_heads,
                attn_pdrop=dropout,
                resid_pdrop=dropout,
                max_position_embeddings=max_positions,
            )
            self.attention = GPT2Attention(config, layer_idx=None)
        else:
            warnings.warn("transformersãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ç°¡æ˜“ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self.attention = None
            self.q_proj = nn.Linear(embed_dim, embed_dim)
            self.k_proj = nn.Linear(embed_dim, embed_dim)
            self.v_proj = nn.Linear(embed_dim, embed_dim)
            self.out_proj = nn.Linear(embed_dim, embed_dim)
            self.dropout = nn.Dropout(dropout)
        
        # è„³å‹é‡å­å±¤ï¼ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ç”¨ - QBNNæ‹¡å¼µï¼‰
        self.brain_layer = BrainQuantumLayer(
            num_neurons=num_neurons,
            input_dim=embed_dim,
            output_dim=embed_dim,
            connection_density=0.2,
            lambda_entangle=0.3
        )
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        transformersãƒ™ãƒ¼ã‚¹ã®Multi-Head Causal Self-Attentionï¼ˆè„³å‹é‡å­æ‹¡å¼µç‰ˆï¼‰
        
        Args:
            x: (batch, seq, embed_dim)
            mask: Optional attention mask
        
        Returns:
            (batch, seq, embed_dim)
        """
        if TRANSFORMERS_AVAILABLE and self.attention is not None:
            # transformersã®GPT2Attentionã‚’ä½¿ç”¨
            hidden_states = x
            
            # è„³å‹é‡å­å‡¦ç†ã§å…¥åŠ›ã‚’å¤‰èª¿ï¼ˆQBNNæ‹¡å¼µï¼‰
            quantum_modulation = self.brain_layer(hidden_states, time_steps=2)
            hidden_states = hidden_states + 0.1 * quantum_modulation
            
            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ï¼ˆtransformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰
            attn_output = self.attention(hidden_states, layer_past=None, use_cache=False, output_attentions=False)[0]
            
            return attn_output
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç°¡æ˜“å®Ÿè£… + è„³å‹é‡å­æ‹¡å¼µ
            batch, seq, _ = x.shape
            
            # è„³å‹é‡å­å‡¦ç†ã§å…¥åŠ›ã‚’å¤‰èª¿
            quantum_modulation = self.brain_layer(x, time_steps=2)
            x_modulated = x + 0.1 * quantum_modulation
            
            # Q, K, V è¨ˆç®—
            Q = self.q_proj(x_modulated)
            K = self.k_proj(x_modulated)
            V = self.v_proj(x_modulated)
            
            # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰å½¢å¼ã«å¤‰æ›
            Q = Q.view(batch, seq, self.num_heads, self.head_dim).transpose(1, 2)
            K = K.view(batch, seq, self.num_heads, self.head_dim).transpose(1, 2)
            V = V.view(batch, seq, self.num_heads, self.head_dim).transpose(1, 2)
            
            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢è¨ˆç®—
            scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
            
            # Causal Maské©ç”¨
            if mask is None:
                causal_mask = torch.triu(torch.ones(seq, seq, device=x.device), diagonal=1).bool()
                causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)
                scores = scores.masked_fill(causal_mask, float('-inf'))
            else:
                if mask.dim() == 2:
                    mask = mask.unsqueeze(0).unsqueeze(0)
                scores = scores.masked_fill(mask == 0, float('-inf'))
            
            # Softmax + Dropout
            attn_weights = F.softmax(scores, dim=-1)
            attn_weights = self.dropout(attn_weights)
            
            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é©ç”¨
            context = torch.matmul(attn_weights, V)
            context = context.transpose(1, 2).contiguous().view(batch, seq, self.embed_dim)
            
            return self.out_proj(context)


# ========================================
# è„³å‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
# ========================================

class BrainQuantumBlock(nn.Module):
    """
    GPTãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆè„³å‹é‡å­æ‹¡å¼µç‰ˆï¼‰
    
    GPTæ¨™æº–æ§‹é€ :
    1. Pre-norm LayerNorm
    2. Multi-Head Causal Self-Attention
    3. Residual Connection
    4. Pre-norm LayerNorm
    5. Feed-Forward Network (æ¨™æº–FFN + è„³å‹é‡å­æ‹¡å¼µ)
    6. Residual Connection
    """
    
    def __init__(self, embed_dim: int, num_heads: int = 4,
                 num_neurons: int = 32, dropout: float = 0.1,
                 ffn_expansion: int = 4):
        super().__init__()
        
        # Pre-norm LayerNorm
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        # è„³å‹é‡å­ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
        self.attention = BrainQuantumAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            num_neurons=num_neurons,
            dropout=dropout
        )
        
        # GPTæ¨™æº–FFN: Linear â†’ GELU â†’ Linear
        ffn_hidden = embed_dim * ffn_expansion
        self.ffn_standard = nn.Sequential(
            nn.Linear(embed_dim, ffn_hidden),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ffn_hidden, embed_dim),
            nn.Dropout(dropout)
        )
        
        # è„³å‹é‡å­æ‹¡å¼µFFNï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.ffn_quantum = BrainQuantumLayer(
            num_neurons=num_neurons * 2,
            input_dim=embed_dim,
            output_dim=embed_dim,
            connection_density=0.25,
            lambda_entangle=0.35
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        GPTãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        
        Args:
            x: (batch, seq, embed_dim)
            mask: Optional attention mask
        
        Returns:
            (batch, seq, embed_dim)
        """
        # 1. Pre-norm + Multi-Head Causal Self-Attention + Residual
        residual = x
        x = self.norm1(x)
        attn_out = self.attention(x, mask)
        x = residual + self.dropout(attn_out)
        
        # 2. Pre-norm + Feed-Forward Network + Residual
        residual = x
        x = self.norm2(x)
        
        # æ¨™æº–FFN + è„³å‹é‡å­æ‹¡å¼µï¼ˆãƒ–ãƒ¬ãƒ³ãƒ‰ï¼‰
        ffn_standard_out = self.ffn_standard(x)
        ffn_quantum_out = self.ffn_quantum(x, time_steps=2)
        
        # ãƒ–ãƒ¬ãƒ³ãƒ‰æ¯”ç‡: æ¨™æº–FFN 70% + é‡å­æ‹¡å¼µ 30%
        ffn_out = 0.7 * ffn_standard_out + 0.3 * ffn_quantum_out
        
        x = residual + ffn_out
        
        return x


# ========================================
# ãƒ‹ãƒ¥ãƒ¼ãƒ­Q Brain ãƒ¢ãƒ‡ãƒ«
# ========================================

class NeuroQuantumBrain(nn.Module):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q Brain - GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformerï¼ˆè„³å‹é‡å­æ‹¡å¼µç‰ˆï¼‰
    
    å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆå›³2-4ã«æº–æ‹ ï¼‰:
    1. å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åŒ– â†’ ãƒˆãƒ¼ã‚¯ãƒ³ID
    2. ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆText Embedding + Position Embeddingï¼‰
    3. ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° â†’ GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformerï¼ˆNå€‹ã®Decoder Blocksï¼‰
    4. Transformerå‡ºåŠ› â†’ å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆFinal LayerNorm + Output Headï¼‰
    5. å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ— â†’ å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰
    
    GPTæ¨™æº–æ§‹é€ :
    - Text Embedding + Position Embeddingï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
    - Dropout
    - Nå€‹ã®GPT Decoder Blocksï¼ˆPre-norm + Attention + FFNï¼‰
    - Final LayerNorm
    - Output Head (Linear to vocab_size)
    """
    
    def __init__(self, vocab_size: int, embed_dim: int = 128,
                 num_heads: int = 4, num_layers: int = 3,
                 num_neurons: int = 48, max_seq_len: int = 256,
                 dropout: float = 0.1, ffn_expansion: int = 4,
                 use_openai_embedding: bool = False,
                 openai_api_key: Optional[str] = None,
                 openai_model: str = "text-embedding-ada-002",
                 tokenizer = None):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.max_seq_len = max_seq_len
        self.use_openai_embedding = use_openai_embedding
        
        # GPTæ¨™æº–: Text Embedding + Position Embedding
        if use_openai_embedding:
            if not OPENAI_AVAILABLE:
                warnings.warn("OpenAI APIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å¾“æ¥ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                self.use_openai_embedding = False
        
        if self.use_openai_embedding:
            self.openai_wrapper = OpenAIEmbeddingWrapper(
                api_key=openai_api_key,
                model=openai_model
            )
            actual_embed_dim = self.openai_wrapper.embed_dim
            if actual_embed_dim != embed_dim:
                warnings.warn(
                    f"OpenAI Embeddingæ¬¡å…ƒ({actual_embed_dim})ãŒè¨­å®šæ¬¡å…ƒ({embed_dim})ã¨ç•°ãªã‚Šã¾ã™ã€‚"
                    f"å°„å½±å±¤ã‚’è¿½åŠ ã—ã¾ã™ã€‚"
                )
                self.projection = nn.Linear(actual_embed_dim, embed_dim)
                self.embed_dim = embed_dim  # å‡ºåŠ›æ¬¡å…ƒã¯è¨­å®šå€¤ã‚’ä½¿ç”¨
            else:
                self.projection = nn.Identity()
            self.text_embedding = None
            self.tokenizer = tokenizer
        else:
            self.text_embedding = nn.Embedding(vocab_size, embed_dim)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
            self.openai_wrapper = None
            self.projection = None
            self.tokenizer = None
        
        # Position Embeddingï¼ˆOpenAIä½¿ç”¨æ™‚ã‚‚å¿…è¦ï¼‰
        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)
        
        # GPT Decoder Blocks
        self.blocks = nn.ModuleList([
            BrainQuantumBlock(
                embed_dim=embed_dim,
                num_heads=num_heads,
                num_neurons=num_neurons,
                dropout=dropout,
                ffn_expansion=ffn_expansion
            ) for _ in range(num_layers)
        ])
        
        # GPTæ¨™æº–: Final LayerNorm
        self.final_norm = nn.LayerNorm(embed_dim)
        
        # GPTæ¨™æº–: Output Head (weight tyingå¯èƒ½ã ãŒã€ã“ã“ã§ã¯ç‹¬ç«‹)
        self.output_head = nn.Linear(embed_dim, vocab_size, bias=False)
        
        # GPTæ¨™æº–: Embedding Dropout
        self.dropout = nn.Dropout(dropout)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–ï¼ˆGPTæ¨™æº–ï¼‰
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """GPTæ¨™æº–ã®é‡ã¿åˆæœŸåŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformer ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ï¼ˆå›³2-4ã®ãƒ•ãƒ­ãƒ¼ã«æº–æ‹ ï¼‰
        
        å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—:
        1. ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆText Embedding + Position Embeddingï¼‰
        2. ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° â†’ GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformer
        3. Transformerå‡ºåŠ› â†’ å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆFinal LayerNorm + Output Headï¼‰
        4. å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ— â†’ ãƒ­ã‚¸ãƒƒãƒˆï¼ˆå‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ï¼‰
        
        Args:
            x: (batch, seq) ãƒˆãƒ¼ã‚¯ãƒ³IDï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            mask: Optional attention mask (Noneã®å ´åˆã¯Causal Maskã‚’è‡ªå‹•ç”Ÿæˆ)
        
        Returns:
            (batch, seq, vocab_size) ãƒ­ã‚¸ãƒƒãƒˆï¼ˆå¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—å¾Œã®å‡ºåŠ›ï¼‰
        """
        batch, seq = x.shape
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        if self.use_openai_embedding and self.openai_wrapper is not None:
            # OpenAI Embeddingã‚’ä½¿ç”¨
            if self.tokenizer is not None:
                # ãƒˆãƒ¼ã‚¯ãƒ³IDã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å¾©å…ƒ
                texts = []
                for batch_idx in range(batch):
                    token_seq = x[batch_idx].cpu().tolist()
                    text = self.tokenizer.decode(token_seq)
                    texts.append(text)
            else:
                raise ValueError(
                    "OpenAI Embeddingä½¿ç”¨æ™‚ã¯ã€tokenizerãŒå¿…è¦ã§ã™ã€‚"
                )
            
            # OpenAI APIã‹ã‚‰ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—
            embeddings_list = []
            for text in texts:
                embedding = self.openai_wrapper.get_embeddings([text])[0]
                embeddings_list.append(embedding)
            
            # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            text_embeds = torch.tensor(
                np.array(embeddings_list), 
                device=x.device, 
                dtype=torch.float32
            )
            
            # æ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã¯å°„å½±
            text_embeds = self.projection(text_embeds)
            
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«åˆã‚ã›ã¦æ‹¡å¼µï¼ˆæ–‡å…¨ä½“ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å„ãƒˆãƒ¼ã‚¯ãƒ³ã«é©ç”¨ï¼‰
            if text_embeds.dim() == 2:
                text_embeds = text_embeds.unsqueeze(1).expand(-1, seq, -1)
        else:
            # Text Embedding: ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
            text_embeds = self.text_embedding(x)  # (batch, seq, embed_dim)
        
        # Position Embedding: ä½ç½®æƒ…å ±ã‚’è¿½åŠ 
        positions = torch.arange(seq, device=x.device).unsqueeze(0).expand(batch, -1)
        pos_embeds = self.position_embedding(positions)  # (batch, seq, embed_dim)
        # åŸ‹ã‚è¾¼ã¿ã®åˆæˆ + Dropout
        h = self.dropout(text_embeds + pos_embeds)
        
        # Causal Maskç”Ÿæˆï¼ˆmaskãŒNoneã®å ´åˆï¼‰
        if mask is None:
            mask = torch.tril(torch.ones(seq, seq, device=x.device)).unsqueeze(0).unsqueeze(0)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° â†’ GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformer
        # Nå€‹ã®GPT Decoder Blocksï¼ˆPre-norm + Multi-Head Causal Self-Attention + FFNï¼‰
        for block in self.blocks:
            h = block(h, mask)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: Transformerå‡ºåŠ› â†’ å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—
        # Final LayerNorm
        h = self.final_norm(h)
        # Output Head: ãƒ™ã‚¯ãƒˆãƒ« â†’ èªå½™ç¢ºç‡ã¸ã®å¤‰æ›
        logits = self.output_head(h)  # (batch, seq, vocab_size)
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ­ã‚¸ãƒƒãƒˆï¼ˆå‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ï¼‰
        return logits
    
    def get_quantum_report(self) -> str:
        """é‡å­çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ"""
        report = "\nâš›ï¸ é‡å­çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ\n" + "-" * 40 + "\n"
        
        for i, block in enumerate(self.blocks):
            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®çµ±è¨ˆ
            attn_stats = block.attention.brain_layer.get_quantum_stats()
            # FFNã®çµ±è¨ˆï¼ˆæ–°ã—ã„æ§‹é€ ã§ã¯ffn_quantumã‚’ä½¿ç”¨ï¼‰
            ffn_stats = block.ffn_quantum.get_quantum_stats()
            
            report += f"Block {i}:\n"
            report += f"  Attention: r={attn_stats['r_mean']:.3f}, T={attn_stats['T_mean']:.3f}, Î»={attn_stats['lambda']:.3f}\n"
            report += f"  FFN:       r={ffn_stats['r_mean']:.3f}, T={ffn_stats['T_mean']:.3f}, Î»={ffn_stats['lambda']:.3f}\n"
        
        return report
    
    @torch.no_grad()
    def generate(self, start_tokens: torch.Tensor, max_length: int = 50,
                 temperature_min: float = 0.4, temperature_max: float = 0.9,
                 top_k: int = 40, top_p: float = 0.9,
                 repetition_penalty: float = 1.2) -> torch.Tensor:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆæ¸©åº¦ç¯„å›²åˆ¶ç´„ç‰ˆï¼‰
        
        Args:
            temperature_min: æœ€å°æ¸©åº¦ï¼ˆÎ¸=Ï€/4ã®ã¨ãã€2Î¸=90Â°ï¼‰
            temperature_max: æœ€å¤§æ¸©åº¦
            â€» 2Î¸ãŒ45Â°ã€œ135Â°ã®ç¯„å›²ã§å‹•ãã‚ˆã†ã«åˆ¶ç´„
        """
        self.eval()
        
        tokens = start_tokens.clone()
        generated = []
        
        for step in range(max_length):
            # å…¥åŠ›æº–å‚™
            x = tokens.unsqueeze(0) if tokens.dim() == 1 else tokens
            if x.size(1) > self.max_seq_len:
                x = x[:, -self.max_seq_len:]
            
            # äºˆæ¸¬
            logits = self(x)
            
            # é‡å­çŠ¶æ…‹ã‹ã‚‰æ¸©åº¦ã‚’å‹•çš„ã«è¨ˆç®—
            # 2Î¸ãŒ45Â°ã€œ135Â°ã®ç¯„å›²ï¼ˆÏ€/4ã€œ3Ï€/4ï¼‰ã«ãªã‚‹ã‚ˆã†ã«åˆ¶ç´„
            if len(self.blocks) > 0:
                # ãƒ–ãƒ­ãƒƒã‚¯ã®Î¸ã‹ã‚‰ç›¸é–¢ä¿‚æ•°rã‚’å–å¾—ï¼ˆæ–°ã—ã„æ§‹é€ ã§ã¯ffn_quantumã‚’ä½¿ç”¨ï¼‰
                r_vals = []
                for block in self.blocks:
                    r_vals.append(block.ffn_quantum.get_r().mean().item())
                r_mean = np.mean(r_vals)
                
                # r âˆˆ [-1, 1] ã‚’æ¸©åº¦ç¯„å›²ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                # r = cos(2Î¸) ãªã®ã§ã€2Î¸ âˆˆ [Ï€/4, 3Ï€/4] ã®ã¨ã r âˆˆ [-0.707, 0.707]
                # ã“ã‚Œã‚’ [temperature_min, temperature_max] ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                r_clamped = np.clip(r_mean, -0.707, 0.707)
                # æ­£è¦åŒ–: [-0.707, 0.707] â†’ [0, 1]
                t_normalized = (r_clamped + 0.707) / 1.414
                # æ¸©åº¦ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                temperature = temperature_min + t_normalized * (temperature_max - temperature_min)
            else:
                temperature = (temperature_min + temperature_max) / 2
            
            next_logits = logits[0, -1] / temperature
            
            # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
            if len(generated) > 0:
                for prev_token in set(generated[-20:]):
                    next_logits[prev_token] /= repetition_penalty
            
            # é‡å­ã‚†ã‚‰ãã‚’è¿½åŠ ï¼ˆåˆ¶ç´„ã•ã‚ŒãŸç¯„å›²ã§ï¼‰
            if len(self.blocks) > 0:
                T_mean = self.blocks[-1].ffn_quantum.get_T().mean()
                # T = |sin(2Î¸)|, 2Î¸ âˆˆ [45Â°, 135Â°] ãªã‚‰ T âˆˆ [0.707, 1.0]
                T_clamped = max(0.707, min(1.0, T_mean.item()))
                quantum_noise = torch.randn_like(next_logits) * T_clamped * 0.15
                next_logits = next_logits + quantum_noise
            
            # Top-K ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_k > 0:
                indices_to_remove = next_logits < torch.topk(next_logits, top_k)[0][-1]
                next_logits[indices_to_remove] = float('-inf')
            
            # Top-P ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = 0
                
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_logits[indices_to_remove] = float('-inf')
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, 1)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³é€£çµ
            if tokens.dim() == 0:
                tokens = next_token.view(1)
            else:
                tokens = torch.cat([tokens, next_token.view(-1)], dim=0)
            generated.append(next_token.item())
        
        return tokens


# ========================================
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆtransformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰
# ========================================

class BrainTokenizer:
    """
    SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
    
    - èªå½™ã‚µã‚¤ã‚ºã‚’æŒ‡å®šã—ã¦å­¦ç¿’å¯èƒ½ï¼ˆ8000-32000æ¨å¥¨ï¼‰
    - BPEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ï¼ˆæ—¥æœ¬èªã«é©ã—ã¦ã„ã‚‹ï¼‰
    - ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãŒå¯èƒ½
    - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆSentencePieceæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ï¼‰
    """
    
    def __init__(self, vocab_size: int = 16000, model_file: str = None):
        """
        Args:
            vocab_size: èªå½™ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 16000ï¼‰
            model_file: æ—¢å­˜ã®SentencePieceãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯æ–°è¦å­¦ç¿’ï¼‰
        """
        self.vocab_size = vocab_size
        self.actual_vocab_size = None
        self.model_file = model_file
        self.sp_model = None
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.pad_token = '<pad>'
        self.unk_token = '<unk>'
        self.bos_token = '<s>'
        self.eos_token = '</s>'
        
        # SentencePieceã‚’ä½¿ç”¨
        if SENTENCEPIECE_AVAILABLE:
            if model_file and os.path.exists(model_file):
                # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
                try:
                    self.sp_model = spm.SentencePieceProcessor()
                    self.sp_model.load(model_file)
                    self.actual_vocab_size = self.sp_model.get_piece_size()
                    self.vocab_size = self.actual_vocab_size
                    print(f"   âœ… SentencePieceãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_file} (èªå½™ã‚µã‚¤ã‚º: {self.actual_vocab_size})")
                except Exception as e:
                    warnings.warn(f"SentencePieceãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}ã€‚æ–°è¦å­¦ç¿’ã—ã¾ã™ã€‚")
                    self.sp_model = None
        else:
            # SentencePieceæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
            self.sp_model = None
        
        # SentencePieceãŒä½¿ãˆãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if self.sp_model is None:
            self._init_fallback()
    
    def _init_fallback(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–"""
        self.token2idx = {'<PAD>': 0, '<UNK>': 1, '<BOS>': 2, '<EOS>': 3}
        self.idx2token = {0: '<PAD>', 1: '<UNK>', 2: '<BOS>', 3: '<EOS>'}
        # vocab_sizeã¯è¨­å®šã•ã‚Œã¦ã„ã‚‹å€¤ã‚’ä¿æŒï¼ˆä¸Šæ›¸ãã—ãªã„ï¼‰
        if not hasattr(self, 'vocab_size') or self.vocab_size is None:
            self.vocab_size = 4
        self.actual_vocab_size = None  # fit()ã§è¨­å®šã•ã‚Œã‚‹
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ID
        self.pad_id = 0
        self.unk_id = 1
        self.bos_id = 2
        self.eos_id = 3
    
    def fit(self, texts: List[str], character_coverage: float = 0.9995, model_prefix: str = "spm_model_brain"):
        """
        SentencePieceã§èªå½™ã‚’å­¦ç¿’
        
        Args:
            texts: å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            character_coverage: æ–‡å­—ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆ0.9995ãŒæ¨å¥¨ã€æ—¥æœ¬èªã®å ´åˆã¯0.9995-0.99995ï¼‰
            model_prefix: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        """
        if not SENTENCEPIECE_AVAILABLE:
            warnings.warn("SentencePieceãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self._fit_fallback(texts)
            return
        
        print(f"   ğŸ”¤ SentencePieceã§èªå½™å­¦ç¿’ä¸­... (ç›®æ¨™èªå½™ã‚µã‚¤ã‚º: {self.vocab_size})")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            temp_file = f.name
            for text in texts:
                f.write(text + '\n')
        
        try:
            # SentencePieceå­¦ç¿’
            spm.SentencePieceTrainer.train(
                input=temp_file,
                model_prefix=model_prefix,
                vocab_size=self.vocab_size,
                character_coverage=character_coverage,
                model_type='bpe',  # BPEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                pad_id=0,
                unk_id=1,
                bos_id=2,
                eos_id=3,
                pad_piece=self.pad_token,
                unk_piece=self.unk_token,
                bos_piece=self.bos_token,
                eos_piece=self.eos_token,
            )
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            model_file_path = model_prefix + '.model'
            self.sp_model = spm.SentencePieceProcessor()
            self.sp_model.load(model_file_path)
            self.actual_vocab_size = self.sp_model.get_piece_size()
            self.vocab_size = self.actual_vocab_size
            self.model_file = model_file_path
            
            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—
            self.pad_id = self.sp_model.pad_id()
            self.unk_id = self.sp_model.unk_id()
            self.bos_id = self.sp_model.bos_id()
            self.eos_id = self.sp_model.eos_id()
            
            print(f"   âœ… SentencePieceèªå½™å­¦ç¿’å®Œäº† (èªå½™ã‚µã‚¤ã‚º: {self.actual_vocab_size})")
            
        except Exception as e:
            warnings.warn(f"SentencePieceå­¦ç¿’ã«å¤±æ•—: {e}ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self._fit_fallback(texts)
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(temp_file):
                os.unlink(temp_file)
            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä»¥å¤–ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            for ext in ['.vocab']:
                temp_file_ext = model_prefix + ext
                if os.path.exists(temp_file_ext):
                    os.unlink(temp_file_ext)
    
    def _fit_fallback(self, texts: List[str]):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç°¡æ˜“èªå½™æ§‹ç¯‰"""
        print(f"   ğŸ”¤ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯èªå½™æ§‹ç¯‰ä¸­...")
        char_counts = Counter()
        for text in texts:
            char_counts.update(list(text))
        
        # vocab_sizeãŒ4ä»¥ä¸‹ï¼ˆç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼‰ã®å ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        target_vocab_size = max(self.vocab_size, 16000) if self.vocab_size <= 4 else self.vocab_size
        
        for char, _ in char_counts.most_common(target_vocab_size - 4):
            if char not in self.token2idx:
                idx = len(self.token2idx)
                self.token2idx[char] = idx
                self.idx2token[idx] = char
        
        self.vocab_size = len(self.token2idx)
        self.actual_vocab_size = self.vocab_size
        print(f"   âœ… èªå½™ã‚µã‚¤ã‚º: {self.vocab_size}")
    
    def encode(self, text: str, add_special: bool = False) -> List[int]:
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        if self.sp_model is not None:
            # SentencePieceä½¿ç”¨
            if add_special:
                return self.sp_model.encode(text, out_type=int, add_bos=True, add_eos=True)
            else:
                return self.sp_model.encode(text, out_type=int, add_bos=False, add_eos=False)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœ€é•·ãƒãƒƒãƒæ–¹å¼
            tokens = []
            if add_special:
                tokens.append(self.bos_id)
            
            i = 0
            text_len = len(text)
            while i < text_len:
                matched = False
                for length in range(min(8, text_len - i), 0, -1):
                    substr = text[i:i+length]
                    if substr in self.token2idx:
                        tokens.append(self.token2idx[substr])
                        i += length
                        matched = True
                        break
                
                if not matched:
                    tokens.append(self.token2idx.get(text[i], self.unk_id))
                    i += 1
            
            if add_special:
                tokens.append(self.eos_id)
            
            return tokens
    
    def decode(self, tokens: List[int], skip_special: bool = True) -> str:
        """ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        if self.sp_model is not None:
            # SentencePieceä½¿ç”¨
            if skip_special:
                # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
                special_ids = {self.pad_id, self.eos_id, self.bos_id, self.unk_id}
                tokens = [t for t in tokens if t not in special_ids]
            return self.sp_model.decode(tokens)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            result = []
            special_ids = {self.pad_id, self.unk_id, self.bos_id, self.eos_id}
            for t in tokens:
                if skip_special and t in special_ids:
                    continue
                token = self.idx2token.get(t, '')
                result.append(token)
            return ''.join(result)


# ========================================
# ãƒ‹ãƒ¥ãƒ¼ãƒ­Q Brain AI
# ========================================

class NeuroQuantumBrainAI:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q Brain ç”ŸæˆAI
    
    OpenAI Embeddingä½¿ç”¨ä¾‹:
        # OpenAI Embeddingã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
        ai = NeuroQuantumBrainAI(
            embed_dim=3072,  # text-embedding-3-largeã®æ¬¡å…ƒ
            use_openai_embedding=True,
            openai_api_key="sk-...",  # ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°OPENAI_API_KEY
            openai_model="text-embedding-3-large"
        )
        
        # å¾“æ¥ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        ai = NeuroQuantumBrainAI(embed_dim=128)
    """
    
    def __init__(self, embed_dim: int = 128, num_heads: int = 4,
                 num_layers: int = 3, num_neurons: int = 75,
                 max_vocab: int = 50000,
                 use_openai_embedding: bool = False,
                 openai_api_key: Optional[str] = None,
                 openai_model: str = "text-embedding-3-large"):
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_neurons = num_neurons
        self.max_vocab = max_vocab
        self.use_openai_embedding = use_openai_embedding
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.openai_model = openai_model
        
        # SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå­¦ç¿’æ™‚ã«æ§‹ç¯‰ã•ã‚Œã‚‹ï¼‰
        self.tokenizer = None  # train()ãƒ¡ã‚½ãƒƒãƒ‰ã§æ§‹ç¯‰ã•ã‚Œã‚‹
        self.model = None
        
        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ: MPS (Apple Silicon) > CUDA > CPU
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print("ğŸ® NVIDIA GPU (CUDA) ã‚’ä½¿ç”¨")
        else:
            self.device = torch.device("cpu")
            print("ğŸ’» CPU ã‚’ä½¿ç”¨")
    
    def train(self, texts: List[str], epochs: int = 20, batch_size: int = 16,
              lr: float = 0.001, seq_length: int = 64):
        """å­¦ç¿’"""
        print("\nğŸ“ å­¦ç¿’é–‹å§‹...")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰ï¼ˆSentencePieceä½¿ç”¨ï¼‰
        print("\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰...")
        
        # SentencePieceã§èªå½™ã‚’å­¦ç¿’
        self.tokenizer = BrainTokenizer(vocab_size=self.max_vocab)
        self.tokenizer.fit(texts)
        
        print(f"   èªå½™ã‚µã‚¤ã‚º: {self.tokenizer.actual_vocab_size}")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        all_tokens = []
        for text in texts:
            tokens = self.tokenizer.encode(text)
            if len(tokens) > 2:
                all_tokens.extend(tokens)
        
        print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens)}")
        
        # OpenAI Embeddingä½¿ç”¨æ™‚ã¯åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’èª¿æ•´
        if self.use_openai_embedding:
            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒ
            if "ada-002" in self.openai_model:
                actual_embed_dim = 1536
            elif "embedding-3-large" in self.openai_model:
                actual_embed_dim = 3072
            elif "embedding-3-small" in self.openai_model:
                actual_embed_dim = 1536
            else:
                actual_embed_dim = 3072  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            if self.embed_dim != actual_embed_dim:
                print(f"   OpenAI Embeddingæ¬¡å…ƒ({actual_embed_dim})ã«åˆã‚ã›ã¦èª¿æ•´")
                self.embed_dim = actual_embed_dim
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        vocab_size = self.tokenizer.actual_vocab_size if self.tokenizer.actual_vocab_size else self.tokenizer.vocab_size
        self.model = NeuroQuantumBrain(
            vocab_size=vocab_size,
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            num_layers=self.num_layers,
            num_neurons=self.num_neurons,
            max_seq_len=256,
            dropout=0.1,
            use_openai_embedding=self.use_openai_embedding,
            openai_api_key=self.openai_api_key,
            openai_model=self.openai_model,
            tokenizer=self.tokenizer if self.use_openai_embedding else None
        ).to(self.device)
        
        if self.use_openai_embedding:
            print(f"   OpenAI Embeddingä½¿ç”¨: {self.openai_model}")
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss(ignore_index=0)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        sequences = []
        for i in range(0, len(all_tokens) - seq_length - 1, seq_length // 2):
            x = all_tokens[i:i+seq_length]
            y = all_tokens[i+1:i+seq_length+1]
            if len(x) == seq_length:
                sequences.append((x, y))
        
        print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences)}")
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            random.shuffle(sequences)
            
            for i in range(0, len(sequences), batch_size):
                batch = sequences[i:i+batch_size]
                
                x = torch.tensor([s[0] for s in batch], dtype=torch.long).to(self.device)
                y = torch.tensor([s[1] for s in batch], dtype=torch.long).to(self.device)
                
                optimizer.zero_grad()
                logits = self.model(x)
                vocab_size = self.tokenizer.actual_vocab_size if self.tokenizer.actual_vocab_size else self.tokenizer.vocab_size
                loss = criterion(logits.view(-1, vocab_size), y.view(-1))
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / max(len(sequences) // batch_size, 1)
            
            if (epoch + 1) % 5 == 0 or epoch == 0:
                print(f"   Epoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}")
        
        print("   å­¦ç¿’å®Œäº†ï¼")
    
    def generate(self, prompt: str, max_length: int = 50,
                 temperature_min: float = 0.4, temperature_max: float = 0.9,
                 top_k: int = 40, top_p: float = 0.9) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆæ¸©åº¦ç¯„å›²åˆ¶ç´„ç‰ˆï¼‰
        
        Args:
            temperature_min: æœ€å°æ¸©åº¦ï¼ˆ2Î¸=90Â°ä»˜è¿‘ï¼‰
            temperature_max: æœ€å¤§æ¸©åº¦ï¼ˆ2Î¸ãŒ45Â°or135Â°ä»˜è¿‘ï¼‰
            â€» 2Î¸ãŒ45Â°ã€œ135Â°ã®ç¯„å›²ã§å‹•ãã“ã¨ã§ã€é©åº¦ãªæºã‚‰ãã‚’ç¶­æŒ
        """
        if self.model is None:
            return "ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        self.model.eval()
        
        tokens = self.tokenizer.encode(prompt)
        if len(tokens) == 0:
            tokens = [2]  # <BOS>
        
        tokens = torch.tensor(tokens, dtype=torch.long).to(self.device)
        
        generated = self.model.generate(
            tokens, max_length=max_length,
            temperature_min=temperature_min, temperature_max=temperature_max,
            top_k=top_k, top_p=top_p
        )
        
        return self.tokenizer.decode(generated.cpu().tolist())
    
    def get_report(self) -> str:
        """ãƒ¢ãƒ‡ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ"""
        if self.model is None:
            return "ãƒ¢ãƒ‡ãƒ«ãªã—"
        
        return self.model.get_quantum_report()


# ========================================
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
# ========================================

def fetch_huggingface_data(max_samples: int = 5000) -> List[str]:
    """Hugging Faceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    texts = []
    
    try:
        from datasets import load_dataset
        print("   ğŸ“¡ Hugging Faceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
        
        # æ—¥æœ¬èªWikipedia
        try:
            ds = load_dataset("range3/wiki40b-ja", split="train", streaming=True)
            count = 0
            for item in ds:
                if 'text' in item and len(item['text']) > 50:
                    texts.append(item['text'][:500])
                    count += 1
                    if count >= max_samples // 3:
                        break
        except:
            pass
        
        # è‹±èªãƒ‡ãƒ¼ã‚¿
        try:
            ds = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
            for item in ds[:max_samples // 3]:
                if 'text' in item and len(item['text']) > 30:
                    texts.append(item['text'])
        except:
            pass
        
        # æ—¥æœ¬èªå¯¾è©±
        try:
            ds = load_dataset("kunishou/databricks-dolly-15k-ja", split="train")
            for item in ds[:max_samples // 3]:
                if 'output' in item:
                    texts.append(item['output'])
        except:
            pass
        
        print(f"   âœ… {len(texts):,} ã‚µãƒ³ãƒ—ãƒ«å–å¾—")
        
    except ImportError:
        print("   âš ï¸ datasetsæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€çµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨")
    except Exception as e:
        print(f"   âš ï¸ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {e}")
    
    return texts


def get_training_data(use_huggingface: bool = False) -> List[str]:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆ50,000ãƒˆãƒ¼ã‚¯ãƒ³å¯¾å¿œå¤§è¦æ¨¡ç‰ˆï¼‰"""
    
    base_texts = [
        # ===== é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° =====
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦æƒ…å ±ã‚’å‡¦ç†ã™ã‚‹é©æ–°çš„ãªè¨ˆç®—æ©Ÿã§ã™ã€‚",
        "é‡å­ãƒ“ãƒƒãƒˆã¯0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®æ€§è³ªã«ã‚ˆã‚Šä¸¦åˆ—è¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚",
        "é‡å­ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¯ã€è¤‡æ•°ã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ãŸçŠ¶æ…‹ã§ã™ã€‚é‡å­é€šä¿¡ã‚„é‡å­æš—å·ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯æš—å·è§£èª­ã‚„æœ€é©åŒ–å•é¡Œã§æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚å°†æ¥çš„ã«ã¯å‰µè–¬ã‚„ææ–™é–‹ç™ºã«ã‚‚è²¢çŒ®ã™ã‚‹ã§ã—ã‚‡ã†ã€‚",
        "é‡å­è¶…è¶Šæ€§ã¨ã¯ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒå¤å…¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚é«˜é€Ÿã«è¨ˆç®—ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™æ¦‚å¿µã§ã™ã€‚",
        "é‡å­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯ã€ã‚·ãƒ§ã‚¢ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„ã‚°ãƒ­ãƒ¼ãƒãƒ¼ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãªã©ãŒã‚ã‚Šã¾ã™ã€‚",
        "é‡å­èª¤ã‚Šè¨‚æ­£ã¯ã€é‡å­è¨ˆç®—ã®ä¿¡é ¼æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã®é‡è¦ãªæŠ€è¡“ã§ã™ã€‚",
        "è¶…ä¼å°é‡å­ãƒ“ãƒƒãƒˆã‚„ã‚¤ã‚ªãƒ³ãƒˆãƒ©ãƒƒãƒ—é‡å­ãƒ“ãƒƒãƒˆãªã©ã€æ§˜ã€…ãªå®Ÿè£…æ–¹å¼ãŒç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "é‡å­ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ã¯ã€çµ„ã¿åˆã‚ã›æœ€é©åŒ–å•é¡Œã‚’è§£ããŸã‚ã®é‡å­è¨ˆç®—æ‰‹æ³•ã§ã™ã€‚",
        "é‡å­ãƒ†ãƒ¬ãƒãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€é‡å­æƒ…å ±ã‚’é›¢ã‚ŒãŸå ´æ‰€ã«è»¢é€ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "é‡å­æš—å·é€šä¿¡ã¯ã€ç›—è´ãŒç†è«–çš„ã«ä¸å¯èƒ½ãªé€šä¿¡æ–¹å¼ã§ã™ã€‚",
        "é‡å­ã‚»ãƒ³ã‚µãƒ¼ã¯ã€æ¥µã‚ã¦é«˜ç²¾åº¦ãªæ¸¬å®šã‚’å¯èƒ½ã«ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        
        # ===== ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ =====
        "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€äººé–“ã®è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚å…¥åŠ›å±¤ã€éš ã‚Œå±¤ã€å‡ºåŠ›å±¤ã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã§ã™ã€‚ç”»åƒèªè­˜ã‚„éŸ³å£°èªè­˜ã§å¤§ããªæˆåŠŸã‚’åã‚ã¦ã„ã¾ã™ã€‚",
        "æ³¨æ„æ©Ÿæ§‹ã¯ã€å…¥åŠ›ã®é‡è¦ãªéƒ¨åˆ†ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹æŠ€è¡“ã§ã™ã€‚æ©Ÿæ¢°ç¿»è¨³ã®å“è³ªã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã—ãŸã€‚",
        "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã‚’ç”¨ã„ãŸé©æ–°çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚GPTã‚„BERTã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚",
        "ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ç”»åƒèªè­˜ã«ç‰¹åŒ–ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ã¦ç‰¹å¾´ã‚’æŠ½å‡ºã—ã¾ã™ã€‚",
        "å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ã®ã«é©ã—ã¦ã„ã¾ã™ã€‚éå»ã®æƒ…å ±ã‚’è¨˜æ†¶ã—ã¦åˆ©ç”¨ã§ãã¾ã™ã€‚",
        "é•·çŸ­æœŸè¨˜æ†¶ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€é•·æœŸçš„ãªä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã§ãã‚‹RNNã®ä¸€ç¨®ã§ã™ã€‚",
        "ãƒãƒƒãƒæ­£è¦åŒ–ã¯ã€å­¦ç¿’ã‚’å®‰å®šã•ã›ã‚‹ãŸã‚ã®æŠ€è¡“ã§ã™ã€‚å„å±¤ã®å…¥åŠ›ã‚’æ­£è¦åŒ–ã—ã¾ã™ã€‚",
        "ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã¯ã€éå­¦ç¿’ã‚’é˜²ããŸã‚ã®æ­£å‰‡åŒ–æ‰‹æ³•ã§ã™ã€‚ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™ã€‚",
        "æ´»æ€§åŒ–é–¢æ•°ã«ã¯ã€ReLUã€ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã€tanhãªã©ãŒã‚ã‚Šã¾ã™ã€‚éç·šå½¢æ€§ã‚’å°å…¥ã™ã‚‹é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚",
        "é€†ä¼æ’­æ³•ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ã«ä½¿ã‚ã‚Œã‚‹å‹¾é…è¨ˆç®—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚",
        "ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ã¯ã€ãƒŸãƒ‹ãƒãƒƒãƒã‚’ä½¿ã£ã¦åŠ¹ç‡çš„ã«å­¦ç¿’ã‚’è¡Œã†æœ€é©åŒ–æ‰‹æ³•ã§ã™ã€‚",
        "æ®‹å·®æ¥ç¶šã¯ã€æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å‹¾é…æ¶ˆå¤±å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚",
        "åŸ‹ã‚è¾¼ã¿å±¤ã¯ã€é›¢æ•£çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’é€£ç¶šçš„ãªãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«å¤‰æ›ã—ã¾ã™ã€‚",
        
        # ===== äººå·¥çŸ¥èƒ½ =====
        "äººå·¥çŸ¥èƒ½ã¯ã€äººé–“ã®çŸ¥çš„æ´»å‹•ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å®Ÿç¾ã—ã‚ˆã†ã¨ã™ã‚‹æŠ€è¡“ã§ã™ã€‚æ§˜ã€…ãªåˆ†é‡ã§å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã¦æ”¹å–„ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã—ã¾ã™ã€‚æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ãªã©ãŒã‚ã‚Šã¾ã™ã€‚",
        "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®è¨€èªã‚’ç†è§£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç¿»è¨³ã€è¦ç´„ã€è³ªå•å¿œç­”ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "ç”ŸæˆAIã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚„ç”»åƒã‚’ç”Ÿæˆã§ãã‚‹äººå·¥çŸ¥èƒ½ã§ã™ã€‚å‰µé€ çš„ãªã‚¿ã‚¹ã‚¯ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚",
        "å¼·åŒ–å­¦ç¿’ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ã‚²ãƒ¼ãƒ ã‚„ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "è»¢ç§»å­¦ç¿’ã¯ã€ã‚ã‚‹èª²é¡Œã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’åˆ¥ã®èª²é¡Œã«é©ç”¨ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ã‚‚åŠ¹æœçš„ã«å­¦ç¿’ã§ãã¾ã™ã€‚",
        "æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¯ã€æ­£è§£ãƒ©ãƒ™ãƒ«ä»˜ãã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚åˆ†é¡ã‚„å›å¸°å•é¡Œã«ä½¿ã‚ã‚Œã¾ã™ã€‚",
        "æ•™å¸«ãªã—å­¦ç¿’ã¯ã€ãƒ©ãƒ™ãƒ«ãªã—ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚„æ¬¡å…ƒå‰Šæ¸›ã«ä½¿ã‚ã‚Œã¾ã™ã€‚",
        "åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã¯ã€å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã¨å¤§é‡ã®ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦å­¦ç¿’ã—ã¾ã™ã€‚",
        "ãƒ¡ã‚¿å­¦ç¿’ã¯ã€å­¦ç¿’ã®ä»•æ–¹ã‚’å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«ç´ æ—©ãé©å¿œã§ãã¾ã™ã€‚",
        "èª¬æ˜å¯èƒ½ãªAIã¯ã€AIã®åˆ¤æ–­ç†ç”±ã‚’äººé–“ã«èª¬æ˜ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "ãƒ•ã‚§ãƒ‡ãƒ¬ãƒ¼ãƒ†ãƒƒãƒ‰ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’å…±æœ‰ã›ãšã«åˆ†æ•£å­¦ç¿’ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚’ä¿è­·ã—ã¾ã™ã€‚",
        "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãªã©è¤‡æ•°ã®å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆçš„ã«å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "çŸ¥è­˜ã‚°ãƒ©ãƒ•ã¯ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚ã‚’è¡¨ç¾ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã™ã€‚",
        
        # ===== ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° =====
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«å‘½ä»¤ã‚’ä¸ãˆã‚‹ãŸã‚ã®è¨€èªã‚’ä½¿ã£ãŸæ´»å‹•ã§ã™ã€‚å‰µé€ çš„ã§è«–ç†çš„ãªæ€è€ƒãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚",
        "Pythonã¯ã€èª­ã¿ã‚„ã™ãæ›¸ãã‚„ã™ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„ãƒ‡ãƒ¼ã‚¿åˆ†æã§åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚",
        "JavaScriptã¯ã€ã‚¦ã‚§ãƒ–é–‹ç™ºã§æœ€ã‚‚åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚",
        "Rustã¯ã€å®‰å…¨æ€§ã¨æ€§èƒ½ã‚’ä¸¡ç«‹ã—ãŸæ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚",
        "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®æ‰‹é †ã‚’å®šç¾©ã—ãŸã‚‚ã®ã§ã™ã€‚åŠ¹ç‡çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯è¨ˆç®—æ™‚é–“ã‚’çŸ­ç¸®ã—ã¾ã™ã€‚",
        "ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«æ ¼ç´ãƒ»æ“ä½œã™ã‚‹ãŸã‚ã®ä»•çµ„ã¿ã§ã™ã€‚é…åˆ—ã€ãƒªã‚¹ãƒˆã€æœ¨ã€ã‚°ãƒ©ãƒ•ãªã©ãŒã‚ã‚Šã¾ã™ã€‚",
        "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã€ãƒ‡ãƒ¼ã‚¿ã¨å‡¦ç†ã‚’ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ã¾ã¨ã‚ã‚‹è¨­è¨ˆæ‰‹æ³•ã§ã™ã€‚",
        "é–¢æ•°å‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã€é–¢æ•°ã‚’ç¬¬ä¸€ç´šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦æ‰±ã†ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã™ã€‚å‰¯ä½œç”¨ã‚’é¿ã‘ã‚‹ã“ã¨ã‚’é‡è¦–ã—ã¾ã™ã€‚",
        "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã§ã¯ã€è¦ä»¶å®šç¾©ã€è¨­è¨ˆã€å®Ÿè£…ã€ãƒ†ã‚¹ãƒˆã€ä¿å®ˆã¨ã„ã†ãƒ—ãƒ­ã‚»ã‚¹ãŒã‚ã‚Šã¾ã™ã€‚",
        "ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®å¤‰æ›´å±¥æ­´ã‚’ç®¡ç†ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚GitãŒåºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚",
        "ãƒ‡ãƒãƒƒã‚°ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒã‚°ã‚’è¦‹ã¤ã‘ã¦ä¿®æ­£ã™ã‚‹ä½œæ¥­ã§ã™ã€‚ãƒ­ã‚°å‡ºåŠ›ã‚„ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå½¹ç«‹ã¡ã¾ã™ã€‚",
        "ãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºã¯ã€ãƒ†ã‚¹ãƒˆã‚’å…ˆã«æ›¸ã„ã¦ã‹ã‚‰å®Ÿè£…ã‚’è¡Œã†é–‹ç™ºæ‰‹æ³•ã§ã™ã€‚",
        "ã‚¢ã‚¸ãƒ£ã‚¤ãƒ«é–‹ç™ºã¯ã€å¤‰åŒ–ã«æŸ”è»Ÿã«å¯¾å¿œã§ãã‚‹åå¾©çš„ãªé–‹ç™ºæ‰‹æ³•ã§ã™ã€‚",
        "DevOpsã¯ã€é–‹ç™ºã¨é‹ç”¨ã‚’çµ±åˆã—ãŸæ‰‹æ³•ã§ã€ç¶™ç¶šçš„ãªãƒ‡ãƒªãƒãƒªãƒ¼ã‚’å®Ÿç¾ã—ã¾ã™ã€‚",
        
        # ===== ç§‘å­¦æŠ€è¡“ =====
        "æŠ€è¡“ã®é€²æ­©ã¯ç§ãŸã¡ã®ç”Ÿæ´»ã‚’å¤§ããå¤‰ãˆã¦ã„ã¾ã™ã€‚ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¯æ—¥å¸¸ã«ä¸å¯æ¬ ã«ãªã‚Šã¾ã—ãŸã€‚",
        "æœªæ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é‡å­åŸç†ã‚’æ´»ç”¨ã™ã‚‹ã§ã—ã‚‡ã†ã€‚ç¾åœ¨ã®é™ç•Œã‚’è¶…ãˆãŸè¨ˆç®—èƒ½åŠ›ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "äººå·¥çŸ¥èƒ½ã¯æ§˜ã€…ãªåˆ†é‡ã§é©æ–°ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚åŒ»ç™‚ã€é‡‘èã€è£½é€ æ¥­ãªã©å¹…åºƒã„å¿œç”¨ãŒã‚ã‚Šã¾ã™ã€‚",
        "ç§‘å­¦æŠ€è¡“ã®ç™ºå±•ã¯äººé¡ã®å¯èƒ½æ€§ã‚’åºƒã’ã¾ã™ã€‚å®‡å®™æ¢æŸ»ã‚„ç’°å¢ƒå•é¡Œã®è§£æ±ºã«ã‚‚è²¢çŒ®ã—ã¦ã„ã¾ã™ã€‚",
        "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¯æƒ…å ±é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚ä¸–ç•Œä¸­ã®äººã€…ãŒã¤ãªãŒã‚Šã€çŸ¥è­˜ã‚’å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚",
        "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆçµŒç”±ã§ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒªã‚½ãƒ¼ã‚¹ã‚’æä¾›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿åˆ†æã¯ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¾¡å€¤ã‚ã‚‹æ´å¯Ÿã‚’æŠ½å‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "IoTã¯ã€æ§˜ã€…ãªãƒ¢ãƒãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã«æ¥ç¶šã•ã‚Œã‚‹æŠ€è¡“ã§ã™ã€‚ã‚¹ãƒãƒ¼ãƒˆãƒ›ãƒ¼ãƒ ã‚„å·¥å ´ã®è‡ªå‹•åŒ–ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ã¯ã€åˆ†æ•£å‹å°å¸³æŠ€è¡“ã§ã™ã€‚æš—å·é€šè²¨ã‚„ã‚¹ãƒãƒ¼ãƒˆã‚³ãƒ³ãƒˆãƒ©ã‚¯ãƒˆã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚",
        "5Gé€šä¿¡ã¯ã€é«˜é€Ÿãƒ»å¤§å®¹é‡ãƒ»ä½é…å»¶ã®é€šä¿¡ã‚’å®Ÿç¾ã—ã¾ã™ã€‚è‡ªå‹•é‹è»¢ã‚„é éš”åŒ»ç™‚ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚",
        "ã‚¨ãƒƒã‚¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç«¯ã§è¡Œã†æŠ€è¡“ã§ã™ã€‚",
        "ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¯ã€ãƒ‡ã‚¸ã‚¿ãƒ«è³‡ç”£ã‚’ä¿è­·ã™ã‚‹ãŸã‚ã®æŠ€è¡“ã¨å®Ÿè·µã§ã™ã€‚",
        
        # ===== å¯¾è©±ãƒ»ä¼šè©± =====
        "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ",
        "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚ã©ã®ã‚ˆã†ãªä»•çµ„ã¿ã§å‹•ã„ã¦ã„ã‚‹ã®ã§ã™ã‹ï¼Ÿ",
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿæ™®é€šã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã©ã†é•ã†ã®ã§ã™ã‹ï¼Ÿ",
        "æ©Ÿæ¢°å­¦ç¿’ã¯ã©ã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã™ã‹ï¼Ÿå…·ä½“çš„ãªä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        "æœªæ¥ã®æŠ€è¡“ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã©ã®ã‚ˆã†ãªä¸–ç•ŒãŒå¾…ã£ã¦ã„ã‚‹ã§ã—ã‚‡ã†ã‹ã€‚",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯å‰µé€ çš„ãªæ´»å‹•ã§ã™ã€‚è‡ªåˆ†ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å½¢ã«ã™ã‚‹å–œã³ãŒã‚ã‚Šã¾ã™ã€‚",
        "ç§‘å­¦ã¯ç§ãŸã¡ã®ä¸–ç•Œã‚’ç†è§£ã™ã‚‹æ‰‹æ®µã§ã™ã€‚å¥½å¥‡å¿ƒã‹ã‚‰å§‹ã¾ã‚‹æ¢æ±‚ã®æ—…ã§ã™ã€‚",
        "æŠ€è¡“é©æ–°ã¯ç¤¾ä¼šã‚’å¤‰é©ã—ã¾ã™ã€‚æ–°ã—ã„å¯èƒ½æ€§ã¨èª²é¡Œã®ä¸¡æ–¹ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚",
        "è³ªå•ãŒã‚ã‚Œã°ãŠæ°—è»½ã«ã©ã†ãã€‚ã§ãã‚‹é™ã‚ŠãŠç­”ãˆã—ã¾ã™ã€‚",
        "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã¾ãŸä½•ã‹ã‚ã‚Œã°èã„ã¦ãã ã•ã„ã€‚",
        "ãã‚Œã¯èˆˆå‘³æ·±ã„è³ªå•ã§ã™ã­ã€‚ä¸€ç·’ã«è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
        "ãªã‚‹ã»ã©ã€ãã®ã‚ˆã†ãªè¦‹æ–¹ã‚‚ã‚ã‚‹ã®ã§ã™ã­ã€‚",
        "ç´ æ™´ã‚‰ã—ã„è¦³ç‚¹ã§ã™ã€‚ã‚‚ã†å°‘ã—è©³ã—ãæ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ",
        "ãã®é€šã‚Šã§ã™ã€‚ã•ã‚‰ã«è©³ã—ãèª¬æ˜ã—ã¾ã—ã‚‡ã†ã€‚",
        "è‰¯ã„è³ªå•ã§ã™ã­ã€‚é †ç•ªã«èª¬æ˜ã—ã¦ã„ãã¾ã™ã€‚",
        "ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ä»Šæ—¥ã‚‚ä¸€æ—¥é ‘å¼µã‚Šã¾ã—ã‚‡ã†ã€‚",
        "ãã‚Œã¯ç´ æ™´ã‚‰ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã§ã™ã­ã€‚å®Ÿç¾ã«å‘ã‘ã¦é ‘å¼µã‚Šã¾ã—ã‚‡ã†ã€‚",
        
        # ===== æ•°å­¦ãƒ»ç‰©ç†å­¦ =====
        "æ•°å­¦ã¯ç§‘å­¦ã®è¨€èªã§ã™ã€‚è‡ªç„¶æ³•å‰‡ã‚’è¨˜è¿°ã™ã‚‹ãŸã‚ã«ä½¿ã‚ã‚Œã¾ã™ã€‚",
        "ç‰©ç†å­¦ã¯è‡ªç„¶ç•Œã®æ³•å‰‡ã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚åŠ›å­¦ã€é›»ç£æ°—å­¦ã€é‡å­åŠ›å­¦ãªã©ãŒã‚ã‚Šã¾ã™ã€‚",
        "å¾®åˆ†ç©åˆ†å­¦ã¯ã€å¤‰åŒ–ã¨ç´¯ç©ã‚’æ‰±ã†æ•°å­¦ã®åˆ†é‡ã§ã™ã€‚ç‰©ç†å­¦ã‚„å·¥å­¦ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚",
        "ç·šå½¢ä»£æ•°ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚„è¡Œåˆ—ã‚’æ‰±ã†æ•°å­¦ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚",
        "ç¢ºç‡è«–ã¯ã€ä¸ç¢ºå®Ÿãªç¾è±¡ã‚’æ•°å­¦çš„ã«æ‰±ã†åˆ†é‡ã§ã™ã€‚çµ±è¨ˆå­¦ã‚„æ©Ÿæ¢°å­¦ç¿’ã¨å¯†æ¥ã«é–¢é€£ã—ã¦ã„ã¾ã™ã€‚",
        "çµ±è¨ˆå­¦ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®åé›†ãƒ»åˆ†æãƒ»è§£é‡ˆã‚’è¡Œã†å­¦å•ã§ã™ã€‚ç§‘å­¦çš„ç ”ç©¶ã‚„ãƒ“ã‚¸ãƒã‚¹ã§é‡è¦ã§ã™ã€‚",
        "ç›¸å¯¾æ€§ç†è«–ã¯ã€æ™‚é–“ã¨ç©ºé–“ã®æ¦‚å¿µã‚’é©æ–°ã—ãŸã‚¢ã‚¤ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³ã®ç†è«–ã§ã™ã€‚",
        "é‡å­åŠ›å­¦ã¯ã€åŸå­ã‚„åˆ†å­ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§æˆã‚Šç«‹ã¤ç‰©ç†æ³•å‰‡ã§ã™ã€‚",
        "ç†±åŠ›å­¦ã¯ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¨ç†±ã®é–¢ä¿‚ã‚’ç ”ç©¶ã™ã‚‹åˆ†é‡ã§ã™ã€‚",
        "é›»ç£æ°—å­¦ã¯ã€é›»æ°—ã¨ç£æ°—ã®ç›¸äº’ä½œç”¨ã‚’ç ”ç©¶ã™ã‚‹åˆ†é‡ã§ã™ã€‚",
        "å¹¾ä½•å­¦ã¯ã€å›³å½¢ã‚„ç©ºé–“ã®æ€§è³ªã‚’ç ”ç©¶ã™ã‚‹æ•°å­¦ã®åˆ†é‡ã§ã™ã€‚",
        "æ•°è«–ã¯ã€æ•´æ•°ã®æ€§è³ªã‚’ç ”ç©¶ã™ã‚‹ç´”ç²‹æ•°å­¦ã®åˆ†é‡ã§ã™ã€‚",
        
        # ===== ç”Ÿç‰©å­¦ãƒ»åŒ»å­¦ =====
        "ç”Ÿç‰©å­¦ã¯ç”Ÿå‘½ç¾è±¡ã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚ç´°èƒã€éºä¼å­ã€é€²åŒ–ãªã©ã‚’æ‰±ã„ã¾ã™ã€‚",
        "åŒ»å­¦ã¯ç—…æ°—ã®äºˆé˜²ã€è¨ºæ–­ã€æ²»ç™‚ã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚äººã€…ã®å¥åº·ã‚’å®ˆã‚Šã¾ã™ã€‚",
        "éºä¼å­å·¥å­¦ã¯ã€DNAã‚’æ“ä½œã—ã¦æœ›ã¾ã—ã„å½¢è³ªã‚’å¾—ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "å…ç–«å­¦ã¯ã€ä½“ã®é˜²å¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ç ”ç©¶ã™ã‚‹åˆ†é‡ã§ã™ã€‚ãƒ¯ã‚¯ãƒãƒ³é–‹ç™ºã«è²¢çŒ®ã—ã¦ã„ã¾ã™ã€‚",
        "ç¥çµŒç§‘å­¦ã¯ã€è„³ã¨ç¥çµŒç³»ã‚’ç ”ç©¶ã™ã‚‹åˆ†é‡ã§ã™ã€‚æ„è­˜ã‚„è¨˜æ†¶ã®ä»•çµ„ã¿ã‚’è§£æ˜ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚",
        "ãƒã‚¤ã‚ªãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã¯ã€ç”Ÿç‰©ã®æ©Ÿèƒ½ã‚’åˆ©ç”¨ã—ãŸæŠ€è¡“ã§ã™ã€‚åŒ»è–¬å“ã‚„é£Ÿå“ã®ç”Ÿç”£ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "ã‚²ãƒãƒ è§£æã¯ã€ç”Ÿç‰©ã®å…¨éºä¼æƒ…å ±ã‚’è§£èª­ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "å†ç”ŸåŒ»ç™‚ã¯ã€æå‚·ã—ãŸçµ„ç¹”ã‚„è‡“å™¨ã‚’å†ç”Ÿã™ã‚‹åŒ»ç™‚æŠ€è¡“ã§ã™ã€‚",
        
        # ===== å“²å­¦ãƒ»æ€è€ƒ =====
        "å“²å­¦ã¯ã€å­˜åœ¨ã€çŸ¥è­˜ã€å€«ç†ãªã©ã®æ ¹æœ¬çš„ãªå•ã„ã‚’æ¢æ±‚ã™ã‚‹å­¦å•ã§ã™ã€‚",
        "è«–ç†å­¦ã¯ã€æ­£ã—ã„æ¨è«–ã®è¦å‰‡ã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚",
        "å€«ç†å­¦ã¯ã€å–„æ‚ªã‚„æ­£ç¾©ã«ã¤ã„ã¦è€ƒãˆã‚‹å“²å­¦ã®ä¸€åˆ†é‡ã§ã™ã€‚",
        "èªè­˜è«–ã¯ã€çŸ¥è­˜ã¨ã¯ä½•ã‹ã€ã©ã®ã‚ˆã†ã«ç²å¾—ã•ã‚Œã‚‹ã‹ã‚’ç ”ç©¶ã—ã¾ã™ã€‚",
        "æ‰¹åˆ¤çš„æ€è€ƒã¯ã€æƒ…å ±ã‚’å®¢è¦³çš„ã«åˆ†æã—è©•ä¾¡ã™ã‚‹èƒ½åŠ›ã§ã™ã€‚",
        "å‰µé€ çš„æ€è€ƒã¯ã€æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚„è§£æ±ºç­–ã‚’ç”Ÿã¿å‡ºã™èƒ½åŠ›ã§ã™ã€‚",
        "å½¢è€Œä¸Šå­¦ã¯ã€å­˜åœ¨ã®æœ¬è³ªã«ã¤ã„ã¦æ¢æ±‚ã™ã‚‹å“²å­¦ã®åˆ†é‡ã§ã™ã€‚",
        "ç¾å­¦ã¯ã€ç¾ã¨èŠ¸è¡“ã«ã¤ã„ã¦ç ”ç©¶ã™ã‚‹å“²å­¦ã®åˆ†é‡ã§ã™ã€‚",
        
        # ===== çµŒæ¸ˆãƒ»ãƒ“ã‚¸ãƒã‚¹ =====
        "çµŒæ¸ˆå­¦ã¯ã€è³‡æºã®é…åˆ†ã¨æ„æ€æ±ºå®šã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚",
        "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã¯ã€é¡§å®¢ã®ãƒ‹ãƒ¼ã‚ºã‚’æº€ãŸã™è£½å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã™ã‚‹ãŸã‚ã®æ´»å‹•ã§ã™ã€‚",
        "ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹ã¯ã€ãŠé‡‘ã®ç®¡ç†ã¨æŠ•è³‡ã«é–¢ã™ã‚‹åˆ†é‡ã§ã™ã€‚",
        "èµ·æ¥­å®¶ç²¾ç¥ã¯ã€æ–°ã—ã„ãƒ“ã‚¸ãƒã‚¹ã‚’å‰µé€ ã—ç™ºå±•ã•ã›ã‚‹å§¿å‹¢ã§ã™ã€‚",
        "ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ç®¡ç†ã¯ã€è£½å“ã®æµã‚Œã‚’åŠ¹ç‡çš„ã«ç®¡ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "äººäº‹ç®¡ç†ã¯ã€çµ„ç¹”ã®äººæã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹ãŸã‚ã®æ´»å‹•ã§ã™ã€‚",
        
        # ===== ç’°å¢ƒãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼ =====
        "å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¯ã€å¤ªé™½å…‰ã€é¢¨åŠ›ã€æ°´åŠ›ãªã©ã®æŒç¶šå¯èƒ½ãªã‚¨ãƒãƒ«ã‚®ãƒ¼æºã§ã™ã€‚",
        "æ°—å€™å¤‰å‹•ã¯ã€äººé¡ãŒç›´é¢ã™ã‚‹æœ€å¤§ã®ç’°å¢ƒèª²é¡Œã®ä¸€ã¤ã§ã™ã€‚",
        "æŒç¶šå¯èƒ½ãªé–‹ç™ºã¯ã€å°†æ¥ã®ä¸–ä»£ã®ãƒ‹ãƒ¼ã‚ºã‚’æãªã‚ãªã„ç™ºå±•ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚",
        "é›»æ°—è‡ªå‹•è»Šã¯ã€ç’°å¢ƒã«å„ªã—ã„æ¬¡ä¸–ä»£ã®äº¤é€šæ‰‹æ®µã§ã™ã€‚",
        "ã‚«ãƒ¼ãƒœãƒ³ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã¯ã€äºŒé…¸åŒ–ç‚­ç´ ã®æ’å‡ºã¨å¸åã‚’å‡è¡¡ã•ã›ã‚‹ã“ã¨ã§ã™ã€‚",
        
        # ===== å®‡å®™ãƒ»å¤©æ–‡å­¦ =====
        "å¤©æ–‡å­¦ã¯ã€å®‡å®™ã®æ§‹é€ ã¨é€²åŒ–ã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚",
        "å®‡å®™æ¢æŸ»ã¯ã€äººé¡ã®çŸ¥è­˜ã®å¢ƒç•Œã‚’åºƒã’ã‚‹å†’é™ºã§ã™ã€‚",
        "ãƒ–ãƒ©ãƒƒã‚¯ãƒ›ãƒ¼ãƒ«ã¯ã€å…‰ã•ãˆã‚‚é€ƒã’ã‚‰ã‚Œãªã„è¶…é‡åŠ›å¤©ä½“ã§ã™ã€‚",
        "éŠ€æ²³ã¯ã€æ•°åå„„ã‹ã‚‰æ•°åƒå„„ã®æ’æ˜ŸãŒé›†ã¾ã£ãŸå·¨å¤§ãªå¤©ä½“ç³»ã§ã™ã€‚",
        "å®‡å®™é–‹ç™ºã¯ã€äººé¡ã®æœªæ¥ã‚’åˆ‡ã‚Šé–‹ãæŒ‘æˆ¦ã§ã™ã€‚",
    ]
    
    # ãƒ‡ãƒ¼ã‚¿å¢—å¹…ï¼ˆå¤šæ§˜ãªçµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆï¼‰
    texts = base_texts * 50
    
    # è¿½åŠ ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
    prefixes = ["å®Ÿéš›ã«ã€", "èˆˆå‘³æ·±ã„ã“ã¨ã«ã€", "é‡è¦ãªã®ã¯ã€", "ç‰¹ã«ã€", "ã•ã‚‰ã«ã€", "ã¤ã¾ã‚Šã€", "ä¾‹ãˆã°ã€", "å…·ä½“çš„ã«ã¯ã€", 
                "ä¸€èˆ¬çš„ã«ã€", "çµè«–ã¨ã—ã¦ã€", "è¦ã™ã‚‹ã«ã€", "è¨€ã„æ›ãˆã‚‹ã¨ã€", "ãªãœãªã‚‰ã€", "ã—ãŸãŒã£ã¦ã€", "ã‚‚ã¡ã‚ã‚“ã€"]
    suffixes = ["ã“ã‚Œã¯é©æ–°çš„ã§ã™ã€‚", "ä»Šå¾Œã®ç™ºå±•ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚", "å¤šãã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", "ç ”ç©¶ãŒé€²ã‚“ã§ã„ã¾ã™ã€‚",
                "æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚", "æœŸå¾…ãŒé«˜ã¾ã£ã¦ã„ã¾ã™ã€‚", "é€²åŒ–ã‚’ç¶šã‘ã¦ã„ã¾ã™ã€‚", "é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã¾ã™ã€‚"]
    
    for text in base_texts[:80]:
        for prefix in prefixes:
            texts.append(prefix + text)
        for suffix in suffixes:
            texts.append(text + suffix)
    
    # Hugging Faceãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if use_huggingface:
        hf_texts = fetch_huggingface_data(max_samples=3000)
        texts.extend(hf_texts)
    
    return texts


# ========================================
# ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
# ========================================

def chat_mode(ai: NeuroQuantumBrainAI):
    """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
    print("\n" + "=" * 60)
    print("ğŸ’¬ ãƒ‹ãƒ¥ãƒ¼ãƒ­Q Brain ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰")
    print("=" * 60)
    print("ã‚³ãƒãƒ³ãƒ‰:")
    print("  /quit, /exit      - çµ‚äº†")
    print("  /temp <min> <max> - æ¸©åº¦ç¯„å›² (ä¾‹: /temp 0.3 0.8)")
    print("  /len <å€¤>         - ç”Ÿæˆé•·ã• (10-100)")
    print("  /stats            - é‡å­çµ±è¨ˆ")
    print("-" * 60)
    
    temp_min = 0.4
    temp_max = 0.9
    max_length = 40
    
    while True:
        try:
            user_input = input("\nğŸ§‘ You: ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['/quit', '/exit', '/q']:
                print("\nğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼")
                break
            
            if user_input.lower().startswith('/temp'):
                try:
                    parts = user_input.split()
                    if len(parts) >= 3:
                        temp_min = max(0.1, min(1.0, float(parts[1])))
                        temp_max = max(temp_min, min(1.5, float(parts[2])))
                    else:
                        val = float(parts[1])
                        temp_min = max(0.1, val - 0.2)
                        temp_max = min(1.5, val + 0.2)
                    print(f"   æ¸©åº¦ç¯„å›²ã‚’ {temp_min:.2f}ã€œ{temp_max:.2f} ã«è¨­å®š")
                except:
                    print("   ä½¿ç”¨æ³•: /temp <min> <max> ã¾ãŸã¯ /temp <å€¤>")
                continue
            
            if user_input.lower().startswith('/len'):
                try:
                    val = int(user_input.split()[1])
                    max_length = max(10, min(100, val))
                    print(f"   ç”Ÿæˆé•·ã•ã‚’ {max_length} ã«è¨­å®š")
                except:
                    print("   ä½¿ç”¨æ³•: /len <10-100>")
                continue
            
            if user_input.lower() == '/stats':
                print(ai.get_report())
                continue
            
            # ç”Ÿæˆï¼ˆæ¸©åº¦ç¯„å›²åˆ¶ç´„ï¼‰
            response = ai.generate(user_input, max_length=max_length, 
                                   temperature_min=temp_min, temperature_max=temp_max)
            print(f"\nğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ­Q: {response}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼")
            break
        except Exception as e:
            print(f"   ã‚¨ãƒ©ãƒ¼: {e}")


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

def main(num_neurons: int = 100):
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    
    Args:
        num_neurons: ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰
    """
    print("\nğŸ”§ ãƒ‹ãƒ¥ãƒ¼ãƒ­Q Brain ã‚’æ§‹ç¯‰ä¸­...")
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {num_neurons}")
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—
    texts = get_training_data()
    print(f"\nğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # AIæ§‹ç¯‰
    ai = NeuroQuantumBrainAI(
        embed_dim=128,
        num_heads=4,
        num_layers=3,
        num_neurons=num_neurons,  # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã‚’æŒ‡å®š
        max_vocab=16000  # é©åˆ‡ãªèªå½™ã‚µã‚¤ã‚ºã«å¤‰æ›´
    )
    
    # å­¦ç¿’
    ai.train(texts, epochs=25, batch_size=16, lr=0.002, seq_length=48)
    
    # é‡å­çµ±è¨ˆ
    print(ai.get_report())
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    print("\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ:")
    print("-" * 50)
    
    prompts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯",
        "äººå·¥çŸ¥èƒ½ã¨ã¯",
        "æœªæ¥ã®æŠ€è¡“",
        "ã“ã‚“ã«ã¡ã¯",
    ]
    
    for prompt in prompts:
        generated = ai.generate(prompt, max_length=40, temperature_min=0.4, temperature_max=0.9)
        print(f"   '{prompt}' â†’ {generated}\n")
    
    # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
    print("\n" + "=" * 60)
    response = input("ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
    if response == 'y':
        chat_mode(ai)
    
    print("\nâœ… ãƒ‹ãƒ¥ãƒ¼ãƒ­Q Brain å®Œæˆï¼")


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='ãƒ‹ãƒ¥ãƒ¼ãƒ­Q Brain - è„³å‹æ•£åœ¨QBNNã«ã‚ˆã‚‹ç”ŸæˆAI')
    parser.add_argument('--neurons', type=int, default=100, help='ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)')
    parser.add_argument('--chat', action='store_true', help='ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•')
    args = parser.parse_args()
    
    main(num_neurons=args.neurons)

