"""
é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ LLM (PyTorchç‰ˆ)

æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆã®åŸç†ã‚’PyTorchãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«çµ±åˆ
- é‡å­é‡ã­åˆã‚ã›çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
- é‡å­å¹²æ¸‰ã«ã‚ˆã‚‹ç¢ºç‡å¢—å¹…
- é«˜é€Ÿãªä¸¦åˆ—å‡¦ç†
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Tuple, Optional
import time
import math
from pseudo_qubit import PseudoQubit


# ============================================================
# é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
# ============================================================

class QuantumSuperpositionLayer(nn.Module):
    """
    é‡å­é‡ã­åˆã‚ã›ãƒ¬ã‚¤ãƒ¤ãƒ¼
    
    å…¨ã¦ã®å¯èƒ½ãªçŠ¶æ…‹ã‚’åŒæ™‚ã«è¡¨ç¾
    """
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        # é‡å­ä½ç›¸ã‚’å­¦ç¿’
        self.phase = nn.Parameter(torch.randn(dim) * 0.1)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # é‡å­çš„ãªä½ç›¸å¤‰èª¿
        phase_shift = torch.cos(self.phase) + 1j * torch.sin(self.phase)
        # å®Ÿæ•°éƒ¨ã®ã¿ä½¿ç”¨ï¼ˆæ“¬ä¼¼é‡å­ï¼‰
        amplitude = torch.cos(self.phase * x)
        return x * amplitude


class QuantumInterferenceLayer(nn.Module):
    """
    é‡å­å¹²æ¸‰ãƒ¬ã‚¤ãƒ¤ãƒ¼
    
    è‰¯ã„å€™è£œã‚’å¢—å¹…ã€æ‚ªã„å€™è£œã‚’æ¸›è¡°
    """
    def __init__(self, dim: int):
        super().__init__()
        self.projection = nn.Linear(dim, dim)
        self.interference_gate = nn.Linear(dim, dim)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # å¹²æ¸‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—
        interference = torch.sigmoid(self.interference_gate(x))
        projected = self.projection(x)
        # æ§‹é€ çš„å¹²æ¸‰
        return projected * interference + x * (1 - interference)


class QuantumAttention(nn.Module):
    """
    é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
    
    å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³é–¢ä¿‚ã‚’ã€Œé‡ã­åˆã‚ã›ã€ã§åŒæ™‚è©•ä¾¡
    """
    def __init__(self, dim: int, n_heads: int = 4):
        super().__init__()
        self.dim = dim
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        
        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        self.out_proj = nn.Linear(dim, dim)
        
        # é‡å­ä½ç›¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.quantum_phase = nn.Parameter(torch.randn(n_heads, 1, 1) * 0.1)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len, _ = x.shape
        
        # Query, Key, Value
        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        
        # é‡å­çš„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢
        # cos(phase) ã«ã‚ˆã‚‹å¹²æ¸‰åŠ¹æœ
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        quantum_interference = torch.cos(self.quantum_phase * scores)
        scores = scores * (1 + 0.1 * quantum_interference)
        
        # ãƒã‚¹ã‚¯é©ç”¨ï¼ˆå› æœçš„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã§ç¢ºç‡åŒ–ï¼ˆé‡å­æ¸¬å®šã«ç›¸å½“ï¼‰
        attn_weights = F.softmax(scores, dim=-1)
        
        # å€¤ã®é‡ã¿ä»˜ã‘å’Œ
        out = torch.matmul(attn_weights, v)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dim)
        
        return self.out_proj(out)


class QuantumTransformerBlock(nn.Module):
    """
    é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ Transformerãƒ–ãƒ­ãƒƒã‚¯
    """
    def __init__(self, dim: int, n_heads: int = 4, ff_dim: int = None):
        super().__init__()
        ff_dim = ff_dim or dim * 4
        
        self.attention = QuantumAttention(dim, n_heads)
        self.superposition = QuantumSuperpositionLayer(dim)
        self.interference = QuantumInterferenceLayer(dim)
        
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        
        self.ff = nn.Sequential(
            nn.Linear(dim, ff_dim),
            nn.GELU(),
            nn.Linear(ff_dim, dim)
        )
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # é‡å­ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ + æ®‹å·®æ¥ç¶š
        x = x + self.attention(self.norm1(x), mask)
        
        # é‡å­é‡ã­åˆã‚ã›
        x = self.superposition(x)
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ + æ®‹å·®æ¥ç¶š
        x = x + self.ff(self.norm2(x))
        
        # é‡å­å¹²æ¸‰
        x = self.interference(x)
        
        return x


# ============================================================
# é‡å­LLMãƒ¢ãƒ‡ãƒ«
# ============================================================

class QuantumLLM(nn.Module):
    """
    é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ è¨€èªãƒ¢ãƒ‡ãƒ«
    
    PyTorch + æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆåŸç†
    """
    def __init__(
        self, 
        vocab_size: int,
        dim: int = 128,
        n_layers: int = 4,
        n_heads: int = 4,
        max_seq_len: int = 256
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.dim = dim
        self.max_seq_len = max_seq_len
        
        # åŸ‹ã‚è¾¼ã¿å±¤
        self.token_embedding = nn.Embedding(vocab_size, dim)
        self.position_embedding = nn.Embedding(max_seq_len, dim)
        
        # é‡å­Transformerãƒ–ãƒ­ãƒƒã‚¯
        self.blocks = nn.ModuleList([
            QuantumTransformerBlock(dim, n_heads)
            for _ in range(n_layers)
        ])
        
        # å‡ºåŠ›å±¤
        self.norm = nn.LayerNorm(dim)
        self.output = nn.Linear(dim, vocab_size)
        
        # å› æœãƒã‚¹ã‚¯
        self.register_buffer(
            'causal_mask',
            torch.tril(torch.ones(max_seq_len, max_seq_len)).unsqueeze(0).unsqueeze(0)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len = x.shape
        
        # åŸ‹ã‚è¾¼ã¿
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)
        x = self.token_embedding(x) + self.position_embedding(positions)
        
        # ãƒã‚¹ã‚¯
        mask = self.causal_mask[:, :, :seq_len, :seq_len]
        
        # Transformerãƒ–ãƒ­ãƒƒã‚¯
        for block in self.blocks:
            x = block(x, mask)
        
        # å‡ºåŠ›
        x = self.norm(x)
        logits = self.output(x)
        
        return logits
    
    @torch.no_grad()
    def generate(
        self, 
        prompt_ids: torch.Tensor,
        max_new_tokens: int = 50,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.9
    ) -> torch.Tensor:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆé‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        """
        self.eval()
        generated = prompt_ids.clone()
        
        for _ in range(max_new_tokens):
            # æœ€å¤§é•·ã‚’è¶…ãˆãªã„ã‚ˆã†ã«
            if generated.shape[1] >= self.max_seq_len:
                break
            
            # äºˆæ¸¬
            logits = self(generated)
            next_logits = logits[:, -1, :] / temperature
            
            # Top-k ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_k > 0:
                indices_to_remove = next_logits < torch.topk(next_logits, top_k)[0][..., -1, None]
                next_logits[indices_to_remove] = float('-inf')
            
            # Top-p (nucleus) ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                next_logits[indices_to_remove] = float('-inf')
            
            # é‡å­æ¸¬å®šï¼ˆç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            generated = torch.cat([generated, next_token], dim=1)
        
        return generated


# ============================================================
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
# ============================================================

class SimpleTokenizer:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—ãƒ¬ãƒ™ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"""
    
    def __init__(self):
        self.char_to_id: Dict[str, int] = {}
        self.id_to_char: Dict[int, str] = {}
        self.vocab_size = 0
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.pad_token = '<PAD>'
        self.unk_token = '<UNK>'
        self.bos_token = '<BOS>'
        self.eos_token = '<EOS>'
        
    def build_vocab(self, texts: List[str]):
        """èªå½™ã‚’æ§‹ç¯‰"""
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        special_tokens = [self.pad_token, self.unk_token, self.bos_token, self.eos_token]
        for token in special_tokens:
            self.char_to_id[token] = len(self.char_to_id)
            self.id_to_char[self.char_to_id[token]] = token
        
        # æ–‡å­—ã‚’åé›†
        chars = set()
        for text in texts:
            chars.update(text)
        
        for char in sorted(chars):
            if char not in self.char_to_id:
                self.char_to_id[char] = len(self.char_to_id)
                self.id_to_char[self.char_to_id[char]] = char
        
        self.vocab_size = len(self.char_to_id)
    
    def encode(self, text: str) -> List[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’IDåˆ—ã«å¤‰æ›"""
        ids = [self.char_to_id.get(self.bos_token, 0)]
        for char in text:
            ids.append(self.char_to_id.get(char, self.char_to_id[self.unk_token]))
        return ids
    
    def decode(self, ids: List[int]) -> str:
        """IDåˆ—ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        chars = []
        for id in ids:
            char = self.id_to_char.get(id, '')
            if char not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]:
                chars.append(char)
        return ''.join(chars)


# ============================================================
# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
# ============================================================

class QuantumLLMTrainer:
    """é‡å­LLMãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""
    
    def __init__(
        self,
        model: QuantumLLM,
        tokenizer: SimpleTokenizer,
        learning_rate: float = 1e-3,
        device: str = 'cpu'
    ):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss()
        
    def prepare_batch(self, texts: List[str], max_len: int = 64) -> Tuple[torch.Tensor, torch.Tensor]:
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        batch_ids = []
        
        for text in texts:
            ids = self.tokenizer.encode(text)
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ
            if len(ids) > max_len:
                ids = ids[:max_len]
            else:
                ids = ids + [self.tokenizer.char_to_id[self.tokenizer.pad_token]] * (max_len - len(ids))
            batch_ids.append(ids)
        
        batch = torch.tensor(batch_ids, device=self.device)
        # å…¥åŠ›: [:-1], ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: [1:]
        return batch[:, :-1], batch[:, 1:]
    
    def train_step(self, texts: List[str]) -> float:
        """1ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’"""
        self.model.train()
        
        inputs, targets = self.prepare_batch(texts)
        
        self.optimizer.zero_grad()
        logits = self.model(inputs)
        
        # æå¤±è¨ˆç®—
        loss = self.criterion(
            logits.reshape(-1, self.tokenizer.vocab_size),
            targets.reshape(-1)
        )
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, texts: List[str], epochs: int = 100, batch_size: int = 8):
        """å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
        n_batches = (len(texts) + batch_size - 1) // batch_size
        
        for epoch in range(epochs):
            total_loss = 0
            
            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            indices = np.random.permutation(len(texts))
            
            for i in range(n_batches):
                batch_indices = indices[i * batch_size:(i + 1) * batch_size]
                batch_texts = [texts[j] for j in batch_indices]
                
                loss = self.train_step(batch_texts)
                total_loss += loss
            
            avg_loss = total_loss / n_batches
            
            if (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")
    
    def generate(self, prompt: str, max_new_tokens: int = 50, temperature: float = 0.8) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        self.model.eval()
        
        prompt_ids = self.tokenizer.encode(prompt)
        prompt_tensor = torch.tensor([prompt_ids], device=self.device)
        
        generated_ids = self.model.generate(
            prompt_tensor,
            max_new_tokens=max_new_tokens,
            temperature=temperature
        )
        
        return self.tokenizer.decode(generated_ids[0].tolist())


# ============================================================
# ãƒ‡ãƒ¢
# ============================================================

def demo():
    """é‡å­LLM ãƒ‡ãƒ¢"""
    print("=" * 70)
    print("  QUANTUM-INSPIRED LLM (PyTorch)")
    print("  æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆ + ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
    print("=" * 70)
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
    training_texts = [
        "ã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚",
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ç´ æ™´ã‚‰ã—ã„æŠ€è¡“ã§ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã§æœªæ¥ã‚’å¤‰ãˆã¾ã—ã‚‡ã†ã€‚",
        "äººå·¥çŸ¥èƒ½ã¯æ—¥ã€…é€²åŒ–ã—ã¦ã„ã¾ã™ã€‚",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯æ¥½ã—ã„ã§ã™ã­ã€‚",
        "æ–°ã—ã„æŠ€è¡“ã‚’å­¦ã¶ã“ã¨ãŒå¤§åˆ‡ã§ã™ã€‚",
        "é‡å­ãƒ“ãƒƒãƒˆã¯0ã¨1ã‚’åŒæ™‚ã«æŒã¦ã¾ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯ç”»åƒèªè­˜ã«ä½¿ã‚ã‚Œã¾ã™ã€‚",
        "ä»Šæ—¥ã‚‚ä¸€æ—¥é ‘å¼µã‚Šã¾ã—ã‚‡ã†ã€‚",
        "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚",
        "è³ªå•ãŒã‚ã‚Œã°èã„ã¦ãã ã•ã„ã€‚",
        "æœªæ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é‡å­æŠ€è¡“ã‚’ä½¿ã„ã¾ã™ã€‚",
        "ç§ã¯é‡å­AIã§ã™ã€‚ä½•ã§ã‚‚èã„ã¦ãã ã•ã„ã€‚",
        "å­¦ç¿’ã™ã‚‹ã“ã¨ã¯æ¥½ã—ã„ã§ã™ã€‚",
        "æ˜æ—¥ã‚‚è‰¯ã„æ—¥ã«ãªã‚Šã¾ã™ã‚ˆã†ã«ã€‚",
    ]
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰
    print("\n" + "â”€" * 70)
    print("  1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰")
    print("â”€" * 70)
    
    tokenizer = SimpleTokenizer()
    tokenizer.build_vocab(training_texts)
    print(f"  èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size}")
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    print("\n" + "â”€" * 70)
    print("  2. é‡å­LLMãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰")
    print("â”€" * 70)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"  ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    model = QuantumLLM(
        vocab_size=tokenizer.vocab_size,
        dim=64,
        n_layers=3,
        n_heads=4,
        max_seq_len=128
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
    trainer = QuantumLLMTrainer(model, tokenizer, learning_rate=1e-3, device=device)
    
    # å­¦ç¿’
    print("\n" + "â”€" * 70)
    print("  3. å­¦ç¿’é–‹å§‹")
    print("â”€" * 70)
    
    start_time = time.time()
    trainer.train(training_texts, epochs=100, batch_size=4)
    train_time = time.time() - start_time
    print(f"  å­¦ç¿’æ™‚é–“: {train_time:.2f}ç§’")
    
    # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    print("\n" + "â”€" * 70)
    print("  4. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("â”€" * 70)
    
    prompts = ["ã“ã‚“ã«ã¡ã¯", "é‡å­", "ä»Šæ—¥", "æ©Ÿæ¢°å­¦ç¿’"]
    
    for prompt in prompts:
        print(f"\n  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'")
        
        start = time.time()
        result = trainer.generate(prompt, max_new_tokens=40, temperature=0.8)
        gen_time = time.time() - start
        
        print(f"  ç”Ÿæˆçµæœ: {result}")
        print(f"  ç”Ÿæˆæ™‚é–“: {gen_time:.3f}ç§’")
    
    # æ¸©åº¦æ¯”è¼ƒ
    print("\n" + "â”€" * 70)
    print("  5. æ¸©åº¦ã«ã‚ˆã‚‹ç”Ÿæˆã®é•ã„")
    print("â”€" * 70)
    
    prompt = "é‡å­"
    for temp in [0.3, 0.8, 1.5]:
        result = trainer.generate(prompt, max_new_tokens=30, temperature=temp)
        print(f"  æ¸©åº¦ {temp}: {result}")
    
    print("\n" + "=" * 70)
    print("  ãƒ‡ãƒ¢å®Œäº†ï¼")
    print("=" * 70)
    
    return trainer


def interactive_mode():
    """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
    print("\n" + "=" * 70)
    print("  é‡å­LLM (PyTorch) - å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
    print("=" * 70)
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
    training_texts = [
        "ã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚",
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ç´ æ™´ã‚‰ã—ã„æŠ€è¡“ã§ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã§æœªæ¥ã‚’å¤‰ãˆã¾ã—ã‚‡ã†ã€‚",
        "äººå·¥çŸ¥èƒ½ã¯æ—¥ã€…é€²åŒ–ã—ã¦ã„ã¾ã™ã€‚",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯æ¥½ã—ã„ã§ã™ã­ã€‚",
        "æ–°ã—ã„æŠ€è¡“ã‚’å­¦ã¶ã“ã¨ãŒå¤§åˆ‡ã§ã™ã€‚",
        "é‡å­ãƒ“ãƒƒãƒˆã¯0ã¨1ã‚’åŒæ™‚ã«æŒã¦ã¾ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯ç”»åƒèªè­˜ã«ä½¿ã‚ã‚Œã¾ã™ã€‚",
        "ä»Šæ—¥ã‚‚ä¸€æ—¥é ‘å¼µã‚Šã¾ã—ã‚‡ã†ã€‚",
        "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚",
        "è³ªå•ãŒã‚ã‚Œã°èã„ã¦ãã ã•ã„ã€‚",
        "æœªæ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é‡å­æŠ€è¡“ã‚’ä½¿ã„ã¾ã™ã€‚",
        "ç§ã¯é‡å­AIã§ã™ã€‚ä½•ã§ã‚‚èã„ã¦ãã ã•ã„ã€‚",
        "å­¦ç¿’ã™ã‚‹ã“ã¨ã¯æ¥½ã—ã„ã§ã™ã€‚",
        "æ˜æ—¥ã‚‚è‰¯ã„æ—¥ã«ãªã‚Šã¾ã™ã‚ˆã†ã«ã€‚",
    ]
    
    # åˆæœŸåŒ–
    print("\n  ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
    tokenizer = SimpleTokenizer()
    tokenizer.build_vocab(training_texts)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = QuantumLLM(
        vocab_size=tokenizer.vocab_size,
        dim=64,
        n_layers=3,
        n_heads=4,
        max_seq_len=128
    )
    
    trainer = QuantumLLMTrainer(model, tokenizer, learning_rate=1e-3, device=device)
    
    # å­¦ç¿’
    print("  å­¦ç¿’ä¸­...")
    trainer.train(training_texts, epochs=50, batch_size=4)
    
    # è¨­å®š
    temperature = 0.8
    max_tokens = 50
    
    print("\n  æº–å‚™å®Œäº†ï¼")
    print("\n  ã‚³ãƒãƒ³ãƒ‰:")
    print("    [ãƒ†ã‚­ã‚¹ãƒˆ] - ç¶šãã‚’ç”Ÿæˆ")
    print("    add [ãƒ†ã‚­ã‚¹ãƒˆ] - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦å†å­¦ç¿’")
    print("    temp [å€¤] - æ¸©åº¦è¨­å®š (0.1-2.0)")
    print("    len [å€¤] - æœ€å¤§ç”Ÿæˆé•·")
    print("    retrain - å†å­¦ç¿’")
    print("    quit - çµ‚äº†")
    
    while True:
        try:
            user_input = input(f"\n  [temp={temperature}] > ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['quit', 'q', 'çµ‚äº†']:
                print("  ã•ã‚ˆã†ãªã‚‰ï¼")
                break
            
            if user_input.lower().startswith('add '):
                new_text = user_input[4:].strip()
                if new_text:
                    training_texts.append(new_text)
                    # èªå½™ã‚’å†æ§‹ç¯‰
                    tokenizer.build_vocab(training_texts)
                    # ãƒ¢ãƒ‡ãƒ«ã‚’å†æ§‹ç¯‰
                    model = QuantumLLM(
                        vocab_size=tokenizer.vocab_size,
                        dim=64,
                        n_layers=3,
                        n_heads=4,
                        max_seq_len=128
                    )
                    trainer = QuantumLLMTrainer(model, tokenizer, learning_rate=1e-3, device=device)
                    print(f"  âœ“ ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                    print("  å†å­¦ç¿’ä¸­...")
                    trainer.train(training_texts, epochs=50, batch_size=4)
                    print("  âœ“ å†å­¦ç¿’å®Œäº†ï¼")
                continue
            
            if user_input.lower().startswith('temp '):
                try:
                    temperature = float(user_input.split()[1])
                    temperature = max(0.1, min(2.0, temperature))
                    print(f"  âœ“ æ¸©åº¦ã‚’ {temperature} ã«è¨­å®š")
                except:
                    print("  ç„¡åŠ¹ãªå€¤ã§ã™")
                continue
            
            if user_input.lower().startswith('len '):
                try:
                    max_tokens = int(user_input.split()[1])
                    max_tokens = max(10, min(200, max_tokens))
                    print(f"  âœ“ æœ€å¤§é•·ã‚’ {max_tokens} ã«è¨­å®š")
                except:
                    print("  ç„¡åŠ¹ãªå€¤ã§ã™")
                continue
            
            if user_input.lower() == 'retrain':
                print("  å†å­¦ç¿’ä¸­...")
                trainer.train(training_texts, epochs=50, batch_size=4)
                print("  âœ“ å†å­¦ç¿’å®Œäº†ï¼")
                continue
            
            # ç”Ÿæˆ
            print("  âš¡ é‡å­ç”Ÿæˆä¸­...")
            start = time.time()
            result = trainer.generate(user_input, max_new_tokens=max_tokens, temperature=temperature)
            gen_time = time.time() - start
            
            print(f"\n  â”Œ{'â”€'*60}")
            print(f"  â”‚ ğŸ¯ çµæœ: {result}")
            print(f"  â””{'â”€'*60}")
            print(f"  â± ç”Ÿæˆæ™‚é–“: {gen_time:.3f}ç§’")
            
        except KeyboardInterrupt:
            print("\n  ã•ã‚ˆã†ãªã‚‰ï¼")
            break


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "-i":
        interactive_mode()
    else:
        demo()

