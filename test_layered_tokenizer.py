#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NeuroQuantum Layered Tokenizer ã®SentencePieceå¯¾å¿œãƒ†ã‚¹ãƒˆ
"""

import sys
sys.path.insert(0, '/home/user/NeuroQuantum/neuroq-runpod')

from neuroquantum_layered import NeuroQuantumTokenizer

def test_tokenizer():
    print("=" * 70)
    print("ğŸ”¤ NeuroQuantumTokenizer SentencePiece ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    # æ—¢å­˜ã®SentencePieceãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    tokenizer = NeuroQuantumTokenizer(
        vocab_size=8000,
        model_file="/home/user/NeuroQuantum/neuroq_tokenizer_8k.model"
    )

    print(f"\nâœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
    print(f"   èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}")
    print(f"   å®Ÿéš›ã®èªå½™ã‚µã‚¤ã‚º: {tokenizer.actual_vocab_size:,}")

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ:")
    print("-" * 70)

    test_texts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¤ã„ã¦æ•™ãˆã¦",
        "äººå·¥çŸ¥èƒ½ãŒæœªæ¥ã‚’å¤‰ãˆã¦ã„ãã¾ã™ã€‚",
        "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯å‰µé€ çš„ãªæ´»å‹•ã§ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯è¤‡é›‘ãªå•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚",
    ]

    for text in test_texts:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        tokens = tokenizer.encode(text)

        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        decoded = tokenizer.decode(tokens)

        print(f"\nåŸæ–‡: {text}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³ID: {tokens[:15]}{'...' if len(tokens) > 15 else ''}")
        print(f"ãƒ‡ã‚³ãƒ¼ãƒ‰: {decoded}")

    print("\n" + "=" * 70)
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†ï¼vocab_size=8000ã®SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
    print("=" * 70)

if __name__ == '__main__':
    test_tokenizer()
