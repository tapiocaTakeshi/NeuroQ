#!/usr/bin/env python3
"""
QBNN ã‚²ãƒ¼ãƒ AI - ä¸‰ç›®ä¸¦ã¹ (Tic-Tac-Toe)
========================================
QBNNã‚’ä½¿ã£ãŸæ•µCPUï¼ˆAIï¼‰ã¨ã®å¯¾æˆ¦ã‚²ãƒ¼ãƒ 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random
import math
from typing import List, Tuple, Optional


# ========================================
# QBNN Layerï¼ˆé‡å­ã‚‚ã¤ã‚Œå±¤ï¼‰
# ========================================

class QBNNLayer(nn.Module):
    """Quantum-Bit Neural Network Layer"""
    
    def __init__(self, input_dim: int, output_dim: int, 
                 lambda_min: float = 0.2, lambda_max: float = 0.5):
        super().__init__()
        self.lambda_min = lambda_min
        self.lambda_max = lambda_max
        
        self.W = nn.Linear(input_dim, output_dim)
        self.J = nn.Parameter(torch.randn(input_dim, output_dim) * 0.02)
        self.lambda_base = nn.Parameter(torch.tensor(0.5))
        self.layer_norm = nn.LayerNorm(output_dim)
        self.call_count = 0
    
    def forward(self, h_prev: torch.Tensor) -> torch.Tensor:
        s_prev = torch.tanh(h_prev)
        h_tilde = self.W(h_prev)
        s_raw = torch.tanh(h_tilde)
        
        # ã‚‚ã¤ã‚Œè£œæ­£
        delta = torch.einsum('...i,ij,...j->...j', s_prev, self.J, s_raw)
        
        # å‹•çš„Î»
        lambda_normalized = torch.sigmoid(self.lambda_base)
        if not self.training:
            phase = self.call_count * 0.2
            dynamic_factor = 0.5 + 0.5 * math.sin(phase)
            self.call_count += 1
        else:
            dynamic_factor = 0.5
        
        lambda_range = self.lambda_max - self.lambda_min
        lambda_eff = self.lambda_min + lambda_range * (lambda_normalized * 0.7 + dynamic_factor * 0.3)
        
        h_hat = h_tilde + lambda_eff * delta
        output = self.layer_norm(h_hat)
        return F.gelu(output)


# ========================================
# QBNN ã‚²ãƒ¼ãƒ AI
# ========================================

class QBNNGameAI(nn.Module):
    """ä¸‰ç›®ä¸¦ã¹ç”¨ã®QBNN AI"""
    
    def __init__(self, hidden_dim: int = 64):
        super().__init__()
        
        # å…¥åŠ›: 9ãƒã‚¹ Ã— 3çŠ¶æ…‹(ç©º/â—‹/Ã—) = 27æ¬¡å…ƒ
        # ã¾ãŸã¯ 9ãƒã‚¹ (0=ç©º, 1=è‡ªåˆ†, -1=ç›¸æ‰‹) = 9æ¬¡å…ƒ
        self.input_dim = 9
        self.hidden_dim = hidden_dim
        self.output_dim = 9  # å„ãƒã‚¹ã¸ã®æ‰“ã¤ç¢ºç‡
        
        # QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.qbnn1 = QBNNLayer(self.input_dim, hidden_dim)
        self.qbnn2 = QBNNLayer(hidden_dim, hidden_dim)
        self.qbnn3 = QBNNLayer(hidden_dim, self.output_dim)
        
        # å‡ºåŠ›å±¤
        self.output = nn.Linear(self.output_dim, self.output_dim)
    
    def forward(self, board_state: torch.Tensor) -> torch.Tensor:
        """
        board_state: (batch, 9) - ç›¤é¢çŠ¶æ…‹
        return: (batch, 9) - å„ãƒã‚¹ã¸ã®è©•ä¾¡å€¤
        """
        x = self.qbnn1(board_state)
        x = self.qbnn2(x)
        x = self.qbnn3(x)
        return self.output(x)
    
    def get_move(self, board: List[int], temperature: float = 0.5) -> int:
        """
        ç›¤é¢ã‹ã‚‰æ¬¡ã®æ‰‹ã‚’é¸æŠ
        board: [0]*9 (0=ç©º, 1=AI, -1=ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼)
        """
        self.eval()
        with torch.no_grad():
            state = torch.tensor(board, dtype=torch.float32).unsqueeze(0)
            logits = self.forward(state)[0]
            
            # æ‰“ã¦ãªã„ãƒã‚¹ã‚’ãƒã‚¹ã‚¯
            mask = torch.tensor([float('-inf') if b != 0 else 0 for b in board])
            logits = logits + mask
            
            # æ¸©åº¦ã§ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
            probs = F.softmax(logits / temperature, dim=0)
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆé‡å­çš„ãªä¸ç¢ºå®šæ€§ã‚’æ¨¡å€£ï¼‰
            if random.random() < 0.3:  # 30%ã®ç¢ºç‡ã§æ¢ç´¢
                valid_moves = [i for i, b in enumerate(board) if b == 0]
                return random.choice(valid_moves)
            else:
                return torch.argmax(probs).item()


# ========================================
# ä¸‰ç›®ä¸¦ã¹ã‚²ãƒ¼ãƒ 
# ========================================

class TicTacToe:
    """ä¸‰ç›®ä¸¦ã¹ã‚²ãƒ¼ãƒ ãƒ­ã‚¸ãƒƒã‚¯"""
    
    def __init__(self):
        self.board = [0] * 9  # 0=ç©º, 1=â—‹, -1=Ã—
        self.current_player = 1  # 1=â—‹(ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼), -1=Ã—(CPU)
        
    def reset(self):
        self.board = [0] * 9
        self.current_player = 1
        
    def display(self):
        """ç›¤é¢è¡¨ç¤º"""
        symbols = {0: 'Â·', 1: 'â—‹', -1: 'Ã—'}
        print("\n   1   2   3")
        for i in range(3):
            row = [symbols[self.board[i*3 + j]] for j in range(3)]
            print(f" {i+1} {row[0]} â”‚ {row[1]} â”‚ {row[2]}")
            if i < 2:
                print("   â”€â”€â”¼â”€â”€â”€â”¼â”€â”€")
        print()
    
    def make_move(self, position: int, player: int) -> bool:
        """æ‰‹ã‚’æ‰“ã¤ (position: 0-8)"""
        if self.board[position] == 0:
            self.board[position] = player
            return True
        return False
    
    def check_winner(self) -> Optional[int]:
        """å‹è€…ã‚’ãƒã‚§ãƒƒã‚¯ (1=â—‹å‹åˆ©, -1=Ã—å‹åˆ©, 0=å¼•ãåˆ†ã‘, None=ç¶šè¡Œ)"""
        # å‹åˆ©ãƒ©ã‚¤ãƒ³
        lines = [
            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # æ¨ª
            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # ç¸¦
            [0, 4, 8], [2, 4, 6]              # æ–œã‚
        ]
        
        for line in lines:
            s = sum(self.board[i] for i in line)
            if s == 3:
                return 1  # â—‹å‹åˆ©
            if s == -3:
                return -1  # Ã—å‹åˆ©
        
        # å¼•ãåˆ†ã‘ãƒã‚§ãƒƒã‚¯
        if 0 not in self.board:
            return 0
        
        return None  # ç¶šè¡Œ
    
    def get_valid_moves(self) -> List[int]:
        """æ‰“ã¦ã‚‹ãƒã‚¹ã®ãƒªã‚¹ãƒˆ"""
        return [i for i, b in enumerate(self.board) if b == 0]


# ========================================
# AIãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
# ========================================

class AITrainer:
    """è‡ªå·±å¯¾æˆ¦ã§AIã‚’è¨“ç·´"""
    
    def __init__(self, ai: QBNNGameAI):
        self.ai = ai
        self.optimizer = torch.optim.Adam(ai.parameters(), lr=0.01)
    
    def generate_training_data(self, num_games: int = 1000) -> List[Tuple]:
        """ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ¬ã‚¤ã§ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        data = []
        
        for _ in range(num_games):
            game = TicTacToe()
            history = []
            
            while True:
                valid_moves = game.get_valid_moves()
                if not valid_moves:
                    break
                
                move = random.choice(valid_moves)
                history.append((game.board.copy(), move, game.current_player))
                game.make_move(move, game.current_player)
                
                winner = game.check_winner()
                if winner is not None:
                    # çµæœã«åŸºã¥ã„ã¦å ±é…¬
                    for board, m, player in history:
                        if winner == 0:
                            reward = 0.3  # å¼•ãåˆ†ã‘
                        elif winner == player:
                            reward = 1.0  # å‹åˆ©
                        else:
                            reward = -0.5  # æ•—åŒ—
                        data.append((board, m, reward))
                    break
                
                game.current_player *= -1
        
        return data
    
    def train(self, epochs: int = 50, games_per_epoch: int = 200):
        """è¨“ç·´"""
        print("\nğŸ® QBNN ã‚²ãƒ¼ãƒ AI è¨“ç·´é–‹å§‹")
        print("=" * 50)
        
        for epoch in range(epochs):
            # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            data = self.generate_training_data(games_per_epoch)
            
            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            random.shuffle(data)
            
            total_loss = 0
            batch_size = 32
            
            for i in range(0, len(data), batch_size):
                batch = data[i:i+batch_size]
                
                boards = torch.tensor([d[0] for d in batch], dtype=torch.float32)
                moves = torch.tensor([d[1] for d in batch], dtype=torch.long)
                rewards = torch.tensor([d[2] for d in batch], dtype=torch.float32)
                
                self.optimizer.zero_grad()
                
                logits = self.ai(boards)
                log_probs = F.log_softmax(logits, dim=1)
                selected_log_probs = log_probs.gather(1, moves.unsqueeze(1)).squeeze()
                
                # Policy Gradient Loss
                loss = -(selected_log_probs * rewards).mean()
                
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
            
            if (epoch + 1) % 10 == 0 or epoch == 0:
                avg_loss = total_loss / (len(data) // batch_size + 1)
                print(f"   Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}")
        
        print("\nâœ… è¨“ç·´å®Œäº†ï¼")


# ========================================
# ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹AIï¼ˆæ¯”è¼ƒç”¨ï¼‰
# ========================================

class MinimaxAI:
    """å®Œç’§ãªAIï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
    
    def get_move(self, board: List[int], player: int = -1) -> int:
        """ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹ã§æœ€é©æ‰‹ã‚’è¿”ã™"""
        best_score = float('-inf')
        best_move = None
        
        for i in range(9):
            if board[i] == 0:
                board[i] = player
                score = self._minimax(board, 0, False, player)
                board[i] = 0
                
                if score > best_score:
                    best_score = score
                    best_move = i
        
        return best_move if best_move is not None else random.choice([i for i in range(9) if board[i] == 0])
    
    def _minimax(self, board: List[int], depth: int, is_maximizing: bool, player: int) -> int:
        winner = self._check_winner(board)
        if winner == player:
            return 10 - depth
        elif winner == -player:
            return depth - 10
        elif 0 not in board:
            return 0
        
        if is_maximizing:
            best = float('-inf')
            for i in range(9):
                if board[i] == 0:
                    board[i] = player
                    best = max(best, self._minimax(board, depth + 1, False, player))
                    board[i] = 0
            return best
        else:
            best = float('inf')
            for i in range(9):
                if board[i] == 0:
                    board[i] = -player
                    best = min(best, self._minimax(board, depth + 1, True, player))
                    board[i] = 0
            return best
    
    def _check_winner(self, board: List[int]) -> Optional[int]:
        lines = [[0,1,2],[3,4,5],[6,7,8],[0,3,6],[1,4,7],[2,5,8],[0,4,8],[2,4,6]]
        for line in lines:
            s = sum(board[i] for i in line)
            if s == 3: return 1
            if s == -3: return -1
        return None


# ========================================
# ã‚²ãƒ¼ãƒ ãƒ—ãƒ¬ã‚¤
# ========================================

def play_game():
    """äººé–“ vs QBNN AI"""
    print("=" * 60)
    print("ğŸ® ä¸‰ç›®ä¸¦ã¹ - QBNN AIå¯¾æˆ¦")
    print("=" * 60)
    print("\nQBNNï¼ˆé‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰AIã¨å¯¾æˆ¦ã—ã¾ã™ï¼")
    print("ã‚ãªãŸã¯ â—‹ ã€CPUã¯ Ã— ã§ã™ã€‚")
    print("ãƒã‚¹ã¯ è¡Œ,åˆ— (ä¾‹: 1,1 = å·¦ä¸Š) ã§æŒ‡å®šã—ã¾ã™ã€‚")
    
    # AIä½œæˆã¨è¨“ç·´
    ai = QBNNGameAI(hidden_dim=64)
    trainer = AITrainer(ai)
    trainer.train(epochs=30, games_per_epoch=300)
    
    # çµ±è¨ˆ
    stats = {'player': 0, 'cpu': 0, 'draw': 0}
    
    while True:
        game = TicTacToe()
        print("\n" + "=" * 40)
        print("ğŸ†• æ–°ã—ã„ã‚²ãƒ¼ãƒ é–‹å§‹ï¼")
        print("=" * 40)
        game.display()
        
        while True:
            if game.current_player == 1:
                # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ‰‹ç•ª
                print("ğŸ‘¤ ã‚ãªãŸã®ç•ªã§ã™ (â—‹)")
                while True:
                    try:
                        inp = input("   ãƒã‚¹ã‚’å…¥åŠ› (è¡Œ,åˆ— ä¾‹: 2,2): ").strip()
                        if inp.lower() in ['q', 'quit', 'exit']:
                            print("\nğŸ“Š æœ€çµ‚çµæœ:")
                            print(f"   ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼å‹åˆ©: {stats['player']}")
                            print(f"   CPUå‹åˆ©: {stats['cpu']}")
                            print(f"   å¼•ãåˆ†ã‘: {stats['draw']}")
                            return
                        
                        row, col = map(int, inp.split(','))
                        pos = (row - 1) * 3 + (col - 1)
                        
                        if 0 <= pos < 9 and game.board[pos] == 0:
                            game.make_move(pos, 1)
                            break
                        else:
                            print("   âŒ ãã®ãƒã‚¹ã«ã¯æ‰“ã¦ã¾ã›ã‚“ï¼")
                    except:
                        print("   âŒ å…¥åŠ›å½¢å¼: è¡Œ,åˆ— (ä¾‹: 1,2)")
            else:
                # CPUã®æ‰‹ç•ª
                print("ğŸ¤– QBNN CPU ã®ç•ªã§ã™ (Ã—)...")
                
                # AIã‹ã‚‰ç›¤é¢ã‚’è¦‹ãŸçŠ¶æ…‹ï¼ˆAIã¯-1ã¨ã—ã¦æ‰“ã¤ï¼‰
                ai_board = [-b for b in game.board]  # è¦–ç‚¹å¤‰æ›
                pos = ai.get_move(ai_board, temperature=0.3)
                
                game.make_move(pos, -1)
                r, c = pos // 3 + 1, pos % 3 + 1
                print(f"   â†’ CPU: ({r},{c})")
            
            game.display()
            
            # å‹æ•—ãƒã‚§ãƒƒã‚¯
            winner = game.check_winner()
            if winner is not None:
                if winner == 1:
                    print("ğŸ‰ ã‚ãªãŸã®å‹ã¡ï¼")
                    stats['player'] += 1
                elif winner == -1:
                    print("ğŸ’» CPUã®å‹ã¡ï¼")
                    stats['cpu'] += 1
                else:
                    print("ğŸ¤ å¼•ãåˆ†ã‘ï¼")
                    stats['draw'] += 1
                
                print(f"\nğŸ“Š æˆ¦ç¸¾: ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ {stats['player']} - {stats['cpu']} CPU (å¼•åˆ†: {stats['draw']})")
                break
            
            game.current_player *= -1
        
        # ç¶šã‘ã‚‹ã‹
        cont = input("\nç¶šã‘ã¾ã™ã‹? (y/n): ").strip().lower()
        if cont != 'y':
            print("\nğŸ“Š æœ€çµ‚çµæœ:")
            print(f"   ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼å‹åˆ©: {stats['player']}")
            print(f"   CPUå‹åˆ©: {stats['cpu']}")
            print(f"   å¼•ãåˆ†ã‘: {stats['draw']}")
            break


def benchmark_ai():
    """AIã®å¼·ã•ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("=" * 60)
    print("ğŸ“Š QBNN AI ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 60)
    
    # QBNN AI
    qbnn_ai = QBNNGameAI(hidden_dim=64)
    trainer = AITrainer(qbnn_ai)
    trainer.train(epochs=50, games_per_epoch=500)
    
    # ãƒ©ãƒ³ãƒ€ãƒ AI
    def random_ai(board):
        valid = [i for i, b in enumerate(board) if b == 0]
        return random.choice(valid) if valid else 0
    
    # ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹AI
    minimax_ai = MinimaxAI()
    
    def play_match(ai1_func, ai2_func, num_games=100):
        """å¯¾æˆ¦"""
        wins = [0, 0, 0]  # ai1å‹, ai2å‹, å¼•åˆ†
        
        for _ in range(num_games):
            game = TicTacToe()
            current_ai = 0  # 0=ai1, 1=ai2
            
            while True:
                if current_ai == 0:
                    move = ai1_func(game.board.copy())
                else:
                    move = ai2_func(game.board.copy())
                
                player = 1 if current_ai == 0 else -1
                game.make_move(move, player)
                
                winner = game.check_winner()
                if winner is not None:
                    if winner == 1:
                        wins[0] += 1
                    elif winner == -1:
                        wins[1] += 1
                    else:
                        wins[2] += 1
                    break
                
                current_ai = 1 - current_ai
        
        return wins
    
    print("\nğŸ® å¯¾æˆ¦çµæœ:")
    
    # QBNN vs ãƒ©ãƒ³ãƒ€ãƒ 
    def qbnn_move(board):
        ai_board = [-b for b in board]
        return qbnn_ai.get_move(ai_board, temperature=0.2)
    
    results = play_match(qbnn_move, random_ai, 100)
    print(f"\n   QBNN vs ãƒ©ãƒ³ãƒ€ãƒ : {results[0]}å‹-{results[1]}æ•—-{results[2]}åˆ†")
    qbnn_vs_random = results[0] / (results[0] + results[1] + 0.001) * 100
    print(f"   â†’ QBNNå‹ç‡: {qbnn_vs_random:.1f}%")
    
    # QBNN vs ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹
    def minimax_move(board):
        return minimax_ai.get_move(board, player=-1)
    
    results = play_match(qbnn_move, minimax_move, 100)
    print(f"\n   QBNN vs ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹(å®Œç’§AI): {results[0]}å‹-{results[1]}æ•—-{results[2]}åˆ†")
    
    # ãƒ©ãƒ³ãƒ€ãƒ  vs ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹ï¼ˆå‚è€ƒï¼‰
    results = play_match(random_ai, minimax_move, 100)
    print(f"\n   ãƒ©ãƒ³ãƒ€ãƒ  vs ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹(å‚è€ƒ): {results[0]}å‹-{results[1]}æ•—-{results[2]}åˆ†")
    
    print("\nâœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ï¼")


if __name__ == '__main__':
    import sys
    
    print("=" * 60)
    print("ğŸ® QBNN ã‚²ãƒ¼ãƒ AI - ä¸‰ç›®ä¸¦ã¹")
    print("=" * 60)
    print("\né¸æŠã—ã¦ãã ã•ã„:")
    print("  1. äººé–“ vs CPU ã§å¯¾æˆ¦")
    print("  2. AIãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆå¼·ã•æ¸¬å®šï¼‰")
    print("  3. ä¸¡æ–¹")
    
    choice = input("\né¸æŠ (1/2/3): ").strip()
    
    if choice == '1':
        play_game()
    elif choice == '2':
        benchmark_ai()
    else:
        benchmark_ai()
        print("\n" + "=" * 60)
        play_game()

