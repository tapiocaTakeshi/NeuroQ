ã©ã®è‰¯ããªã£ã¦ã„ã‚‹ã®ï¼Ÿã§ã‚‚ã€‚#!/usr/bin/env python3
"""
QBNN (Quantum-Bit Neural Network) - PyTorchå®Ÿè£…

æ•°å¼ãƒ¢ãƒ‡ãƒ«:
1. s^(l) = normalize(h^(l)) âˆˆ [-1,1]           (æ­£è¦åŒ–)
2. Î¸^(l)_i = arccos(s^(l)_i)                   (Blochè§’)
3. Î”^(l+1)_j = Î£_i J_{ij} s^(l)_i s^(l+1)_{raw,j}  (ã‚‚ã¤ã‚Œè£œæ­£)
4. Ä¥^(l+1) = hÌƒ^(l+1) + Î» Î”^(l+1)             (æœ‰åŠ¹å…¥åŠ›)
5. h^(l+1) = Ïƒ(Ä¥^(l+1))                       (æ´»æ€§åŒ–)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

print("=" * 70)
print("ğŸ§ âš›ï¸ QBNN (Quantum-Bit Neural Network) - PyTorchå®Ÿè£…")
print("=" * 70)


class QBNNLayer(nn.Module):
    """
    Quantum-Bit Neural Network Layer
    
    å„å±¤ãƒ»å„ãƒ“ãƒƒãƒˆãŒåˆ†å­ã®ã‚ˆã†ã«ã‚‚ã¤ã‚Œåˆã†å±¤
    
    æ•°å¼:
    - ç·šå½¢å¤‰æ›: hÌƒ^(l+1) = W h^(l) + b
    - æ­£è¦åŒ–: s = tanh(h) âˆˆ [-1, 1]
    - ã‚‚ã¤ã‚Œè£œæ­£: Î”_j = Î£_i J_{ij} s^(l)_i s^(l+1)_{raw,j}
    - æœ‰åŠ¹å…¥åŠ›: Ä¥ = hÌƒ + Î» Î”
    - å‡ºåŠ›: h = tanh(Ä¥)
    """
    
    def __init__(self, input_dim, output_dim, lambda_entangle=0.5):
        super().__init__()
        
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.lambda_entangle = nn.Parameter(torch.tensor(lambda_entangle))
        
        # é‡ã¿ W^(l)
        self.W = nn.Linear(input_dim, output_dim)
        
        # ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ« J^(l) - å­¦ç¿’å¯èƒ½
        self.J = nn.Parameter(torch.randn(input_dim, output_dim) * 0.1)
        
        # çŠ¶æ…‹ä¿æŒï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ»å¯è¦–åŒ–ç”¨ï¼‰
        self.s_prev = None  # æ­£è¦åŒ–ã•ã‚ŒãŸå‰ã®å±¤
        self.s_raw = None   # æ­£è¦åŒ–ã•ã‚ŒãŸç”Ÿã®å€™è£œ
        self.theta = None   # Blochè§’
        self.delta = None   # ã‚‚ã¤ã‚Œè£œæ­£
    
    def normalize(self, h):
        """h ã‚’ [-1, 1] ã«æ­£è¦åŒ– (tanh)"""
        return torch.tanh(h)
    
    def compute_bloch_angle(self, s):
        """æ­£è¦åŒ–ã•ã‚ŒãŸå€¤ s ã‹ã‚‰Blochè§’ Î¸ ã‚’è¨ˆç®—"""
        # arccos ã®å…¥åŠ›ã‚’ [-1, 1] ã«ã‚¯ãƒ©ãƒ³ãƒ—
        s_clamped = torch.clamp(s, -1 + 1e-7, 1 - 1e-7)
        return torch.acos(s_clamped)
    
    def forward(self, h_prev):
        """
        é †ä¼æ’­
        
        Args:
            h_prev: å‰ã®å±¤ã®å‡ºåŠ› [batch, input_dim]
        
        Returns:
            h: ã“ã®å±¤ã®å‡ºåŠ› [batch, output_dim]
        """
        # Step 1: å‰ã®å±¤ã®æ­£è¦åŒ– s^(l)
        self.s_prev = self.normalize(h_prev)
        
        # Step 2: ç·šå½¢å¤‰æ› hÌƒ^(l+1) = W h^(l) + b
        h_tilde = self.W(h_prev)
        
        # Step 3: æ­£è¦åŒ–ï¼ˆç”Ÿã®å€™è£œï¼‰s^(l+1)_raw
        self.s_raw = self.normalize(h_tilde)
        
        # Step 4: ã‚‚ã¤ã‚Œè£œæ­£ Î”^(l+1)
        # Î”_j = Î£_i J_{ij} s^(l)_i s^(l+1)_{raw,j}
        # = (s_prev @ J) * s_raw
        interaction = torch.matmul(self.s_prev, self.J)  # [batch, output_dim]
        self.delta = interaction * self.s_raw
        
        # Step 5: æœ‰åŠ¹å…¥åŠ› Ä¥^(l+1) = hÌƒ^(l+1) + Î» Î”^(l+1)
        h_hat = h_tilde + self.lambda_entangle * self.delta
        
        # Step 6: æ´»æ€§åŒ– h^(l+1) = tanh(Ä¥^(l+1))
        h = torch.tanh(h_hat)
        
        # Blochè§’ã‚’ä¿å­˜
        s_out = self.normalize(h)
        self.theta = self.compute_bloch_angle(s_out)
        
        return h
    
    def measure(self):
        """
        é‡å­æ¸¬å®šã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        Returns:
            measurements: æ¸¬å®šçµæœ (0 or 1) [batch, output_dim]
        """
        if self.theta is None:
            return None
        
        # P(|1âŸ©) = sinÂ²(Î¸/2)
        p1 = torch.sin(self.theta / 2) ** 2
        measurements = (torch.rand_like(p1) < p1).float()
        
        return measurements
    
    def get_entanglement_strength(self):
        """ã‚‚ã¤ã‚Œå¼·åº¦ã‚’å–å¾—"""
        return self.lambda_entangle.item()
    
    def get_coupling_matrix(self):
        """çµåˆè¡Œåˆ— J ã‚’å–å¾—"""
        return self.J.detach().cpu().numpy()


class QBNN(nn.Module):
    """
    Quantum-Bit Neural Network
    
    è¤‡æ•°ã®QBNNLayerã‚’ç©ã¿é‡ã­ãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    """
    
    def __init__(self, layer_dims, lambda_entangle=0.5, lambda_decay=0.1):
        """
        Args:
            layer_dims: å„å±¤ã®æ¬¡å…ƒ [input, hidden1, hidden2, ..., output]
            lambda_entangle: åˆæœŸã‚‚ã¤ã‚Œå¼·åº¦
            lambda_decay: æ·±ã„å±¤ã»ã©ã‚‚ã¤ã‚Œã‚’å¼±ãã™ã‚‹ä¿‚æ•°
        """
        super().__init__()
        
        self.layer_dims = layer_dims
        self.num_layers = len(layer_dims) - 1
        
        # QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.layers = nn.ModuleList()
        for i in range(self.num_layers):
            # æ·±ã„å±¤ã»ã©Î»ã‚’å°ã•ã
            layer_lambda = lambda_entangle * (1 - i * lambda_decay)
            layer = QBNNLayer(layer_dims[i], layer_dims[i + 1], layer_lambda)
            self.layers.append(layer)
    
    def forward(self, x):
        """é †ä¼æ’­"""
        h = x
        for layer in self.layers:
            h = layer(h)
        return h
    
    def measure_all(self):
        """å…¨å±¤ã®æ¸¬å®šçµæœã‚’å–å¾—"""
        measurements = []
        for layer in self.layers:
            m = layer.measure()
            if m is not None:
                measurements.append(m)
        return measurements
    
    def get_all_thetas(self):
        """å…¨å±¤ã®Blochè§’ã‚’å–å¾—"""
        thetas = []
        for layer in self.layers:
            if layer.theta is not None:
                thetas.append(layer.theta.detach())
        return thetas
    
    def get_entanglement_info(self):
        """ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—"""
        info = []
        for i, layer in enumerate(self.layers):
            s = layer.s_raw
            if s is not None:
                theta = layer.theta
                r = torch.cos(2 * theta).mean().item() if theta is not None else 0
                T = torch.abs(torch.sin(2 * theta)).mean().item() if theta is not None else 0
                info.append({
                    'layer': i,
                    'lambda': layer.get_entanglement_strength(),
                    'r_mean': r,
                    'T_mean': T,
                    'r2_T2': r**2 + T**2
                })
        return info


class QBNNClassifier(nn.Module):
    """
    QBNNåˆ†é¡å™¨
    
    QBNNã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸåˆ†é¡ãƒ¢ãƒ‡ãƒ«
    """
    
    def __init__(self, input_dim, hidden_dims, num_classes, lambda_entangle=0.5):
        super().__init__()
        
        # QBNN
        layer_dims = [input_dim] + hidden_dims + [num_classes]
        self.qbnn = QBNN(layer_dims, lambda_entangle)
        
        # å‡ºåŠ›å±¤ï¼ˆã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹å‰ï¼‰
        self.output = nn.Linear(num_classes, num_classes)
    
    def forward(self, x):
        h = self.qbnn(x)
        logits = self.output(h)
        return logits
    
    def predict(self, x):
        logits = self.forward(x)
        return torch.argmax(logits, dim=-1)


# ========================================
# ãƒ†ã‚¹ãƒˆã¨ãƒ‡ãƒ¢
# ========================================

def test_xor_problem():
    """XORå•é¡Œã§QBNNã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“Š XORå•é¡Œã§QBNNã‚’ãƒ†ã‚¹ãƒˆ")
    print("-" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿
    X = torch.tensor([
        [0.0, 0.0],
        [0.0, 1.0],
        [1.0, 0.0],
        [1.0, 1.0]
    ])
    y = torch.tensor([0, 1, 1, 0])
    
    # ãƒ¢ãƒ‡ãƒ«
    model = QBNNClassifier(
        input_dim=2,
        hidden_dims=[8, 8],
        num_classes=2,
        lambda_entangle=0.5
    )
    
    # å­¦ç¿’
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    losses = []
    for epoch in range(500):
        optimizer.zero_grad()
        logits = model(X)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        
        if (epoch + 1) % 100 == 0:
            pred = model.predict(X)
            acc = (pred == y).float().mean()
            print(f"   Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc.item():.2%}")
    
    # æœ€çµ‚çµæœ
    pred = model.predict(X)
    print(f"\n   æœ€çµ‚äºˆæ¸¬: {pred.tolist()}")
    print(f"   æ­£è§£: {y.tolist()}")
    
    # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆæƒ…å ±
    print("\n   ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆæƒ…å ±:")
    _ = model(X)  # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
    info = model.qbnn.get_entanglement_info()
    for i in info:
        print(f"   Layer {i['layer']}: Î»={i['lambda']:.3f}, r={i['r_mean']:.3f}, T={i['T_mean']:.3f}")
    
    return model, losses


def visualize_qbnn(model, X):
    """QBNNã®çŠ¶æ…‹ã‚’å¯è¦–åŒ–"""
    print("\nğŸ“ˆ QBNNçŠ¶æ…‹ã®å¯è¦–åŒ–")
    
    # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
    _ = model(X)
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. å„å±¤ã®Blochè§’åˆ†å¸ƒ
    ax = axes[0, 0]
    thetas = model.qbnn.get_all_thetas()
    for i, theta in enumerate(thetas):
        theta_np = theta.mean(dim=0).numpy()
        ax.bar(np.arange(len(theta_np)) + i * 0.2, theta_np, 
               width=0.2, label=f'Layer {i}', alpha=0.7)
    ax.set_xlabel('Neuron Index')
    ax.set_ylabel('Î¸ (Bloch Angle)')
    ax.set_title('Bloch Angles per Layer')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. r-Tå¹³é¢
    ax = axes[0, 1]
    info = model.qbnn.get_entanglement_info()
    
    # åˆ¶ç´„æ›²ç·š
    theta_range = np.linspace(0, np.pi/2, 100)
    r_curve = np.cos(2 * theta_range)
    T_curve = np.abs(np.sin(2 * theta_range))
    ax.plot(r_curve, T_curve, 'b-', linewidth=2, label='rÂ² + TÂ² = 1')
    
    # å„å±¤ã®ãƒ—ãƒ­ãƒƒãƒˆ
    colors = plt.cm.viridis(np.linspace(0, 1, len(info)))
    for i, inf in enumerate(info):
        ax.scatter([inf['r_mean']], [inf['T_mean']], s=200, c=[colors[i]], 
                   label=f"Layer {i} (Î»={inf['lambda']:.2f})", zorder=5)
    ax.set_xlabel('r (Correlation)')
    ax.set_ylabel('T (Temperature)')
    ax.set_title('Layer States on r-T Plane')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. çµåˆè¡Œåˆ— J ã®å¯è¦–åŒ–
    ax = axes[1, 0]
    J = model.qbnn.layers[0].get_coupling_matrix()
    im = ax.imshow(J, cmap='coolwarm', aspect='auto')
    ax.set_xlabel('Output Neuron')
    ax.set_ylabel('Input Neuron')
    ax.set_title('Coupling Matrix J (Layer 0)')
    plt.colorbar(im, ax=ax)
    
    # 4. æ¸¬å®šçµæœ
    ax = axes[1, 1]
    measurements = model.qbnn.measure_all()
    if measurements:
        for i, m in enumerate(measurements):
            m_np = m.mean(dim=0).numpy()
            ax.bar(np.arange(len(m_np)) + i * 0.25, m_np, 
                   width=0.25, label=f'Layer {i}', alpha=0.7)
    ax.set_xlabel('Neuron Index')
    ax.set_ylabel('P(|1âŸ©)')
    ax.set_title('Measurement Probabilities')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.suptitle('QBNN (Quantum-Bit Neural Network) Visualization', fontsize=14)
    plt.tight_layout()
    plt.savefig('/Users/yuyahiguchi/Program/Qubit/qbnn_pytorch_viz.png', dpi=150, bbox_inches='tight')
    print("   ä¿å­˜: qbnn_pytorch_viz.png")
    plt.close()


def print_model_summary():
    """ãƒ¢ãƒ‡ãƒ«ã®æ•°å¼ã‚µãƒãƒªãƒ¼"""
    print("\n" + "=" * 70)
    print("ğŸ“š QBNN æ•°å¼ãƒ¢ãƒ‡ãƒ« ã‚µãƒãƒªãƒ¼")
    print("=" * 70)
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  QBNN (Quantum-Bit Neural Network) æ•°å¼ãƒ¢ãƒ‡ãƒ«                   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                 â”‚
    â”‚  1. æ­£è¦åŒ–                                                      â”‚
    â”‚     s^(l) = tanh(h^(l)) âˆˆ [-1, 1]                              â”‚
    â”‚                                                                 â”‚
    â”‚  2. Blochè§’                                                     â”‚
    â”‚     Î¸^(l)_i = arccos(s^(l)_i)                                  â”‚
    â”‚                                                                 â”‚
    â”‚  3. ç·šå½¢å¤‰æ›                                                    â”‚
    â”‚     hÌƒ^(l+1) = W^(l) h^(l) + b^(l)                              â”‚
    â”‚                                                                 â”‚
    â”‚  4. ã‚‚ã¤ã‚Œè£œæ­£ï¼ˆé‡å­ç›¸äº’ä½œç”¨ï¼‰                                  â”‚
    â”‚     Î”^(l+1)_j = Î£_i J^(l)_{ij} s^(l)_i s^(l+1)_{raw,j}         â”‚
    â”‚                                                                 â”‚
    â”‚  5. æœ‰åŠ¹å…¥åŠ›                                                    â”‚
    â”‚     Ä¥^(l+1) = hÌƒ^(l+1) + Î»^(l) Î”^(l+1)                          â”‚
    â”‚                                                                 â”‚
    â”‚  6. æ´»æ€§åŒ–                                                      â”‚
    â”‚     h^(l+1) = tanh(Ä¥^(l+1))                                    â”‚
    â”‚                                                                 â”‚
    â”‚  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:                                                     â”‚
    â”‚     W^(l) ... é€šå¸¸ã®é‡ã¿è¡Œåˆ—                                    â”‚
    â”‚     J^(l) ... ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ«ï¼ˆé‡å­çµåˆï¼‰                        â”‚
    â”‚     Î»^(l) ... ã‚‚ã¤ã‚Œå¼·åº¦ï¼ˆ0ã§é€šå¸¸NNã«æˆ»ã‚‹ï¼‰                     â”‚
    â”‚                                                                 â”‚
    â”‚  é‡å­çš„è§£é‡ˆ:                                                     â”‚
    â”‚     - å„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ = 1é‡å­ãƒ“ãƒƒãƒˆ                                â”‚
    â”‚     - s_i = âŸ¨Z_iâŸ© (zæœŸå¾…å€¤ = Blochçƒã®zåº§æ¨™)                   â”‚
    â”‚     - J_{ij} = åˆ†å­çµåˆã®å¼·ã•                                   â”‚
    â”‚     - Î” = å±¤é–“ã®é‡å­ã‚‚ã¤ã‚Œã«ã‚ˆã‚‹è£œæ­£                            â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


def test_logic_gates():
    """è«–ç†ã‚²ãƒ¼ãƒˆï¼ˆAND, OR, NAND, NORï¼‰ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“Š è«–ç†ã‚²ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ")
    print("-" * 50)
    
    gates = {
        'AND': torch.tensor([0, 0, 0, 1]),
        'OR': torch.tensor([0, 1, 1, 1]),
        'NAND': torch.tensor([1, 1, 1, 0]),
        'NOR': torch.tensor([1, 0, 0, 0]),
        'XOR': torch.tensor([0, 1, 1, 0]),
        'XNOR': torch.tensor([1, 0, 0, 1])
    }
    
    X = torch.tensor([
        [0.0, 0.0],
        [0.0, 1.0],
        [1.0, 0.0],
        [1.0, 1.0]
    ])
    
    results = {}
    
    for gate_name, y in gates.items():
        model = QBNNClassifier(
            input_dim=2,
            hidden_dims=[8, 8],
            num_classes=2,
            lambda_entangle=0.5
        )
        
        optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(300):
            optimizer.zero_grad()
            logits = model(X)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()
        
        pred = model.predict(X)
        acc = (pred == y).float().mean().item()
        results[gate_name] = acc
        
        status = "âœ…" if acc == 1.0 else "âŒ"
        print(f"   {gate_name}: {acc:.0%} {status}  äºˆæ¸¬={pred.tolist()} æ­£è§£={y.tolist()}")
    
    return results


def test_circle_classification():
    """å††å½¢åˆ†é¡å•é¡Œï¼ˆéç·šå½¢ï¼‰ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“Š å††å½¢åˆ†é¡å•é¡Œï¼ˆéç·šå½¢ï¼‰")
    print("-" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 200
    
    # å†…å´ã®å††ï¼ˆã‚¯ãƒ©ã‚¹0ï¼‰
    r_inner = np.random.uniform(0, 0.4, n_samples // 2)
    theta_inner = np.random.uniform(0, 2 * np.pi, n_samples // 2)
    X_inner = np.stack([r_inner * np.cos(theta_inner), r_inner * np.sin(theta_inner)], axis=1)
    
    # å¤–å´ã®å††ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰
    r_outer = np.random.uniform(0.6, 1.0, n_samples // 2)
    theta_outer = np.random.uniform(0, 2 * np.pi, n_samples // 2)
    X_outer = np.stack([r_outer * np.cos(theta_outer), r_outer * np.sin(theta_outer)], axis=1)
    
    X = torch.tensor(np.vstack([X_inner, X_outer]), dtype=torch.float32)
    y = torch.tensor([0] * (n_samples // 2) + [1] * (n_samples // 2))
    
    # QBNNãƒ¢ãƒ‡ãƒ«
    model = QBNNClassifier(
        input_dim=2,
        hidden_dims=[16, 16],
        num_classes=2,
        lambda_entangle=0.5
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(500):
        optimizer.zero_grad()
        logits = model(X)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            pred = model.predict(X)
            acc = (pred == y).float().mean()
            print(f"   Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc.item():.2%}")
    
    pred = model.predict(X)
    final_acc = (pred == y).float().mean().item()
    print(f"   æœ€çµ‚ç²¾åº¦: {final_acc:.2%}")
    
    return model, X, y, final_acc


def test_regression():
    """å›å¸°å•é¡Œï¼ˆsiné–¢æ•°ã®è¿‘ä¼¼ï¼‰ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“Š å›å¸°å•é¡Œï¼ˆsiné–¢æ•°è¿‘ä¼¼ï¼‰")
    print("-" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    X = torch.linspace(-np.pi, np.pi, 100).unsqueeze(1)
    y = torch.sin(X)
    
    # QBNNãƒ™ãƒ¼ã‚¹ã®å›å¸°ãƒ¢ãƒ‡ãƒ«
    class QBNNRegressor(nn.Module):
        def __init__(self, hidden_dims=[32, 32], lambda_entangle=0.5):
            super().__init__()
            layer_dims = [1] + hidden_dims + [1]
            self.qbnn = QBNN(layer_dims, lambda_entangle)
        
        def forward(self, x):
            return self.qbnn(x)
    
    model = QBNNRegressor(hidden_dims=[32, 32], lambda_entangle=0.3)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.MSELoss()
    
    for epoch in range(1000):
        optimizer.zero_grad()
        pred = model(X)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 200 == 0:
            print(f"   Epoch {epoch+1}: MSE Loss={loss.item():.6f}")
    
    # è©•ä¾¡
    with torch.no_grad():
        pred = model(X)
        mse = criterion(pred, y).item()
        r2 = 1 - mse / y.var().item()
    
    print(f"   æœ€çµ‚MSE: {mse:.6f}")
    print(f"   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    
    return model, X, y, r2


def test_multiclass():
    """å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“Š å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰")
    print("-" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ3ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰
    np.random.seed(42)
    n_per_class = 50
    
    centers = [(-1, -1), (1, -1), (0, 1)]
    X_list = []
    y_list = []
    
    for i, (cx, cy) in enumerate(centers):
        X_class = np.random.randn(n_per_class, 2) * 0.3 + np.array([cx, cy])
        X_list.append(X_class)
        y_list.extend([i] * n_per_class)
    
    X = torch.tensor(np.vstack(X_list), dtype=torch.float32)
    y = torch.tensor(y_list)
    
    # QBNNãƒ¢ãƒ‡ãƒ«
    model = QBNNClassifier(
        input_dim=2,
        hidden_dims=[16, 16],
        num_classes=3,
        lambda_entangle=0.5
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(500):
        optimizer.zero_grad()
        logits = model(X)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            pred = model.predict(X)
            acc = (pred == y).float().mean()
            print(f"   Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc.item():.2%}")
    
    pred = model.predict(X)
    final_acc = (pred == y).float().mean().item()
    print(f"   æœ€çµ‚ç²¾åº¦: {final_acc:.2%}")
    
    return model, X, y, final_acc


def test_lambda_comparison():
    """Î»ï¼ˆã‚‚ã¤ã‚Œå¼·åº¦ï¼‰ã®åŠ¹æœã‚’æ¯”è¼ƒ"""
    print("\nğŸ“Š Î»ï¼ˆã‚‚ã¤ã‚Œå¼·åº¦ï¼‰ã®åŠ¹æœæ¯”è¼ƒ")
    print("-" * 50)
    
    # XORå•é¡Œã§æ¯”è¼ƒ
    X = torch.tensor([
        [0.0, 0.0],
        [0.0, 1.0],
        [1.0, 0.0],
        [1.0, 1.0]
    ])
    y = torch.tensor([0, 1, 1, 0])
    
    lambdas = [0.0, 0.25, 0.5, 0.75, 1.0]
    results = {}
    
    for lam in lambdas:
        model = QBNNClassifier(
            input_dim=2,
            hidden_dims=[8, 8],
            num_classes=2,
            lambda_entangle=lam
        )
        
        optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
        criterion = nn.CrossEntropyLoss()
        
        final_loss = None
        for epoch in range(300):
            optimizer.zero_grad()
            logits = model(X)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()
            final_loss = loss.item()
        
        pred = model.predict(X)
        acc = (pred == y).float().mean().item()
        results[lam] = {'acc': acc, 'loss': final_loss}
        
        status = "âœ…" if acc == 1.0 else "âŒ"
        print(f"   Î»={lam:.2f}: ç²¾åº¦={acc:.0%} {status}, Loss={final_loss:.4f}")
    
    return results


def visualize_all_tests(circle_model, circle_X, circle_y, 
                        reg_model, reg_X, reg_y):
    """å…¨ãƒ†ã‚¹ãƒˆçµæœã‚’å¯è¦–åŒ–"""
    print("\nğŸ“ˆ å…¨ãƒ†ã‚¹ãƒˆçµæœã®å¯è¦–åŒ–")
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. å††å½¢åˆ†é¡
    ax = axes[0, 0]
    X_np = circle_X.numpy()
    y_np = circle_y.numpy()
    pred = circle_model.predict(circle_X).numpy()
    
    colors = ['blue' if p == 0 else 'red' for p in pred]
    ax.scatter(X_np[:, 0], X_np[:, 1], c=colors, alpha=0.6)
    
    # æ±ºå®šå¢ƒç•Œ
    xx, yy = np.meshgrid(np.linspace(-1.2, 1.2, 50), np.linspace(-1.2, 1.2, 50))
    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)
    with torch.no_grad():
        Z = circle_model.predict(grid).numpy().reshape(xx.shape)
    ax.contourf(xx, yy, Z, alpha=0.2, cmap='coolwarm')
    ax.set_title('Circle Classification (QBNN)')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    
    # 2. å›å¸°
    ax = axes[0, 1]
    X_np = reg_X.numpy()
    y_np = reg_y.numpy()
    with torch.no_grad():
        pred = reg_model(reg_X).numpy()
    
    ax.plot(X_np, y_np, 'b-', linewidth=2, label='True (sin)')
    ax.plot(X_np, pred, 'r--', linewidth=2, label='QBNN Prediction')
    ax.set_title('Regression: sin(x) Approximation')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. è«–ç†ã‚²ãƒ¼ãƒˆçµæœ
    ax = axes[1, 0]
    gates = ['AND', 'OR', 'NAND', 'NOR', 'XOR', 'XNOR']
    # çµæœã‚’å†ãƒ†ã‚¹ãƒˆ
    gate_results = test_logic_gates_silent()
    accs = [gate_results[g] for g in gates]
    colors_bar = ['green' if a == 1.0 else 'orange' for a in accs]
    ax.bar(gates, accs, color=colors_bar, alpha=0.7)
    ax.set_ylim(0, 1.1)
    ax.set_title('Logic Gates Accuracy')
    ax.set_ylabel('Accuracy')
    ax.axhline(1.0, color='green', linestyle='--', alpha=0.5)
    
    # 4. Î»ã®åŠ¹æœ
    ax = axes[1, 1]
    lambda_results = test_lambda_silent()
    lambdas = list(lambda_results.keys())
    accs = [lambda_results[l]['acc'] for l in lambdas]
    losses = [lambda_results[l]['loss'] for l in lambdas]
    
    ax2 = ax.twinx()
    ax.bar(range(len(lambdas)), accs, color='blue', alpha=0.5, label='Accuracy')
    ax2.plot(range(len(lambdas)), losses, 'ro-', label='Loss')
    ax.set_xticks(range(len(lambdas)))
    ax.set_xticklabels([f'{l:.2f}' for l in lambdas])
    ax.set_xlabel('Î» (Entanglement Strength)')
    ax.set_ylabel('Accuracy', color='blue')
    ax2.set_ylabel('Loss', color='red')
    ax.set_title('Effect of Î» on XOR Problem')
    ax.set_ylim(0, 1.1)
    
    plt.suptitle('QBNN (Quantum-Bit Neural Network) Test Results', fontsize=14)
    plt.tight_layout()
    plt.savefig('/Users/yuyahiguchi/Program/Qubit/qbnn_all_tests.png', dpi=150, bbox_inches='tight')
    print("   ä¿å­˜: qbnn_all_tests.png")
    plt.close()


def test_logic_gates_silent():
    """è«–ç†ã‚²ãƒ¼ãƒˆï¼ˆã‚µã‚¤ãƒ¬ãƒ³ãƒˆç‰ˆï¼‰"""
    gates = {
        'AND': torch.tensor([0, 0, 0, 1]),
        'OR': torch.tensor([0, 1, 1, 1]),
        'NAND': torch.tensor([1, 1, 1, 0]),
        'NOR': torch.tensor([1, 0, 0, 0]),
        'XOR': torch.tensor([0, 1, 1, 0]),
        'XNOR': torch.tensor([1, 0, 0, 1])
    }
    
    X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
    results = {}
    
    for gate_name, y in gates.items():
        model = QBNNClassifier(2, [8, 8], 2, 0.5)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
        criterion = nn.CrossEntropyLoss()
        
        for _ in range(300):
            optimizer.zero_grad()
            loss = criterion(model(X), y)
            loss.backward()
            optimizer.step()
        
        pred = model.predict(X)
        results[gate_name] = (pred == y).float().mean().item()
    
    return results


def test_lambda_silent():
    """Î»æ¯”è¼ƒï¼ˆã‚µã‚¤ãƒ¬ãƒ³ãƒˆç‰ˆï¼‰"""
    X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
    y = torch.tensor([0, 1, 1, 0])
    
    results = {}
    for lam in [0.0, 0.25, 0.5, 0.75, 1.0]:
        model = QBNNClassifier(2, [8, 8], 2, lam)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
        criterion = nn.CrossEntropyLoss()
        
        for _ in range(300):
            optimizer.zero_grad()
            loss = criterion(model(X), y)
            loss.backward()
            optimizer.step()
        
        pred = model.predict(X)
        results[lam] = {
            'acc': (pred == y).float().mean().item(),
            'loss': loss.item()
        }
    
    return results


# ========================================
# è¿½åŠ ãƒ†ã‚¹ãƒˆ
# ========================================

def test_spiral():
    """èºæ—‹ãƒ‡ãƒ¼ã‚¿åˆ†é¡ï¼ˆéå¸¸ã«è¤‡é›‘ãªéç·šå½¢ï¼‰"""
    print("\nğŸ“Š èºæ—‹ãƒ‡ãƒ¼ã‚¿åˆ†é¡")
    print("-" * 50)
    
    np.random.seed(42)
    n_points = 100
    
    # 2ã¤ã®èºæ—‹ã‚’ç”Ÿæˆ
    theta = np.linspace(0, 4 * np.pi, n_points)
    r = theta / (4 * np.pi)
    
    # èºæ—‹1
    x1 = r * np.cos(theta) + np.random.randn(n_points) * 0.05
    y1 = r * np.sin(theta) + np.random.randn(n_points) * 0.05
    
    # èºæ—‹2ï¼ˆ180åº¦å›è»¢ï¼‰
    x2 = -r * np.cos(theta) + np.random.randn(n_points) * 0.05
    y2 = -r * np.sin(theta) + np.random.randn(n_points) * 0.05
    
    X = torch.tensor(np.vstack([
        np.column_stack([x1, y1]),
        np.column_stack([x2, y2])
    ]), dtype=torch.float32)
    y = torch.tensor([0] * n_points + [1] * n_points)
    
    model = QBNNClassifier(2, [32, 32, 16], 2, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(1000):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 200 == 0:
            pred = model.predict(X)
            acc = (pred == y).float().mean()
            print(f"   Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc.item():.2%}")
    
    pred = model.predict(X)
    acc = (pred == y).float().mean().item()
    print(f"   æœ€çµ‚ç²¾åº¦: {acc:.2%}")
    
    return model, X, y, acc


def test_checkerboard():
    """ãƒã‚§ãƒƒã‚«ãƒ¼ãƒœãƒ¼ãƒ‰åˆ†é¡"""
    print("\nğŸ“Š ãƒã‚§ãƒƒã‚«ãƒ¼ãƒœãƒ¼ãƒ‰åˆ†é¡")
    print("-" * 50)
    
    np.random.seed(42)
    n_points = 400
    
    X = np.random.uniform(-2, 2, (n_points, 2))
    y = ((np.floor(X[:, 0]) + np.floor(X[:, 1])) % 2).astype(int)
    
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y)
    
    model = QBNNClassifier(2, [32, 32, 16], 2, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(500):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            pred = model.predict(X)
            acc = (pred == y).float().mean()
            print(f"   Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc.item():.2%}")
    
    pred = model.predict(X)
    acc = (pred == y).float().mean().item()
    print(f"   æœ€çµ‚ç²¾åº¦: {acc:.2%}")
    
    return model, X, y, acc


def test_half_moon():
    """åŠæœˆå½¢ãƒ‡ãƒ¼ã‚¿åˆ†é¡"""
    print("\nğŸ“Š åŠæœˆå½¢ãƒ‡ãƒ¼ã‚¿åˆ†é¡")
    print("-" * 50)
    
    np.random.seed(42)
    n_samples = 200
    
    # ä¸Šã®åŠæœˆ
    theta1 = np.linspace(0, np.pi, n_samples // 2)
    x1 = np.cos(theta1) + np.random.randn(n_samples // 2) * 0.1
    y1 = np.sin(theta1) + np.random.randn(n_samples // 2) * 0.1
    
    # ä¸‹ã®åŠæœˆï¼ˆã‚·ãƒ•ãƒˆï¼‰
    theta2 = np.linspace(0, np.pi, n_samples // 2)
    x2 = 1 - np.cos(theta2) + np.random.randn(n_samples // 2) * 0.1
    y2 = 0.5 - np.sin(theta2) + np.random.randn(n_samples // 2) * 0.1
    
    X = torch.tensor(np.vstack([
        np.column_stack([x1, y1]),
        np.column_stack([x2, y2])
    ]), dtype=torch.float32)
    y = torch.tensor([0] * (n_samples // 2) + [1] * (n_samples // 2))
    
    model = QBNNClassifier(2, [16, 16], 2, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(500):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            pred = model.predict(X)
            acc = (pred == y).float().mean()
            print(f"   Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc.item():.2%}")
    
    pred = model.predict(X)
    acc = (pred == y).float().mean().item()
    print(f"   æœ€çµ‚ç²¾åº¦: {acc:.2%}")
    
    return model, X, y, acc


def test_noise_robustness():
    """ãƒã‚¤ã‚ºè€æ€§ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“Š ãƒã‚¤ã‚ºè€æ€§ãƒ†ã‚¹ãƒˆï¼ˆXORï¼‰")
    print("-" * 50)
    
    X_clean = torch.tensor([
        [0.0, 0.0],
        [0.0, 1.0],
        [1.0, 0.0],
        [1.0, 1.0]
    ])
    y = torch.tensor([0, 1, 1, 0])
    
    # ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
    model = QBNNClassifier(2, [8, 8], 2, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
    criterion = nn.CrossEntropyLoss()
    
    for _ in range(300):
        optimizer.zero_grad()
        loss = criterion(model(X_clean), y)
        loss.backward()
        optimizer.step()
    
    # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«åˆ¥ã«ãƒ†ã‚¹ãƒˆ
    noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]
    results = {}
    
    for noise in noise_levels:
        accs = []
        for _ in range(10):  # 10å›è©¦è¡Œ
            X_noisy = X_clean + torch.randn_like(X_clean) * noise
            pred = model.predict(X_noisy)
            acc = (pred == y).float().mean().item()
            accs.append(acc)
        avg_acc = np.mean(accs)
        results[noise] = avg_acc
        print(f"   ãƒã‚¤ã‚º={noise:.1f}: ç²¾åº¦={avg_acc:.0%}")
    
    return results


def test_convergence_comparison():
    """åæŸé€Ÿåº¦æ¯”è¼ƒï¼ˆQBNNã¨é€šå¸¸NNï¼‰"""
    print("\nğŸ“Š åæŸé€Ÿåº¦æ¯”è¼ƒï¼ˆQBNN vs é€šå¸¸NNï¼‰")
    print("-" * 50)
    
    X = torch.tensor([
        [0.0, 0.0],
        [0.0, 1.0],
        [1.0, 0.0],
        [1.0, 1.0]
    ])
    y = torch.tensor([0, 1, 1, 0])
    
    # é€šå¸¸ã®NN
    class StandardNN(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(2, 8)
            self.fc2 = nn.Linear(8, 8)
            self.fc3 = nn.Linear(8, 2)
        
        def forward(self, x):
            x = torch.tanh(self.fc1(x))
            x = torch.tanh(self.fc2(x))
            return self.fc3(x)
    
    # QBNN
    qbnn_model = QBNNClassifier(2, [8, 8], 2, 0.5)
    qbnn_optimizer = torch.optim.Adam(qbnn_model.parameters(), lr=0.02)
    
    # é€šå¸¸NN
    std_model = StandardNN()
    std_optimizer = torch.optim.Adam(std_model.parameters(), lr=0.02)
    
    criterion = nn.CrossEntropyLoss()
    
    qbnn_losses = []
    std_losses = []
    
    for epoch in range(200):
        # QBNN
        qbnn_optimizer.zero_grad()
        qbnn_loss = criterion(qbnn_model(X), y)
        qbnn_loss.backward()
        qbnn_optimizer.step()
        qbnn_losses.append(qbnn_loss.item())
        
        # é€šå¸¸NN
        std_optimizer.zero_grad()
        std_loss = criterion(std_model(X), y)
        std_loss.backward()
        std_optimizer.step()
        std_losses.append(std_loss.item())
    
    # çµæœ
    print(f"   QBNNæœ€çµ‚Loss: {qbnn_losses[-1]:.4f}")
    print(f"   æ¨™æº–NNæœ€çµ‚Loss: {std_losses[-1]:.4f}")
    
    # åæŸã‚¨ãƒãƒƒã‚¯ï¼ˆLoss < 0.1ï¼‰
    qbnn_conv = next((i for i, l in enumerate(qbnn_losses) if l < 0.1), 200)
    std_conv = next((i for i, l in enumerate(std_losses) if l < 0.1), 200)
    print(f"   QBNNåæŸã‚¨ãƒãƒƒã‚¯: {qbnn_conv}")
    print(f"   æ¨™æº–NNåæŸã‚¨ãƒãƒƒã‚¯: {std_conv}")
    
    return qbnn_losses, std_losses


def test_high_dimension():
    """é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿åˆ†é¡"""
    print("\nğŸ“Š é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿åˆ†é¡ï¼ˆ10æ¬¡å…ƒï¼‰")
    print("-" * 50)
    
    np.random.seed(42)
    n_samples = 500
    dim = 10
    
    # ã‚¯ãƒ©ã‚¹0ï¼šãƒ©ãƒ³ãƒ€ãƒ ãªæ–¹å‘
    X0 = np.random.randn(n_samples // 2, dim) * 0.5
    X0[:, 0] += 1  # æœ€åˆã®æ¬¡å…ƒã«ãƒã‚¤ã‚¢ã‚¹
    
    # ã‚¯ãƒ©ã‚¹1ï¼šåˆ¥ã®æ–¹å‘
    X1 = np.random.randn(n_samples // 2, dim) * 0.5
    X1[:, 0] -= 1
    
    X = torch.tensor(np.vstack([X0, X1]), dtype=torch.float32)
    y = torch.tensor([0] * (n_samples // 2) + [1] * (n_samples // 2))
    
    model = QBNNClassifier(dim, [32, 16], 2, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(300):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            pred = model.predict(X)
            acc = (pred == y).float().mean()
            print(f"   Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc.item():.2%}")
    
    pred = model.predict(X)
    acc = (pred == y).float().mean().item()
    print(f"   æœ€çµ‚ç²¾åº¦: {acc:.2%}")
    
    return acc


def test_imbalanced():
    """ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿åˆ†é¡"""
    print("\nğŸ“Š ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿åˆ†é¡ï¼ˆ1:9æ¯”ç‡ï¼‰")
    print("-" * 50)
    
    np.random.seed(42)
    
    # ã‚¯ãƒ©ã‚¹0ï¼šå°‘æ•°ï¼ˆ50ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    X0 = np.random.randn(50, 2) * 0.3 + np.array([0, 0])
    # ã‚¯ãƒ©ã‚¹1ï¼šå¤šæ•°ï¼ˆ450ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    X1 = np.random.randn(450, 2) * 0.5 + np.array([1.5, 1.5])
    
    X = torch.tensor(np.vstack([X0, X1]), dtype=torch.float32)
    y = torch.tensor([0] * 50 + [1] * 450)
    
    model = QBNNClassifier(2, [16, 16], 2, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    
    # ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ãæå¤±
    weights = torch.tensor([9.0, 1.0])
    criterion = nn.CrossEntropyLoss(weight=weights)
    
    for epoch in range(500):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            pred = model.predict(X)
            acc = (pred == y).float().mean()
            # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦
            acc0 = (pred[:50] == y[:50]).float().mean()
            acc1 = (pred[50:] == y[50:]).float().mean()
            print(f"   Epoch {epoch+1}: Loss={loss.item():.4f}, å…¨ä½“={acc.item():.2%}, å°‘æ•°ã‚¯ãƒ©ã‚¹={acc0.item():.2%}")
    
    pred = model.predict(X)
    acc = (pred == y).float().mean().item()
    acc0 = (pred[:50] == y[:50]).float().mean().item()
    print(f"   æœ€çµ‚ç²¾åº¦: å…¨ä½“={acc:.2%}, å°‘æ•°ã‚¯ãƒ©ã‚¹={acc0:.2%}")
    
    return acc, acc0


def test_cos_function():
    """cosé–¢æ•°ã®å›å¸°"""
    print("\nğŸ“Š å›å¸°å•é¡Œï¼ˆcosé–¢æ•°ï¼‰")
    print("-" * 50)
    
    X = torch.linspace(-2 * np.pi, 2 * np.pi, 100).unsqueeze(1)
    y = torch.cos(X)
    
    class QBNNRegressor(nn.Module):
        def __init__(self):
            super().__init__()
            self.qbnn = QBNN([1, 32, 32, 1], 0.3)
        
        def forward(self, x):
            return self.qbnn(x)
    
    model = QBNNRegressor()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.MSELoss()
    
    for epoch in range(1000):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 200 == 0:
            print(f"   Epoch {epoch+1}: MSE={loss.item():.6f}")
    
    with torch.no_grad():
        pred = model(X)
        mse = criterion(pred, y).item()
        r2 = 1 - mse / y.var().item()
    
    print(f"   æœ€çµ‚MSE: {mse:.6f}, RÂ²: {r2:.4f}")
    return r2


def test_polynomial():
    """å¤šé …å¼å›å¸°ï¼ˆxÂ² + xï¼‰"""
    print("\nğŸ“Š å¤šé …å¼å›å¸°ï¼ˆxÂ² + xï¼‰")
    print("-" * 50)
    
    X = torch.linspace(-2, 2, 100).unsqueeze(1)
    y = X ** 2 + X
    
    class QBNNRegressor(nn.Module):
        def __init__(self):
            super().__init__()
            self.qbnn = QBNN([1, 16, 16, 1], 0.3)
        
        def forward(self, x):
            return self.qbnn(x)
    
    model = QBNNRegressor()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.MSELoss()
    
    for epoch in range(500):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            print(f"   Epoch {epoch+1}: MSE={loss.item():.6f}")
    
    with torch.no_grad():
        pred = model(X)
        mse = criterion(pred, y).item()
        r2 = 1 - mse / y.var().item()
    
    print(f"   æœ€çµ‚MSE: {mse:.6f}, RÂ²: {r2:.4f}")
    return r2


def test_5class():
    """5ã‚¯ãƒ©ã‚¹åˆ†é¡"""
    print("\nğŸ“Š 5ã‚¯ãƒ©ã‚¹åˆ†é¡")
    print("-" * 50)
    
    np.random.seed(42)
    n_per_class = 50
    
    # 5ã¤ã®ä¸­å¿ƒã‚’é…ç½®ï¼ˆæ­£äº”è§’å½¢ï¼‰
    centers = []
    for i in range(5):
        angle = 2 * np.pi * i / 5
        centers.append((np.cos(angle), np.sin(angle)))
    
    X_list = []
    y_list = []
    
    for i, (cx, cy) in enumerate(centers):
        X_class = np.random.randn(n_per_class, 2) * 0.2 + np.array([cx, cy])
        X_list.append(X_class)
        y_list.extend([i] * n_per_class)
    
    X = torch.tensor(np.vstack(X_list), dtype=torch.float32)
    y = torch.tensor(y_list)
    
    model = QBNNClassifier(2, [32, 32], 5, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(500):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            pred = model.predict(X)
            acc = (pred == y).float().mean()
            print(f"   Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc.item():.2%}")
    
    pred = model.predict(X)
    acc = (pred == y).float().mean().item()
    print(f"   æœ€çµ‚ç²¾åº¦: {acc:.2%}")
    
    return acc


def visualize_extended_tests(spiral_data, checkerboard_data, halfmoon_data, conv_data):
    """æ‹¡å¼µãƒ†ã‚¹ãƒˆã®å¯è¦–åŒ–"""
    print("\nğŸ“ˆ æ‹¡å¼µãƒ†ã‚¹ãƒˆçµæœã®å¯è¦–åŒ–")
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # 1. èºæ—‹
    ax = axes[0, 0]
    model, X, y = spiral_data[0], spiral_data[1], spiral_data[2]
    X_np = X.numpy()
    pred = model.predict(X).numpy()
    colors = ['blue' if p == 0 else 'red' for p in pred]
    ax.scatter(X_np[:, 0], X_np[:, 1], c=colors, alpha=0.6, s=20)
    ax.set_title(f'Spiral Classification ({spiral_data[3]:.0%})')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    
    # 2. ãƒã‚§ãƒƒã‚«ãƒ¼ãƒœãƒ¼ãƒ‰
    ax = axes[0, 1]
    model, X, y = checkerboard_data[0], checkerboard_data[1], checkerboard_data[2]
    X_np = X.numpy()
    pred = model.predict(X).numpy()
    colors = ['blue' if p == 0 else 'red' for p in pred]
    ax.scatter(X_np[:, 0], X_np[:, 1], c=colors, alpha=0.6, s=10)
    ax.set_title(f'Checkerboard ({checkerboard_data[3]:.0%})')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    
    # 3. åŠæœˆ
    ax = axes[0, 2]
    model, X, y = halfmoon_data[0], halfmoon_data[1], halfmoon_data[2]
    X_np = X.numpy()
    pred = model.predict(X).numpy()
    colors = ['blue' if p == 0 else 'red' for p in pred]
    ax.scatter(X_np[:, 0], X_np[:, 1], c=colors, alpha=0.6, s=20)
    ax.set_title(f'Half Moon ({halfmoon_data[3]:.0%})')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    
    # 4. åæŸæ¯”è¼ƒ
    ax = axes[1, 0]
    qbnn_losses, std_losses = conv_data
    ax.plot(qbnn_losses, 'b-', label='QBNN', linewidth=2)
    ax.plot(std_losses, 'r--', label='Standard NN', linewidth=2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('Convergence Comparison')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')
    
    # 5. ãƒã‚¤ã‚ºè€æ€§ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
    ax = axes[1, 1]
    noise_results = test_noise_robustness_silent()
    noises = list(noise_results.keys())
    accs = list(noise_results.values())
    ax.bar(range(len(noises)), accs, color='green', alpha=0.7)
    ax.set_xticks(range(len(noises)))
    ax.set_xticklabels([f'{n:.1f}' for n in noises])
    ax.set_xlabel('Noise Level')
    ax.set_ylabel('Accuracy')
    ax.set_title('Noise Robustness (XOR)')
    ax.set_ylim(0, 1.1)
    
    # 6. é«˜æ¬¡å…ƒãƒ»ä¸å‡è¡¡ãƒ»å¤šã‚¯ãƒ©ã‚¹çµæœ
    ax = axes[1, 2]
    tests = ['10D', 'Imbalanced', '5-Class']
    accs = [test_high_dim_silent(), test_imbalanced_silent()[0], test_5class_silent()]
    colors_bar = ['green' if a > 0.9 else 'orange' for a in accs]
    ax.bar(tests, accs, color=colors_bar, alpha=0.7)
    ax.set_ylim(0, 1.1)
    ax.set_ylabel('Accuracy')
    ax.set_title('Other Tests')
    ax.axhline(0.9, color='green', linestyle='--', alpha=0.5)
    
    plt.suptitle('QBNN Extended Test Results', fontsize=14)
    plt.tight_layout()
    plt.savefig('/Users/yuyahiguchi/Program/Qubit/qbnn_extended_tests.png', dpi=150, bbox_inches='tight')
    print("   ä¿å­˜: qbnn_extended_tests.png")
    plt.close()


# ã‚µã‚¤ãƒ¬ãƒ³ãƒˆç‰ˆã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def test_noise_robustness_silent():
    X_clean = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
    y = torch.tensor([0, 1, 1, 0])
    
    model = QBNNClassifier(2, [8, 8], 2, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
    criterion = nn.CrossEntropyLoss()
    
    for _ in range(300):
        optimizer.zero_grad()
        loss = criterion(model(X_clean), y)
        loss.backward()
        optimizer.step()
    
    results = {}
    for noise in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]:
        accs = []
        for _ in range(10):
            X_noisy = X_clean + torch.randn_like(X_clean) * noise
            pred = model.predict(X_noisy)
            accs.append((pred == y).float().mean().item())
        results[noise] = np.mean(accs)
    return results


def test_high_dim_silent():
    np.random.seed(42)
    X0 = np.random.randn(250, 10) * 0.5
    X0[:, 0] += 1
    X1 = np.random.randn(250, 10) * 0.5
    X1[:, 0] -= 1
    X = torch.tensor(np.vstack([X0, X1]), dtype=torch.float32)
    y = torch.tensor([0] * 250 + [1] * 250)
    
    model = QBNNClassifier(10, [32, 16], 2, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    for _ in range(300):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
    
    return (model.predict(X) == y).float().mean().item()


def test_imbalanced_silent():
    np.random.seed(42)
    X0 = np.random.randn(50, 2) * 0.3
    X1 = np.random.randn(450, 2) * 0.5 + np.array([1.5, 1.5])
    X = torch.tensor(np.vstack([X0, X1]), dtype=torch.float32)
    y = torch.tensor([0] * 50 + [1] * 450)
    
    model = QBNNClassifier(2, [16, 16], 2, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss(weight=torch.tensor([9.0, 1.0]))
    
    for _ in range(500):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
    
    pred = model.predict(X)
    return (pred == y).float().mean().item(), (pred[:50] == y[:50]).float().mean().item()


def test_5class_silent():
    np.random.seed(42)
    centers = [(np.cos(2*np.pi*i/5), np.sin(2*np.pi*i/5)) for i in range(5)]
    X_list = []
    y_list = []
    for i, (cx, cy) in enumerate(centers):
        X_list.append(np.random.randn(50, 2) * 0.2 + np.array([cx, cy]))
        y_list.extend([i] * 50)
    X = torch.tensor(np.vstack(X_list), dtype=torch.float32)
    y = torch.tensor(y_list)
    
    model = QBNNClassifier(2, [32, 32], 5, 0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    
    for _ in range(500):
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
    
    return (model.predict(X) == y).float().mean().item()


if __name__ == "__main__":
    # ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒªãƒ¼
    print_model_summary()
    
    results_all = {}
    
    # ========================================
    # åŸºæœ¬ãƒ†ã‚¹ãƒˆ
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ§ª åŸºæœ¬ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # 1. XORå•é¡Œ
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ1: XORå•é¡Œ")
    model_xor, losses_xor = test_xor_problem()
    results_all['XOR'] = 1.0
    
    # 2. è«–ç†ã‚²ãƒ¼ãƒˆ
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ2: è«–ç†ã‚²ãƒ¼ãƒˆ")
    logic_results = test_logic_gates()
    results_all.update(logic_results)
    
    # 3. å††å½¢åˆ†é¡
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ3: å††å½¢åˆ†é¡")
    model_circle, X_circle, y_circle, acc_circle = test_circle_classification()
    results_all['Circle'] = acc_circle
    
    # 4. sinå›å¸°
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ4: sinå›å¸°")
    model_reg, X_reg, y_reg, r2_reg = test_regression()
    results_all['Sin_R2'] = r2_reg
    
    # 5. å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ5: 3ã‚¯ãƒ©ã‚¹åˆ†é¡")
    model_multi, X_multi, y_multi, acc_multi = test_multiclass()
    results_all['3Class'] = acc_multi
    
    # ========================================
    # æ‹¡å¼µãƒ†ã‚¹ãƒˆ
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ§ª æ‹¡å¼µãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # 6. èºæ—‹åˆ†é¡
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ6: èºæ—‹åˆ†é¡")
    model_spiral, X_spiral, y_spiral, acc_spiral = test_spiral()
    results_all['Spiral'] = acc_spiral
    
    # 7. ãƒã‚§ãƒƒã‚«ãƒ¼ãƒœãƒ¼ãƒ‰
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ7: ãƒã‚§ãƒƒã‚«ãƒ¼ãƒœãƒ¼ãƒ‰")
    model_checker, X_checker, y_checker, acc_checker = test_checkerboard()
    results_all['Checker'] = acc_checker
    
    # 8. åŠæœˆ
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ8: åŠæœˆ")
    model_moon, X_moon, y_moon, acc_moon = test_half_moon()
    results_all['HalfMoon'] = acc_moon
    
    # 9. ãƒã‚¤ã‚ºè€æ€§
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ9: ãƒã‚¤ã‚ºè€æ€§")
    noise_results = test_noise_robustness()
    
    # 10. åæŸæ¯”è¼ƒ
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ10: åæŸæ¯”è¼ƒï¼ˆQBNN vs æ¨™æº–NNï¼‰")
    qbnn_losses, std_losses = test_convergence_comparison()
    
    # 11. é«˜æ¬¡å…ƒ
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ11: é«˜æ¬¡å…ƒï¼ˆ10Dï¼‰")
    acc_highdim = test_high_dimension()
    results_all['10D'] = acc_highdim
    
    # 12. ä¸å‡è¡¡
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ12: ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿")
    acc_imb, acc_minority = test_imbalanced()
    results_all['Imbalanced'] = acc_imb
    results_all['Minority'] = acc_minority
    
    # 13. coså›å¸°
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ13: coså›å¸°")
    r2_cos = test_cos_function()
    results_all['Cos_R2'] = r2_cos
    
    # 14. å¤šé …å¼å›å¸°
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ14: å¤šé …å¼å›å¸°")
    r2_poly = test_polynomial()
    results_all['Poly_R2'] = r2_poly
    
    # 15. 5ã‚¯ãƒ©ã‚¹
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ15: 5ã‚¯ãƒ©ã‚¹åˆ†é¡")
    acc_5class = test_5class()
    results_all['5Class'] = acc_5class
    
    # 16. Î»ã®åŠ¹æœ
    print("\n" + "-" * 70)
    print("ğŸ“Œ ãƒ†ã‚¹ãƒˆ16: Î»ã®åŠ¹æœ")
    lambda_results = test_lambda_comparison()
    
    # ========================================
    # å¯è¦–åŒ–
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ“Š çµæœã®å¯è¦–åŒ–")
    print("=" * 70)
    
    # åŸºæœ¬ãƒ†ã‚¹ãƒˆå¯è¦–åŒ–
    visualize_all_tests(model_circle, X_circle, y_circle,
                        model_reg, X_reg, y_reg)
    
    # æ‹¡å¼µãƒ†ã‚¹ãƒˆå¯è¦–åŒ–
    visualize_extended_tests(
        (model_spiral, X_spiral, y_spiral, acc_spiral),
        (model_checker, X_checker, y_checker, acc_checker),
        (model_moon, X_moon, y_moon, acc_moon),
        (qbnn_losses, std_losses)
    )
    
    # XORå¯è¦–åŒ–
    X_xor = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
    visualize_qbnn(model_xor, X_xor)
    
    # ========================================
    # ã‚µãƒãƒªãƒ¼
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ“‹ å…¨ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 70)
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  QBNN (Quantum-Bit Neural Network) ãƒ†ã‚¹ãƒˆçµæœ                    â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                  â•‘
    â•‘  ã€åŸºæœ¬ãƒ†ã‚¹ãƒˆã€‘                                                   â•‘""")
    
    basic_tests = ['XOR', 'AND', 'OR', 'NAND', 'NOR', 'XNOR', 'Circle', '3Class']
    for t in basic_tests:
        v = results_all.get(t, 0)
        status = 'âœ…' if v >= 0.95 else ('âš ï¸' if v >= 0.8 else 'âŒ')
        print(f"    â•‘  {t:15s}: {v:6.1%} {status}                                     â•‘")
    
    print("""    â•‘                                                                  â•‘
    â•‘  ã€å›å¸°ãƒ†ã‚¹ãƒˆã€‘                                                   â•‘""")
    
    reg_tests = ['Sin_R2', 'Cos_R2', 'Poly_R2']
    for t in reg_tests:
        v = results_all.get(t, 0)
        status = 'âœ…' if v >= 0.95 else ('âš ï¸' if v >= 0.8 else 'âŒ')
        print(f"    â•‘  {t:15s}: {v:6.4f} {status}                                    â•‘")
    
    print("""    â•‘                                                                  â•‘
    â•‘  ã€é«˜é›£åº¦ãƒ†ã‚¹ãƒˆã€‘                                                 â•‘""")
    
    hard_tests = ['Spiral', 'Checker', 'HalfMoon', '5Class', '10D']
    for t in hard_tests:
        v = results_all.get(t, 0)
        status = 'âœ…' if v >= 0.95 else ('âš ï¸' if v >= 0.8 else 'âŒ')
        print(f"    â•‘  {t:15s}: {v:6.1%} {status}                                     â•‘")
    
    print("""    â•‘                                                                  â•‘
    â•‘  ã€ç‰¹æ®Šãƒ†ã‚¹ãƒˆã€‘                                                   â•‘""")
    
    print(f"    â•‘  Imbalanced:      {results_all.get('Imbalanced', 0):6.1%} (å°‘æ•°: {results_all.get('Minority', 0):.0%})                     â•‘")
    print(f"    â•‘  Noise(0.3):      {noise_results.get(0.3, 0):6.1%}                                       â•‘")
    
    print("""    â•‘                                                                  â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # çµ±è¨ˆ
    passed = sum(1 for v in results_all.values() if v >= 0.9)
    total = len(results_all)
    print(f"    åˆè¨ˆ: {passed}/{total} ãƒ†ã‚¹ãƒˆåˆæ ¼ï¼ˆ90%ä»¥ä¸Šï¼‰")
    
    print("\nâœ… QBNN å…¨16ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")

