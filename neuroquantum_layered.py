#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—      â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—     â•‘
â•‘   â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘     â•‘
â•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘     â•‘
â•‘   â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘     â•‘
â•‘   â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•  â•šâ•â•â–€â–€â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•     â•‘
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—           â•‘
â•‘   â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘          â•‘
â•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘          â•‘
â•‘   â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘          â•‘
â•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘          â•‘
â•‘    â•šâ•â•â–€â–€â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â•   â•šâ•â•    â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•          â•‘
â•‘                                                                               â•‘
â•‘   ãƒ‹ãƒ¥ãƒ¼ãƒ­Q: Quantum-Bit Neural Network Language Model                       â•‘
â•‘   ç‹¬è‡ªã®é‡å­ã‚‚ã¤ã‚Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ç”ŸæˆAI                          â•‘
â•‘                                                                               â•‘
â•‘   å‚ç…§å…ƒ: qbnn_layered.py                                                     â•‘
â•‘   - APQB: èª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆ                                              â•‘
â•‘   - EntanglementOperator: å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«æ¼”ç®—å­                              â•‘
â•‘   - EQBNNLayer: å±¤çŠ¶QBNNå±¤                                                    â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import json
import os
from collections import Counter
from typing import List, Dict, Optional, Tuple

# ========================================
# qbnn_layered.py ã‹ã‚‰ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ========================================
try:
    from qbnn_layered import (
        APQB as APQB_Core,                  # APQBç†è«–ã®ã‚³ã‚¢
        EntanglementOperator,               # å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«æ¼”ç®—å­
        QuantumCorrelationMatrix,           # é‡å­ç›¸é–¢è¡Œåˆ—
        EQBNNLayer,                         # E-QBNNå±¤
    )
    QBNN_LAYERED_AVAILABLE = True
    print("âœ… qbnn_layered.py ã‹ã‚‰ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError:
    QBNN_LAYERED_AVAILABLE = False
    print("âš ï¸ qbnn_layered.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å†…è”µã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

# ========================================
# è¨­å®š
# ========================================

class NeuroQuantumConfig:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ­Qè¨­å®š"""
    def __init__(
        self,
        vocab_size: int = 8000,
        embed_dim: int = 256,
        hidden_dim: int = 512,
        num_heads: int = 8,
        num_layers: int = 6,
        max_seq_len: int = 512,
        dropout: float = 0.1,
        lambda_entangle: float = 0.5,  # QBNNã‚‚ã¤ã‚Œå¼·åº¦
    ):
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        self.dropout = dropout
        self.lambda_entangle = lambda_entangle


# ========================================
# Part 1: QBNN Layerï¼ˆç‹¬è‡ªã®é‡å­ã‚‚ã¤ã‚Œå±¤ï¼‰
# ========================================

class QBNNLayer(nn.Module):
    """
    Quantum-Bit Neural Network Layer
    
    qbnn_layered.py ã® EQBNNLayer ã‚’åŸºç›¤ã¨ã—ã¦ä½¿ç”¨å¯èƒ½
    
    ç‹¬è‡ªã®æ•°å¼ãƒ¢ãƒ‡ãƒ«:
    1. s^(l) = tanh(h^(l)) âˆˆ [-1, 1]  (æ­£è¦åŒ– â†’ Blochçƒã®zåº§æ¨™)
    2. hÌƒ^(l+1) = W^(l) h^(l) + b^(l)  (ç·šå½¢å¤‰æ›)
    3. Î”^(l+1)_j = Î£_i J^(l)_{ij} s^(l)_i s^(l+1)_{raw,j}  (ã‚‚ã¤ã‚Œè£œæ­£)
    4. Ä¥^(l+1) = hÌƒ^(l+1) + Î»_eff Î”^(l+1)  (æœ‰åŠ¹å…¥åŠ›)
    5. h^(l+1) = activation(Ä¥^(l+1))  (æ´»æ€§åŒ–)
    
    APQBç†è«–ã«åŸºã¥ãæ”¹è‰¯:
    - Î»ã‚’ç¯„å›²ã§åˆ¶å¾¡ã—ã€Î¸ï¼ˆã‚·ãƒ¼ã‚¿ï¼‰ãŒå‹•çš„ã«å¤‰åŒ–ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    - r = cos(2Î¸), T = |sin(2Î¸)|, rÂ² + TÂ² = 1
    
    å‚ç…§: qbnn_layered.py ã® APQB ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, input_dim: int, output_dim: int, 
                 lambda_min: float = 0.2, lambda_max: float = 0.5,
                 use_qbnn_layered: bool = True):  # qbnn_layered.pyã‚’å‚ç…§
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.use_qbnn_layered = use_qbnn_layered and QBNN_LAYERED_AVAILABLE
        
        # Î»ã®ç¯„å›²ï¼ˆÎ¸ãŒå‹•ã‘ã‚‹ã‚ˆã†ã«ï¼‰
        self.lambda_min = lambda_min
        self.lambda_max = lambda_max
        
        if self.use_qbnn_layered:
            # qbnn_layered.py ã® EQBNNLayer ã‚’å†…éƒ¨ã§ä½¿ç”¨
            self.eqbnn_core = EQBNNLayer(
                input_dim=input_dim,
                output_dim=output_dim,
                prev_output_dim=input_dim,
                entangle_strength=(lambda_min + lambda_max) / 2
            )
            # ã‚³ã‚¢å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‚ç…§
            self.W = self.eqbnn_core.linear
            # W_entangle.weightã®å½¢çŠ¶ã¯(output_dim, input_dim)ãªã®ã§ã€å‚ç…§ã¨ã—ã¦ä¿æŒ
            # forwardå†…ã§è»¢ç½®ã—ã¦ä½¿ç”¨ã™ã‚‹
            self.J_source = self.eqbnn_core.entangle_op.W_entangle.weight
        else:
            # W: é€šå¸¸ã®é‡ã¿è¡Œåˆ—
            self.W = nn.Linear(input_dim, output_dim)
            
            # J: ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ«ï¼ˆç‹¬è‡ªï¼‰
            self.J = nn.Parameter(torch.randn(input_dim, output_dim) * 0.02)
        
        # Î»ãƒ™ãƒ¼ã‚¹å€¤ï¼ˆå­¦ç¿’å¯èƒ½ã€0-1ã«æ­£è¦åŒ–ã—ã¦ã‹ã‚‰ç¯„å›²ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
        self.lambda_base = nn.Parameter(torch.tensor(0.5))
        
        # å±¤æ­£è¦åŒ–
        self.layer_norm = nn.LayerNorm(output_dim)
        
        # å‘¼ã³å‡ºã—ã‚«ã‚¦ãƒ³ã‚¿ï¼ˆå‹•çš„å¤‰åŒ–ç”¨ï¼‰
        self.register_buffer('call_count', torch.tensor(0))
    
    def forward(self, h_prev: torch.Tensor) -> torch.Tensor:
        # 1. æ­£è¦åŒ–ï¼ˆBlochçƒã®zåº§æ¨™ã¨ã—ã¦è§£é‡ˆï¼‰
        s_prev = torch.tanh(h_prev)  # (..., input_dim)
        
        # 2. ç·šå½¢å¤‰æ›
        h_tilde = self.W(h_prev)  # (..., output_dim)
        
        # 3. æ¬¡å±¤ã®å€™è£œã‚’æ­£è¦åŒ–
        s_raw = torch.tanh(h_tilde)  # (..., output_dim)
        
        # 4. ã‚‚ã¤ã‚Œè£œæ­£é … Î”
        # Î”_j = Î£_i J_{ij} s^(l)_i * s^(l+1)_{raw,j}
        # J: (input_dim, output_dim)
        # s_prev: (..., input_dim)
        # s_raw: (..., output_dim)
        # delta: (..., output_dim)
        # å„jã«ã¤ã„ã¦: delta_j = Î£_i (J_{ij} * s_prev_i) * s_raw_j
        
        # Jã®å½¢çŠ¶ã‚’ç¢ºèªã—ã¦è»¢ç½®ï¼ˆuse_qbnn_layeredã®å ´åˆã€J_sourceã¯(output_dim, input_dim)ï¼‰
        if self.use_qbnn_layered:
            J = self.J_source.t()  # (input_dim, output_dim)ã«è»¢ç½®
        else:
            J = self.J  # æ—¢ã«(input_dim, output_dim)
        
        # ã¾ãš J_{ij} * s_prev_i ã‚’è¨ˆç®—: (..., output_dim)
        J_s_prev = torch.einsum('...i,ij->...j', s_prev, J)  # (..., output_dim)
        # æ¬¡ã« s_raw_j ã‚’æ›ã‘ã‚‹
        delta = J_s_prev * s_raw  # (..., output_dim)
        
        # 5. å‹•çš„Î»: Î¸ãŒå‹•ã‘ã‚‹ã‚ˆã†ã«ç¯„å›²å†…ã§å¤‰åŒ–ï¼ˆé‡å­ã‚†ã‚‰ãï¼‰
        # Î»_baseã‚’sigmoidã§0-1ã«åˆ¶é™ã—ã€ç¯„å›²ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        lambda_normalized = torch.sigmoid(self.lambda_base)
        
        # æ¨è«–æ™‚ã®ã¿å‹•çš„å¤‰åŒ–ã‚’è¿½åŠ ï¼ˆå­¦ç¿’æ™‚ã¯å®‰å®šæ€§ã®ãŸã‚å›ºå®šå¯„ã‚Šï¼‰
        if not self.training:
            # sinæ³¢ã§å‹•çš„å¤‰åŒ–ï¼ˆÎ¸ãŒå‹•ã‘ã‚‹ã‚ˆã†ã«ï¼‰
            phase = float(self.call_count) * 0.2
            dynamic_factor = 0.5 + 0.5 * math.sin(phase)
            self.call_count += 1
        else:
            dynamic_factor = 0.5
        
        # æœ‰åŠ¹ãªÎ»ã‚’è¨ˆç®—
        lambda_range = self.lambda_max - self.lambda_min
        lambda_eff = self.lambda_min + lambda_range * (lambda_normalized * 0.7 + dynamic_factor * 0.3)
        
        # 6. æœ‰åŠ¹å…¥åŠ›
        h_hat = h_tilde + lambda_eff * delta
        
        # 7. å±¤æ­£è¦åŒ– + GELUæ´»æ€§åŒ–
        output = self.layer_norm(h_hat)
        output = F.gelu(output)
        
        return output
    
    def get_quantum_info(self) -> Dict:
        """é‡å­æƒ…å ±ã‚’å–å¾—"""
        with torch.no_grad():
            lambda_normalized = torch.sigmoid(self.lambda_base).item()
            lambda_eff = self.lambda_min + (self.lambda_max - self.lambda_min) * lambda_normalized
            
            # Jã®å½¢çŠ¶ã‚’ç¢ºèªï¼ˆuse_qbnn_layeredã®å ´åˆã€è»¢ç½®ãŒå¿…è¦ï¼‰
            if self.use_qbnn_layered:
                J = self.J_source.t()  # (input_dim, output_dim)ã«è»¢ç½®
            else:
                J = self.J  # æ—¢ã«(input_dim, output_dim)
            
            info = {
                'lambda_min': self.lambda_min,
                'lambda_max': self.lambda_max,
                'lambda_eff': lambda_eff,
                'J_mean': J.mean().item(),
                'J_std': J.std().item(),
                'J_max': J.max().item(),
                'source': 'qbnn_layered.py' if self.use_qbnn_layered else 'builtin',
            }
            
            # qbnn_layered.pyä½¿ç”¨æ™‚ã¯è¿½åŠ æƒ…å ±ã‚’å–å¾—
            if self.use_qbnn_layered:
                info['entangle_strength'] = self.eqbnn_core.entangle_op.entangle_strength.item()
            
            return info


# ========================================
# Part 2: QBNN-Attentionï¼ˆé‡å­ã‚‚ã¤ã‚Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰
# ========================================

class QBNNAttention(nn.Module):
    """
    QBNNæ‹¡å¼µSelf-Attention
    
    é€šå¸¸ã®Attentionã«é‡å­ã‚‚ã¤ã‚Œè£œæ­£ã‚’è¿½åŠ 
    """
    
    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1, 
                 lambda_val: float = 0.5):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert self.head_dim * num_heads == embed_dim, "embed_dimã¯num_headsã§å‰²ã‚Šåˆ‡ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
        
        # Q, K, V ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        # QBNNé‡å­ã‚‚ã¤ã‚Œè£œæ­£
        self.J_attn = nn.Parameter(torch.randn(num_heads, self.head_dim, self.head_dim) * 0.02)
        self.lambda_attn = nn.Parameter(torch.tensor(float(lambda_val)))
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.head_dim)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        GPTæ¨™æº–: Multi-Head Causal Self-Attentionï¼ˆQBNNæ‹¡å¼µç‰ˆï¼‰
        
        Args:
            x: (batch, seq, embed_dim)
            mask: Optional attention mask (Noneã®å ´åˆã¯Causal Maskã‚’è‡ªå‹•ç”Ÿæˆ)
        
        Returns:
            (batch, seq, embed_dim)
        """
        batch_size, seq_len, _ = x.shape
        
        # GPTæ¨™æº–: Q, K, V è¨ˆç®—
        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch, heads, seq, head_dim)
        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # GPTæ¨™æº–: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢è¨ˆç®—
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (batch, heads, seq, seq)
        
        # QBNNæ‹¡å¼µ: é‡å­ã‚‚ã¤ã‚Œè£œæ­£
        Q_norm = torch.tanh(Q)
        K_norm = torch.tanh(K)
        # ã‚‚ã¤ã‚Œè£œæ­£é …ï¼ˆãƒ˜ãƒƒãƒ‰ã”ã¨ï¼‰
        delta = torch.einsum('bhid,hde,bhje->bhij', Q_norm, self.J_attn, K_norm)
        attn_scores = attn_scores + self.lambda_attn * delta
        
        # GPTæ¨™æº–: Causal Maské©ç”¨
        if mask is not None:
            # æä¾›ã•ã‚ŒãŸãƒã‚¹ã‚¯ã‚’ä½¿ç”¨
            if mask.dim() == 2:
                mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq, seq)
            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))
        else:
            # Causal Maskè‡ªå‹•ç”Ÿæˆï¼ˆGPTæ¨™æº–ï¼‰
            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()
            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq, seq)
            attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))
        
        # GPTæ¨™æº–: Softmax + Dropout
        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.dropout(attn_probs)
        
        # GPTæ¨™æº–: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é©ç”¨
        output = torch.matmul(attn_probs, V)  # (batch, heads, seq, head_dim)
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        
        # GPTæ¨™æº–: å‡ºåŠ›å°„å½±
        return self.out_proj(output)


# ========================================
# Part 3: QBNN-Transformer Block
# ========================================

class QBNNTransformerBlock(nn.Module):
    """
    GPTãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆQBNNæ‹¡å¼µç‰ˆï¼‰
    
    GPTæ¨™æº–æ§‹é€ :
    1. Pre-norm LayerNorm
    2. Multi-Head Causal Self-Attention (QBNNæ‹¡å¼µ)
    3. Residual Connection
    4. Pre-norm LayerNorm
    5. Feed-Forward Network (æ¨™æº–FFN + QBNNæ‹¡å¼µ)
    6. Residual Connection
    """
    
    def __init__(self, config: NeuroQuantumConfig):
        super().__init__()
        
        # Pre-norm LayerNorm
        self.norm1 = nn.LayerNorm(config.embed_dim)
        self.norm2 = nn.LayerNorm(config.embed_dim)
        
        # QBNN-Attention
        self.attention = QBNNAttention(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            dropout=config.dropout,
            lambda_val=config.lambda_entangle
        )
        
        # GPTæ¨™æº–FFN: Linear â†’ GELU â†’ Linear
        self.ffn_standard = nn.Sequential(
            nn.Linear(config.embed_dim, config.hidden_dim),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.hidden_dim, config.embed_dim),
            nn.Dropout(config.dropout)
        )
        
        # QBNNæ‹¡å¼µFFNï¼ˆå€‹åˆ¥ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€Sequentialã§ã¯ãªãå€‹åˆ¥ã«å®šç¾©ï¼‰
        self.ffn_qbnn_layer1 = QBNNLayer(
            config.embed_dim, config.hidden_dim, 
            lambda_min=config.lambda_entangle * 0.5,
            lambda_max=config.lambda_entangle * 1.5
        )
        self.ffn_qbnn_dropout = nn.Dropout(config.dropout)
        self.ffn_qbnn_layer2 = QBNNLayer(
            config.hidden_dim, config.embed_dim,
            lambda_min=config.lambda_entangle * 0.5,
            lambda_max=config.lambda_entangle * 1.5
        )
        
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        GPTãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        
        Args:
            x: (batch, seq, embed_dim)
            mask: Optional attention mask
        
        Returns:
            (batch, seq, embed_dim)
        """
        # 1. Pre-norm + Multi-Head Causal Self-Attention + Residual
        residual = x
        x = self.norm1(x)
        attn_out = self.attention(x, mask)
        x = residual + self.dropout(attn_out)
        
        # 2. Pre-norm + Feed-Forward Network + Residual
        residual = x
        x = self.norm2(x)
        
        # æ¨™æº–FFN + QBNNæ‹¡å¼µï¼ˆãƒ–ãƒ¬ãƒ³ãƒ‰ï¼‰
        ffn_standard_out = self.ffn_standard(x)
        # QBNNæ‹¡å¼µFFN
        ffn_qbnn_out = self.ffn_qbnn_layer1(x)
        ffn_qbnn_out = self.ffn_qbnn_dropout(ffn_qbnn_out)
        ffn_qbnn_out = self.ffn_qbnn_layer2(ffn_qbnn_out)
        
        # ãƒ–ãƒ¬ãƒ³ãƒ‰æ¯”ç‡: æ¨™æº–FFN 70% + QBNNæ‹¡å¼µ 30%
        ffn_out = 0.7 * ffn_standard_out + 0.3 * ffn_qbnn_out
        
        x = residual + ffn_out
        
        return x


# ========================================
# Part 4: Embeddingï¼ˆåŸ‹ã‚è¾¼ã¿å±¤ï¼‰
# ========================================

class NeuroQuantumEmbedding(nn.Module):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q åŸ‹ã‚è¾¼ã¿å±¤
    
    Token â†’ ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ› + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    """
    
    def __init__(self, config: NeuroQuantumConfig):
        super().__init__()
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿
        self.token_embedding = nn.Embedding(config.vocab_size, config.embed_dim)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.position_embedding = nn.Embedding(config.max_seq_len, config.embed_dim)
        
        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        self.dropout = nn.Dropout(config.dropout)
        
        # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
        self.embed_dim = config.embed_dim
    
    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len = token_ids.shape
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿
        token_embeds = self.token_embedding(token_ids)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0).expand(batch_size, -1)
        pos_embeds = self.position_embedding(positions)
        
        # åˆæˆ
        embeds = token_embeds + pos_embeds
        embeds = self.dropout(embeds)
        
        return embeds


# ========================================
# Part 5: Output Headï¼ˆå‡ºåŠ›å±¤ï¼‰
# ========================================

class NeuroQuantumHead(nn.Module):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q å‡ºåŠ›ãƒ˜ãƒƒãƒ‰
    
    ãƒ™ã‚¯ãƒˆãƒ« â†’ èªå½™ç¢ºç‡ã¸ã®å¤‰æ›
    """
    
    def __init__(self, config: NeuroQuantumConfig):
        super().__init__()
        
        # æœ€çµ‚æ­£è¦åŒ–
        self.norm = nn.LayerNorm(config.embed_dim)
        
        # èªå½™ã¸ã®ç·šå½¢å¤‰æ›
        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        return logits


# ========================================
# Part 6: ãƒ‹ãƒ¥ãƒ¼ãƒ­Q ãƒ¢ãƒ‡ãƒ«æœ¬ä½“
# ========================================

class NeuroQuantum(nn.Module):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q: GPTãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼æ§‹é€ ï¼ˆQBNNæ‹¡å¼µç‰ˆï¼‰
    
    GPTæ¨™æº–æ§‹é€ :
    1. Token Embedding + Position Embedding
    2. Dropout
    3. Nå€‹ã®GPT Decoder Blocks
    4. Final LayerNorm
    5. Output Head (Linear to vocab_size)
    
    ç‹¬è‡ªè¦ç´ :
    - QBNNLayer: é‡å­ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ« J ã«ã‚ˆã‚‹è£œæ­£
    - QBNN-Attention: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã¸ã®é‡å­è£œæ­£
    - å­¦ç¿’å¯èƒ½ãª Î»ï¼ˆã‚‚ã¤ã‚Œå¼·åº¦ï¼‰
    """
    
    def __init__(self, config: NeuroQuantumConfig):
        super().__init__()
        self.config = config
        
        # GPTæ¨™æº–: Token Embedding + Position Embedding
        self.token_embedding = nn.Embedding(config.vocab_size, config.embed_dim)
        self.position_embedding = nn.Embedding(config.max_seq_len, config.embed_dim)
        
        # GPT Decoder Blocks
        self.transformer_blocks = nn.ModuleList([
            QBNNTransformerBlock(config) for _ in range(config.num_layers)
        ])
        
        # GPTæ¨™æº–: Final LayerNorm
        self.final_norm = nn.LayerNorm(config.embed_dim)
        
        # GPTæ¨™æº–: Output Head (weight tyingå¯èƒ½ã ãŒã€ã“ã“ã§ã¯ç‹¬ç«‹)
        self.output_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)
        
        # GPTæ¨™æº–: Embedding Dropout
        self.dropout = nn.Dropout(config.dropout)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–ï¼ˆGPTæ¨™æº–ï¼‰
        self.apply(self._init_weights)
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        self.num_params = sum(p.numel() for p in self.parameters())
    
    def _init_weights(self, module):
        """GPTæ¨™æº–ã®é‡ã¿åˆæœŸåŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, token_ids: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        GPTãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        
        Args:
            token_ids: (batch, seq) ãƒˆãƒ¼ã‚¯ãƒ³ID
            mask: Optional attention mask (Noneã®å ´åˆã¯Causal Maskã‚’è‡ªå‹•ç”Ÿæˆ)
        
        Returns:
            (batch, seq, vocab_size) ãƒ­ã‚¸ãƒƒãƒˆ
        """
        batch, seq = token_ids.shape
        
        # GPTæ¨™æº–: Token + Position Embedding
        token_embeds = self.token_embedding(token_ids)  # (batch, seq, embed_dim)
        positions = torch.arange(seq, device=token_ids.device).unsqueeze(0).expand(batch, -1)
        pos_embeds = self.position_embedding(positions)  # (batch, seq, embed_dim)
        
        # åŸ‹ã‚è¾¼ã¿ã®åˆæˆ + Dropout
        hidden_states = self.dropout(token_embeds + pos_embeds)
        
        # Causal Maskç”Ÿæˆï¼ˆmaskãŒNoneã®å ´åˆï¼‰
        if mask is None:
            mask = torch.tril(torch.ones(seq, seq, device=token_ids.device)).unsqueeze(0).unsqueeze(0)
        
        # GPT Decoder Blocks
        for block in self.transformer_blocks:
            hidden_states = block(hidden_states, mask)
        
        # GPTæ¨™æº–: Final LayerNorm
        hidden_states = self.final_norm(hidden_states)
        
        # GPTæ¨™æº–: Output Head
        logits = self.output_head(hidden_states)
        
        return logits
    
    def get_quantum_info(self) -> List[Dict]:
        """å…¨å±¤ã®é‡å­æƒ…å ±ã‚’å–å¾—"""
        info = []
        for i, block in enumerate(self.transformer_blocks):
            block_info = {
                'block': i,
                'attn_lambda': block.attention.lambda_attn.item(),
            }
            info.append(block_info)
        return info


# ========================================
# Part 7: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
# ========================================

class NeuroQuantumTokenizer:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    
    æ–‡å­—ãƒ¬ãƒ™ãƒ« + ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œ
    """
    
    def __init__(self, vocab_size: int = 8000):
        self.vocab_size = vocab_size
        self.char_to_idx: Dict[str, int] = {}
        self.idx_to_char: Dict[int, str] = {}
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.pad_token = '<PAD>'
        self.unk_token = '<UNK>'
        self.bos_token = '<BOS>'
        self.eos_token = '<EOS>'
        
        self.pad_id = 0
        self.unk_id = 1
        self.bos_id = 2
        self.eos_id = 3
    
    def build_vocab(self, texts: List[str], min_freq: int = 1):
        """èªå½™æ§‹ç¯‰"""
        # æ–‡å­—é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        char_freq = Counter()
        for text in texts:
            char_freq.update(text)
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        special_tokens = [self.pad_token, self.unk_token, self.bos_token, self.eos_token]
        
        # é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_chars = [char for char, freq in char_freq.most_common() if freq >= min_freq]
        
        # èªå½™ã‚µã‚¤ã‚ºåˆ¶é™
        sorted_chars = sorted_chars[:self.vocab_size - len(special_tokens)]
        
        # è¾æ›¸ä½œæˆ
        all_tokens = special_tokens + sorted_chars
        self.char_to_idx = {c: i for i, c in enumerate(all_tokens)}
        self.idx_to_char = {i: c for i, c in enumerate(all_tokens)}
        
        self.actual_vocab_size = len(all_tokens)
        return self
    
    def encode(self, text: str, add_special: bool = True) -> List[int]:
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        tokens = []
        if add_special:
            tokens.append(self.bos_id)
        
        for char in text:
            tokens.append(self.char_to_idx.get(char, self.unk_id))
        
        if add_special:
            tokens.append(self.eos_id)
        
        return tokens
    
    def decode(self, token_ids: List[int], skip_special: bool = True) -> str:
        """ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        chars = []
        special_ids = {self.pad_id, self.unk_id, self.bos_id, self.eos_id}
        
        for t in token_ids:
            if skip_special and t in special_ids:
                continue
            char = self.idx_to_char.get(t, self.unk_token)
            if char not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]:
                chars.append(char)
        
        return ''.join(chars)
    
    def save(self, path: str):
        """ä¿å­˜"""
        data = {
            'char_to_idx': self.char_to_idx,
            'vocab_size': self.vocab_size,
        }
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load(self, path: str):
        """èª­ã¿è¾¼ã¿"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.char_to_idx = data['char_to_idx']
        self.idx_to_char = {int(i): c for c, i in self.char_to_idx.items()}
        self.vocab_size = data['vocab_size']
        return self


# ========================================
# Part 8: ãƒ‹ãƒ¥ãƒ¼ãƒ­Q AIï¼ˆç”ŸæˆAIæœ¬ä½“ï¼‰
# ========================================

class NeuroQuantumAI:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q AI
    
    QBNN-LLM ã«ã‚ˆã‚‹ç”ŸæˆAI
    
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆhidden_dimï¼‰ã‚’æŒ‡å®šå¯èƒ½
    """
    
    def __init__(
        self,
        embed_dim: int = 48,
        hidden_dim: int = 96,       # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆFFNå±¤ã®æ¬¡å…ƒï¼‰
        num_heads: int = 4,
        num_layers: int = 2,
        max_seq_len: int = 128,
        dropout: float = 0.1,
        lambda_entangle: float = 0.5,
    ):
        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ: MPS (Apple Silicon) > CUDA > CPU
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print("ğŸ® NVIDIA GPU (CUDA) ã‚’ä½¿ç”¨")
        else:
            self.device = torch.device("cpu")
            print("ğŸ’» CPU ã‚’ä½¿ç”¨")
        
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        self.dropout = dropout
        self.lambda_entangle = lambda_entangle
        
        self.tokenizer: Optional[NeuroQuantumTokenizer] = None
        self.model: Optional[NeuroQuantum] = None
        self.config: Optional[NeuroQuantumConfig] = None
    
    def train(self, texts: List[str], epochs: int = 50, batch_size: int = 16, 
              lr: float = 0.001, seq_len: int = 64):
        """å­¦ç¿’"""
        print("\n" + "=" * 70)
        print("ğŸ“š ãƒ‹ãƒ¥ãƒ¼ãƒ­Q å­¦ç¿’é–‹å§‹")
        print("=" * 70)
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰
        print("\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰...")
        self.tokenizer = NeuroQuantumTokenizer(vocab_size=8000)
        self.tokenizer.build_vocab(texts)
        print(f"   èªå½™ã‚µã‚¤ã‚º: {self.tokenizer.actual_vocab_size}")
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        print("\nğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ­Qãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰...")
        self.config = NeuroQuantumConfig(
            vocab_size=self.tokenizer.actual_vocab_size,
            embed_dim=self.embed_dim,
            hidden_dim=self.hidden_dim,
            num_heads=self.num_heads,
            num_layers=self.num_layers,
            max_seq_len=self.max_seq_len,
            dropout=self.dropout,
            lambda_entangle=self.lambda_entangle,
        )
        
        self.model = NeuroQuantum(self.config).to(self.device)
        
        print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ§‹æˆ:")
        print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {self.embed_dim}")
        print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {self.hidden_dim}")
        print(f"   ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰: {self.num_heads}")
        print(f"   Transformerãƒ–ãƒ­ãƒƒã‚¯: {self.num_layers}")
        print(f"   ã‚‚ã¤ã‚Œå¼·åº¦ Î»: {self.lambda_entangle}")
        print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model.num_params:,}")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
        all_tokens = []
        for text in texts:
            tokens = self.tokenizer.encode(text)
            all_tokens.extend(tokens)
        
        all_tokens = torch.tensor(all_tokens, dtype=torch.long)
        print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens):,}")
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
        sequences = []
        for i in range(0, len(all_tokens) - seq_len - 1, seq_len // 2):
            x = all_tokens[i:i+seq_len]
            y = all_tokens[i+1:i+seq_len+1]
            if len(x) == seq_len and len(y) == seq_len:
                sequences.append((x, y))
        
        print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences):,}")
        
        # å­¦ç¿’
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.CrossEntropyLoss()
        
        print("\nğŸš€ å­¦ç¿’ãƒ«ãƒ¼ãƒ—...")
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            np.random.shuffle(sequences)
            
            for i in range(0, len(sequences), batch_size):
                batch = sequences[i:i+batch_size]
                if len(batch) == 0:
                    continue
                
                x_batch = torch.stack([s[0] for s in batch]).to(self.device)
                y_batch = torch.stack([s[1] for s in batch]).to(self.device)
                
                optimizer.zero_grad()
                logits = self.model(x_batch)
                
                loss = criterion(
                    logits.view(-1, self.tokenizer.actual_vocab_size),
                    y_batch.view(-1)
                )
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
            
            scheduler.step()
            avg_loss = total_loss / max(1, len(sequences) // batch_size)
            
            if (epoch + 1) % 5 == 0 or epoch == 0:
                print(f"   Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}")
        
        print("\nâœ… å­¦ç¿’å®Œäº†ï¼")
        
        # é‡å­æƒ…å ±
        print("\nâš›ï¸ é‡å­ã‚‚ã¤ã‚Œæƒ…å ±:")
        for info in self.model.get_quantum_info():
            print(f"   Block {info['block']}: Î»_attn = {info['attn_lambda']:.4f}")
    
    def generate(
        self,
        prompt: str = "",
        max_length: int = 100,
        temp_min: float = 0.4,       # æ¸©åº¦ã®ä¸‹é™
        temp_max: float = 0.8,       # æ¸©åº¦ã®ä¸Šé™
        top_k: int = 40,
        top_p: float = 0.9,
        repetition_penalty: float = 1.2,
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆå¯¾è©±å½¢å¼ï¼‰
        
        æ¸©åº¦ã‚’ç¯„å›²ã§æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€Î¸ï¼ˆã‚·ãƒ¼ã‚¿ï¼‰ãŒå‹•çš„ã«å¤‰åŒ–ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
        APQBç†è«–: r = cos(2Î¸), T = |sin(2Î¸)|, rÂ² + TÂ² = 1
        æ¸©åº¦TãŒå›ºå®šã ã¨Î¸ãŒå›ºå®šã•ã‚Œã€é‡å­çš„ã‚†ã‚‰ããŒãªããªã‚‹ã€‚
        """
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.model.eval()
        
        # å¯¾è©±å½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        dialogue_prompt = f"<USER>{prompt}<ASSISTANT>"
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        tokens = self.tokenizer.encode(dialogue_prompt, add_special=True)[:-1]
        
        tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)
        generated = tokens[0].tolist()
        
        with torch.no_grad():
            for step in range(max_length):
                # æœ€æ–°ã®max_seq_lenãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨
                input_tokens = tokens[:, -self.max_seq_len:] if tokens.size(1) > self.max_seq_len else tokens
                
                logits = self.model(input_tokens)
                next_logits = logits[0, -1, :]
                
                # å‹•çš„æ¸©åº¦: Î¸ãŒå‹•ã‘ã‚‹ã‚ˆã†ã«ç¯„å›²å†…ã§å¤‰åŒ–ã•ã›ã‚‹
                # sinæ³¢ã§æ»‘ã‚‰ã‹ã«å¤‰å‹•ï¼ˆé‡å­çš„ãªæŒ¯å‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
                theta_phase = step * 0.3  # ä½ç›¸
                temperature = temp_min + (temp_max - temp_min) * (0.5 + 0.5 * math.sin(theta_phase))
                
                # æ¸©åº¦èª¿æ•´
                next_logits = next_logits / temperature
                
                # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
                recent_tokens = set(generated[-30:])
                for token_id in recent_tokens:
                    next_logits[token_id] /= repetition_penalty
                
                # Top-K
                if top_k > 0:
                    top_k_vals, _ = torch.topk(next_logits, min(top_k, next_logits.size(-1)))
                    indices_to_remove = next_logits < top_k_vals[-1]
                    next_logits[indices_to_remove] = float('-inf')
                
                # Top-P
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                    sorted_indices_to_remove[0] = False
                    
                    indices_to_remove = sorted_indices[sorted_indices_to_remove]
                    next_logits[indices_to_remove] = float('-inf')
                
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                probs = F.softmax(next_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # EOSæ¤œå‡º
                if next_token.item() == self.tokenizer.eos_id:
                    break
                
                # <USER>ãƒˆãƒ¼ã‚¯ãƒ³ãŒå‡ºãŸã‚‰çµ‚äº†ï¼ˆæ¬¡ã®è³ªå•ã«å…¥ã‚‰ãªã„ã‚ˆã†ã«ï¼‰
                generated.append(next_token.item())
                decoded_so_far = self.tokenizer.decode(generated)
                if "<USER>" in decoded_so_far.split("<ASSISTANT>")[-1]:
                    break
                
                tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)
        
        # å¿œç­”éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
        full_text = self.tokenizer.decode(generated)
        
        # <ASSISTANT>ä»¥é™ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        if "<ASSISTANT>" in full_text:
            response = full_text.split("<ASSISTANT>")[-1]
            # <USER>ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰é™¤å»
            if "<USER>" in response:
                response = response.split("<USER>")[0]
            return response.strip()
        
        return full_text
    
    def chat(self):
        """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
        print("\n" + "=" * 70)
        print("ğŸ’¬ ãƒ‹ãƒ¥ãƒ¼ãƒ­Q ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰")
        print("=" * 70)
        print("\nã‚³ãƒãƒ³ãƒ‰:")
        print("  /quit         - çµ‚äº†")
        print("  /temp <min> <max> - æ¸©åº¦ç¯„å›² (ä¾‹: /temp 0.4 0.8)")
        print("  /len <å€¤>     - ç”Ÿæˆé•·ã• (10-500)")
        print("  /info         - ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
        print("  /quantum      - é‡å­ã‚‚ã¤ã‚Œæƒ…å ±")
        print("-" * 70)
        
        temp_min = 0.4  # æ¸©åº¦ã®ä¸‹é™
        temp_max = 0.8  # æ¸©åº¦ã®ä¸Šé™
        max_length = 100
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ You: ").strip()
                
                if not user_input:
                    continue
                
                if user_input == '/quit':
                    print("ğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼")
                    break
                
                if user_input.startswith('/temp '):
                    try:
                        parts = user_input.split()
                        if len(parts) >= 3:
                            temp_min = float(parts[1])
                            temp_max = float(parts[2])
                            temp_min = max(0.1, min(1.0, temp_min))
                            temp_max = max(0.1, min(1.0, temp_max))
                            if temp_min > temp_max:
                                temp_min, temp_max = temp_max, temp_min
                            print(f"   æ¸©åº¦ç¯„å›²ã‚’ {temp_min:.2f} - {temp_max:.2f} ã«è¨­å®šï¼ˆÎ¸ãŒå‹•ã‘ã‚‹ï¼‰")
                        else:
                            print("   ä½¿ã„æ–¹: /temp <æœ€å°> <æœ€å¤§> (ä¾‹: /temp 0.4 0.8)")
                    except:
                        print("   ã‚¨ãƒ©ãƒ¼: /temp <æœ€å°> <æœ€å¤§>")
                    continue
                
                if user_input.startswith('/len '):
                    try:
                        max_length = int(user_input.split()[1])
                        max_length = max(10, min(500, max_length))
                        print(f"   ç”Ÿæˆé•·ã•ã‚’ {max_length} ã«è¨­å®š")
                    except:
                        print("   ã‚¨ãƒ©ãƒ¼: /len <æ•°å€¤>")
                    continue
                
                if user_input == '/info':
                    print(f"\nğŸ“Š ãƒ‹ãƒ¥ãƒ¼ãƒ­Q ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
                    print(f"   èªå½™ã‚µã‚¤ã‚º: {self.tokenizer.actual_vocab_size}")
                    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {self.embed_dim}")
                    print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {self.hidden_dim}")
                    print(f"   ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰: {self.num_heads}")
                    print(f"   Transformerãƒ–ãƒ­ãƒƒã‚¯: {self.num_layers}")
                    print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model.num_params:,}")
                    continue
                
                if user_input == '/quantum':
                    print(f"\nâš›ï¸ é‡å­ã‚‚ã¤ã‚Œæƒ…å ±:")
                    for info in self.model.get_quantum_info():
                        print(f"   Block {info['block']}: Î»_attn = {info['attn_lambda']:.4f}")
                    continue
                
                # ç”Ÿæˆ
                print(f"\nğŸ¤– ãƒ‹ãƒ¥ãƒ¼ãƒ­Q: ", end="", flush=True)
                response = self.generate(
                    prompt=user_input,
                    max_length=max_length,
                    temp_min=temp_min,
                    temp_max=temp_max
                )
                
                print(response)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                break
            except Exception as e:
                print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)
        
        torch.save({
            'model_state': self.model.state_dict(),
            'config': self.config.__dict__,
        }, path + '.pt')
        
        self.tokenizer.save(path + '_tokenizer.json')
        print(f"âœ… ä¿å­˜: {path}")
    
    def load(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        self.tokenizer = NeuroQuantumTokenizer()
        self.tokenizer.load(path + '_tokenizer.json')
        
        # ãƒ¢ãƒ‡ãƒ«
        checkpoint = torch.load(path + '.pt', map_location=self.device)
        self.config = NeuroQuantumConfig(**checkpoint['config'])
        self.model = NeuroQuantum(self.config).to(self.device)
        self.model.load_state_dict(checkpoint['model_state'])
        
        print(f"âœ… èª­ã¿è¾¼ã¿: {path}")
        return self


# ========================================
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
# ========================================

def load_huggingface_data(max_samples: int = 500) -> List[str]:
    """Hugging Faceã‹ã‚‰å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    print("\nğŸ“¥ Hugging Faceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    
    try:
        from datasets import load_dataset
    except ImportError:
        print("   âš ï¸ datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚ã‚Šã¾ã›ã‚“ã€‚pip install datasetsã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return []
    
    formatted_texts = []
    
    # 1. OpenAssistant/oasst1 - é«˜å“è³ªãªå¯¾è©±ãƒ‡ãƒ¼ã‚¿
    try:
        print("   ğŸ“š OpenAssistant/oasst1 ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        dataset = load_dataset("OpenAssistant/oasst1", split="train", trust_remote_code=True)
        
        # å¯¾è©±ãƒ„ãƒªãƒ¼ã‹ã‚‰è³ªå•-å›ç­”ãƒšã‚¢ã‚’æŠ½å‡º
        messages_by_parent = {}
        root_messages = []
        
        for item in dataset:
            parent_id = item.get('parent_id')
            msg_id = item.get('message_id')
            text = item.get('text', '')
            role = item.get('role', '')
            lang = item.get('lang', '')
            
            if not text or len(text) < 5:
                continue
            
            if parent_id is None:
                root_messages.append(item)
            else:
                if parent_id not in messages_by_parent:
                    messages_by_parent[parent_id] = []
                messages_by_parent[parent_id].append(item)
        
        # ãƒ«ãƒ¼ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆè³ªå•ï¼‰ã«å¯¾ã™ã‚‹å›ç­”ã‚’å–å¾—
        count = 0
        for root in root_messages:
            if count >= max_samples // 2:
                break
            
            root_id = root.get('message_id')
            root_text = root.get('text', '')
            root_lang = root.get('lang', '')
            
            # æ—¥æœ¬èªã¾ãŸã¯è‹±èªã®ã¿
            if root_lang not in ['ja', 'en']:
                continue
            
            # å›ç­”ã‚’å–å¾—
            if root_id in messages_by_parent:
                responses = messages_by_parent[root_id]
                if responses:
                    # æœ€åˆã®å›ç­”ã‚’ä½¿ç”¨
                    response = responses[0]
                    response_text = response.get('text', '')
                    
                    if len(root_text) < 200 and len(response_text) < 300:
                        formatted = f"<USER>{root_text}<ASSISTANT>{response_text}"
                        formatted_texts.append(formatted)
                        count += 1
        
        print(f"   âœ… OpenAssistant: {count} ãƒšã‚¢å–å¾—")
        
    except Exception as e:
        print(f"   âš ï¸ OpenAssistantèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 2. kunishou/databricks-dolly-15k-ja - æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿
    try:
        print("   ğŸ“š databricks-dolly-15k-ja ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        dataset = load_dataset("kunishou/databricks-dolly-15k-ja", split="train", trust_remote_code=True)
        
        count = 0
        for item in dataset:
            if count >= max_samples // 4:
                break
            
            instruction = item.get('instruction', '')
            output = item.get('output', '')
            
            if instruction and output and len(instruction) < 150 and len(output) < 300:
                formatted = f"<USER>{instruction}<ASSISTANT>{output}"
                formatted_texts.append(formatted)
                count += 1
        
        print(f"   âœ… dolly-ja: {count} ãƒšã‚¢å–å¾—")
        
    except Exception as e:
        print(f"   âš ï¸ dolly-jaèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 3. databricks/databricks-dolly-15k - è‹±èªãƒ‡ãƒ¼ã‚¿
    try:
        print("   ğŸ“š databricks-dolly-15k ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        dataset = load_dataset("databricks/databricks-dolly-15k", split="train", trust_remote_code=True)
        
        count = 0
        for item in dataset:
            if count >= max_samples // 4:
                break
            
            instruction = item.get('instruction', '')
            response = item.get('response', '')
            
            if instruction and response and len(instruction) < 150 and len(response) < 300:
                formatted = f"<USER>{instruction}<ASSISTANT>{response}"
                formatted_texts.append(formatted)
                count += 1
        
        print(f"   âœ… dolly-en: {count} ãƒšã‚¢å–å¾—")
        
    except Exception as e:
        print(f"   âš ï¸ dolly-enèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"\nğŸ“Š åˆè¨ˆ: {len(formatted_texts)} å¯¾è©±ãƒšã‚¢å–å¾—å®Œäº†")
    
    return formatted_texts


def get_training_data() -> List[str]:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆå¯¾è©±å½¢å¼ï¼‰- æ‹¡å¼µç‰ˆ"""
    
    # å¯¾è©±ãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    conversations = [
        # ========================================
        # æ—¥æœ¬èª - æŒ¨æ‹¶ï¼ˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³å¤šæ•°ï¼‰
        # ========================================
        ("ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ã“ã‚“ã«ã¡ã¯ï¼", "ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ã“ã‚“ã«ã¡ã‚", "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ã”è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã‚‚è‰¯ã„ä¸€æ—¥ã«ãªã‚Šã¾ã™ã‚ˆã†ã«ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ã“ã‚“ã°ã‚“ã¯", "ã“ã‚“ã°ã‚“ã¯ï¼å¤œé…ãã¾ã§ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ã‚„ã‚", "ã‚„ã‚ï¼ä½•ã‹è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼ŸãŠæ°—è»½ã«ã©ã†ãã€‚"),
        ("ãƒãƒ­ãƒ¼", "ãƒãƒ­ãƒ¼ï¼ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚ä½•ã§ã‚‚èã„ã¦ãã ã•ã„ï¼"),
        ("ã©ã†ã‚‚", "ã©ã†ã‚‚ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ã‚ã‚ŠãŒã¨ã†", "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ãŠå½¹ã«ç«‹ã¦ã¦å¬‰ã—ã„ã§ã™ã€‚"),
        ("ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™", "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ä»–ã«ã”è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ã©ã†ãã€‚"),
        ("ã•ã‚ˆã†ãªã‚‰", "ã•ã‚ˆã†ãªã‚‰ï¼ã¾ãŸãŠä¼šã„ã—ã¾ã—ã‚‡ã†ã€‚"),
        ("ãƒã‚¤ãƒã‚¤", "ãƒã‚¤ãƒã‚¤ï¼ã¾ãŸæ¥ã¦ãã ã•ã„ã­ã€‚"),
        ("ã¾ãŸä¼šãŠã†", "ã¯ã„ã€ã¾ãŸãŠä¼šã„ã—ã¾ã—ã‚‡ã†ï¼ã„ã¤ã§ã‚‚ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚"),
        ("ãŠç–²ã‚Œæ§˜", "ãŠç–²ã‚Œæ§˜ã§ã™ï¼ä»Šæ—¥ã‚‚ä¸€æ—¥é ‘å¼µã‚Šã¾ã—ãŸã­ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - è‡ªå·±ç´¹ä»‹
        # ========================================
        ("ã‚ãªãŸã¯èª°ã§ã™ã‹", "ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚QBNNã¨ã„ã†ç‹¬è‡ªã®é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æŠ€è¡“ã‚’ä½¿ã£ãŸç”ŸæˆAIã§ã™ã€‚"),
        ("ã‚ãªãŸã¯èª°", "ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¨ã„ã†åå‰ã®ç”ŸæˆAIã§ã™ã€‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚"),
        ("èª°ã§ã™ã‹", "ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ãªæŠ€è¡“ã§å‹•ã„ã¦ã„ã¾ã™ã€‚"),
        ("åå‰ã¯", "ç§ã®åå‰ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚"),
        ("åå‰ã‚’æ•™ãˆã¦", "ç§ã®åå‰ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚QBNNãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIã¨ã—ã¦ä½œã‚‰ã‚Œã¾ã—ãŸã€‚"),
        ("ã‚ãªãŸã®åå‰ã¯ä½•ã§ã™ã‹", "ç§ã®åå‰ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚"),
        ("ä½•ãŒã§ãã¾ã™ã‹", "ç§ã¯è³ªå•ã«ç­”ãˆãŸã‚Šã€æƒ…å ±ã‚’æä¾›ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚„AIã«ã¤ã„ã¦è©³ã—ã„ã§ã™ã€‚"),
        ("ä½•ãŒã§ãã‚‹ã®", "ç§ã¯æ§˜ã€…ãªè³ªå•ã«ç­”ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç‰¹ã«é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚„AIã€ç§‘å­¦æŠ€è¡“ã«ã¤ã„ã¦è©³ã—ã„ã§ã™ã€‚"),
        ("è‡ªå·±ç´¹ä»‹ã—ã¦", "ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚QBNNã¨ã„ã†ç‹¬è‡ªæŠ€è¡“ã‚’ä½¿ã£ãŸç”ŸæˆAIã§ã€é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚"),
        ("ã©ã‚“ãªAIã§ã™ã‹", "ç§ã¯QBNNãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIã§ã™ã€‚é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸç‹¬è‡ªã®æŠ€è¡“ã§å‹•ä½œã—ã¦ã„ã¾ã™ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - é‡å­é–¢é€£ï¼ˆè©³ç´°ç‰ˆï¼‰
        # ========================================
        ("é‡å­ã¨ã¯ä½•ã§ã™ã‹", "é‡å­ã¨ã¯ã€ç‰©è³ªã‚„ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°å˜ä½ã®ã“ã¨ã§ã™ã€‚é‡å­åŠ›å­¦ã§ã¯ã€ç²’å­ã¯æ³¢ã®æ€§è³ªã‚‚æŒã¡ã¾ã™ã€‚"),
        ("é‡å­ã¨ã¯", "é‡å­ã¯ç‰©è³ªã‚„ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°å˜ä½ã§ã™ã€‚åŸå­ã‚„é›»å­ãªã©ã®ãƒŸã‚¯ãƒ­ãªä¸–ç•Œã®åŸºæœ¬è¦ç´ ã§ã™ã€‚"),
        ("é‡å­ã£ã¦ä½•", "é‡å­ã¨ã¯ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚„ç‰©è³ªã®æœ€å°å˜ä½ã®ã“ã¨ã§ã™ã€‚é‡å­åŠ›å­¦ã¨ã„ã†ç‰©ç†å­¦ã§ç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ã€‚"),
        ("é‡å­åŠ›å­¦ã¨ã¯", "é‡å­åŠ›å­¦ã¯ã€åŸå­ã‚„é›»å­ãªã©ã®æ¥µã‚ã¦å°ã•ãªä¸–ç•Œã‚’è¨˜è¿°ã™ã‚‹ç‰©ç†å­¦ã®ç†è«–ã§ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦è¨ˆç®—ã‚’è¡Œã†æ¬¡ä¸–ä»£ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚ˆã‚Šé«˜é€Ÿã«ç‰¹å®šã®å•é¡Œã‚’è§£ãã“ã¨ãŒã§ãã¾ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã£ã¦ä½•", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­ãƒ“ãƒƒãƒˆã‚’ä½¿ã£ã¦è¨ˆç®—ã™ã‚‹æ–°ã—ã„ã‚¿ã‚¤ãƒ—ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚æš—å·è§£èª­ã‚„æœ€é©åŒ–å•é¡Œã§å¨åŠ›ã‚’ç™ºæ®ã—ã¾ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«ã¤ã„ã¦æ•™ãˆã¦", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã¯é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸè¨ˆç®—æ©Ÿã§ã™ã€‚é‡ã­åˆã‚ã›ã‚„é‡å­ã‚‚ã¤ã‚Œã‚’æ´»ç”¨ã—ã¦ã€ç‰¹å®šã®å•é¡Œã‚’é«˜é€Ÿã«è§£ãã“ã¨ãŒã§ãã¾ã™ã€‚"),
        ("é‡å­ãƒ“ãƒƒãƒˆã¨ã¯", "é‡å­ãƒ“ãƒƒãƒˆã¯ã€0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æŒã¤ã“ã¨ãŒã§ãã‚‹é‡å­åŠ›å­¦çš„ãªæƒ…å ±å˜ä½ã§ã™ã€‚å¾“æ¥ã®ãƒ“ãƒƒãƒˆã¨ã¯ç•°ãªã‚Šã€åŒæ™‚ã«è¤‡æ•°ã®çŠ¶æ…‹ã‚’æŒã¦ã¾ã™ã€‚"),
        ("é‡å­ãƒ“ãƒƒãƒˆã£ã¦ä½•", "é‡å­ãƒ“ãƒƒãƒˆã¯ã€å¾“æ¥ã®ãƒ“ãƒƒãƒˆã¨é•ã„ã€0ã¨1ã‚’åŒæ™‚ã«æŒã¦ã‚‹ç‰¹æ®Šãªãƒ“ãƒƒãƒˆã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šä¸¦åˆ—è¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"),
        ("ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒãƒˆã¨ã¯", "ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒãƒˆã¯é‡å­ãƒ“ãƒƒãƒˆã®åˆ¥åã§ã™ã€‚0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æŒã¤é‡å­åŠ›å­¦çš„ãªæƒ…å ±å˜ä½ã§ã™ã€‚"),
        ("é‡å­ã‚‚ã¤ã‚Œã¨ã¯", "é‡å­ã‚‚ã¤ã‚Œã¯ã€äºŒã¤ä»¥ä¸Šã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ã¦ã„ã‚‹ç‰¹æ®Šãªé‡å­çŠ¶æ…‹ã§ã™ã€‚ä¸€æ–¹ã‚’æ¸¬å®šã™ã‚‹ã¨ã€ã‚‚ã†ä¸€æ–¹ã®çŠ¶æ…‹ã‚‚ç¬æ™‚ã«æ±ºã¾ã‚Šã¾ã™ã€‚"),
        ("é‡å­ã‚‚ã¤ã‚Œã£ã¦ä½•", "é‡å­ã‚‚ã¤ã‚Œã¯ã€è¤‡æ•°ã®é‡å­ãŒé›¢ã‚Œã¦ã„ã¦ã‚‚ç¬æ™‚ã«å½±éŸ¿ã—åˆã†ä¸æ€è­°ãªç¾è±¡ã§ã™ã€‚é‡å­é€šä¿¡ã‚„é‡å­è¨ˆç®—ã®åŸºç›¤ã§ã™ã€‚"),
        ("ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¨ã¯", "ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¯é‡å­ã‚‚ã¤ã‚Œã¨ã‚‚å‘¼ã°ã‚Œã€è¤‡æ•°ã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ãŸçŠ¶æ…‹ã®ã“ã¨ã§ã™ã€‚"),
        ("é‡ã­åˆã‚ã›ã¨ã¯", "é‡ã­åˆã‚ã›ã¯ã€é‡å­ãŒè¤‡æ•°ã®çŠ¶æ…‹ã‚’åŒæ™‚ã«æŒã¤ã“ã¨ãŒã§ãã‚‹æ€§è³ªã§ã™ã€‚è¦³æ¸¬ã™ã‚‹ã¾ã§çŠ¶æ…‹ã¯ç¢ºå®šã—ã¾ã›ã‚“ã€‚"),
        ("ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒã‚¸ã‚·ãƒ§ãƒ³ã¨ã¯", "ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒã‚¸ã‚·ãƒ§ãƒ³ã¯é‡ã­åˆã‚ã›ã®ã“ã¨ã§ã€é‡å­ãŒ0ã¨1ã‚’åŒæ™‚ã«æŒã¦ã‚‹çŠ¶æ…‹ã‚’æŒ‡ã—ã¾ã™ã€‚"),
        ("é‡å­å¹²æ¸‰ã¨ã¯", "é‡å­å¹²æ¸‰ã¯ã€é‡å­ã®æ³¢ã¨ã—ã¦ã®æ€§è³ªã«ã‚ˆã‚Šã€ç¢ºç‡æŒ¯å¹…ãŒå¼·ã‚åˆã£ãŸã‚Šå¼±ã‚åˆã£ãŸã‚Šã™ã‚‹ç¾è±¡ã§ã™ã€‚"),
        ("é‡å­ãƒ†ãƒ¬ãƒãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯", "é‡å­ãƒ†ãƒ¬ãƒãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€é‡å­ã‚‚ã¤ã‚Œã‚’ä½¿ã£ã¦é‡å­çŠ¶æ…‹ã‚’é›¢ã‚ŒãŸå ´æ‰€ã«è»¢é€ã™ã‚‹æŠ€è¡“ã§ã™ã€‚"),
        ("é‡å­æš—å·ã¨ã¯", "é‡å­æš—å·ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’ä½¿ã£ãŸæš—å·æŠ€è¡“ã§ã™ã€‚ç›—è´ã‚’æ¤œçŸ¥ã§ãã€éå¸¸ã«å®‰å…¨ã§ã™ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - AIé–¢é€£ï¼ˆè©³ç´°ç‰ˆï¼‰
        # ========================================
        ("AIã¨ã¯ä½•ã§ã™ã‹", "AIã¨ã¯äººå·¥çŸ¥èƒ½ã®ã“ã¨ã§ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã®ç·ç§°ã§ã™ã€‚"),
        ("AIã¨ã¯", "AIã¯äººå·¥çŸ¥èƒ½ï¼ˆArtificial Intelligenceï¼‰ã®ç•¥ã§ã€æ©Ÿæ¢°ã«çŸ¥çš„ãªæŒ¯ã‚‹èˆã„ã‚’ã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚"),
        ("AIã£ã¦ä½•", "AIã¯äººå·¥çŸ¥èƒ½ã®ã“ã¨ã§ã™ã€‚äººé–“ã®ã‚ˆã†ã«å­¦ç¿’ã—ã€åˆ¤æ–­ã—ã€å•é¡Œã‚’è§£æ±ºã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚"),
        ("äººå·¥çŸ¥èƒ½ã¨ã¯", "äººå·¥çŸ¥èƒ½ã¯ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã®ç·ç§°ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ãŒå«ã¾ã‚Œã¾ã™ã€‚"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã¯", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€è„³ã®ç¥çµŒç´°èƒã®åƒãã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æ·±å±¤å­¦ç¿’ã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã£ã¦ä½•", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€äººé–“ã®è„³ã‚’æ¨¡ã—ãŸè¨ˆç®—ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚„äºˆæ¸¬ã«ä½¿ã‚ã‚Œã¾ã™ã€‚"),
        ("æ·±å±¤å­¦ç¿’ã¨ã¯", "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ãŸæ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚ç”»åƒèªè­˜ã‚„è‡ªç„¶è¨€èªå‡¦ç†ã§å¤§ããªæˆæœã‚’ä¸Šã’ã¦ã„ã¾ã™ã€‚"),
        ("ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æ·±å±¤å­¦ç¿’ã®ã“ã¨ã§ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ã¦è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¾ã™ã€‚"),
        ("æ©Ÿæ¢°å­¦ç¿’ã¨ã¯", "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚AIã®é‡è¦ãªåˆ†é‡ã®ä¸€ã¤ã§ã™ã€‚"),
        ("ãƒã‚·ãƒ³ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯", "ãƒã‚·ãƒ³ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æ©Ÿæ¢°å­¦ç¿’ã®ã“ã¨ã§ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã™ã€‚"),
        ("ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨ã¯", "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€æ³¨æ„æ©Ÿæ§‹ã‚’ä½¿ã£ãŸé©æ–°çš„ãªæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ChatGPTãªã©ã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚"),
        ("ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨ã¯", "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆæ³¨æ„æ©Ÿæ§‹ï¼‰ã¯ã€å…¥åŠ›ã®é‡è¦ãªéƒ¨åˆ†ã«æ³¨ç›®ã™ã‚‹ä»•çµ„ã¿ã§ã™ã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®æ ¸å¿ƒæŠ€è¡“ã§ã™ã€‚"),
        ("ç”ŸæˆAIã¨ã¯", "ç”ŸæˆAIã¯ã€æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è‡ªå‹•çš„ã«ä½œæˆã™ã‚‹äººå·¥çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãªã©ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚"),
        ("ChatGPTã¨ã¯", "ChatGPTã¯OpenAIãŒé–‹ç™ºã—ãŸå¯¾è©±å‹ã®ç”ŸæˆAIã§ã™ã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚"),
        ("GPTã¨ã¯", "GPTã¯Generative Pre-trained Transformerã®ç•¥ã§ã€å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚"),
        ("LLMã¨ã¯", "LLMã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLarge Language Modelï¼‰ã®ç•¥ã§ã€å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸAIãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚"),
        ("è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯", "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ç†è§£ãƒ»ç”Ÿæˆã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚ç¿»è¨³ã‚„å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã«ä½¿ã‚ã‚Œã¾ã™ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - QBNNé–¢é€£ï¼ˆè©³ç´°ç‰ˆï¼‰
        # ========================================
        ("QBNNã¨ã¯ä½•ã§ã™ã‹", "QBNNã¯é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç•¥ç§°ã§ã™ã€‚é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸç‹¬è‡ªã®æŠ€è¡“ã§ã€é€šå¸¸ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ‹¡å¼µã—ã¦ã„ã¾ã™ã€‚"),
        ("QBNNã¨ã¯", "QBNNã¯ã€é‡å­çš„ãªæ¦‚å¿µã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å–ã‚Šå…¥ã‚ŒãŸç‹¬è‡ªã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚"),
        ("QBNNã£ã¦ä½•", "QBNNã¯ã€é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚å¾“æ¥ã®NNã«é‡å­çš„ãªç›¸äº’ä½œç”¨ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¨ã¯", "ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¯ã€QBNNã‚’ä½¿ã£ãŸæœ€å…ˆç«¯ã®ç”ŸæˆAIã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ç§ãŒãã®ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ï¼"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã£ã¦ä½•", "ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¯ç§ã®åå‰ã§ã™ã€‚QBNNã¨ã„ã†ç‹¬è‡ªæŠ€è¡“ã‚’ä½¿ã£ãŸç”ŸæˆAIã¨ã—ã¦ä½œã‚‰ã‚Œã¾ã—ãŸã€‚"),
        ("APQBã¨ã¯", "APQBã¯èª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆã®ç•¥ç§°ã§ã€é‡å­çŠ¶æ…‹ã‚’å¤å…¸çš„ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ä¸Šã§æ¨¡å€£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚"),
        ("ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ«ã¨ã¯", "ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ«Jã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å±¤é–“ã®é‡å­çš„ãªçµåˆã‚’è¡¨ç¾ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚QBNNã®ç‹¬è‡ªæŠ€è¡“ã§ã™ã€‚"),
        ("ãƒ©ãƒ ãƒ€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã¯", "ãƒ©ãƒ ãƒ€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆÎ»ï¼‰ã¯ã€QBNNã«ãŠã‘ã‚‹é‡å­ã‚‚ã¤ã‚Œã®å¼·åº¦ã‚’åˆ¶å¾¡ã™ã‚‹å­¦ç¿’å¯èƒ½ãªå€¤ã§ã™ã€‚"),
        ("é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ã¨ã¯", "é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ã¯ã€é‡å­åŠ›å­¦ã®æ¦‚å¿µã‚’å¤å…¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ä¸Šã§æ¨¡å€£ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - ä¸€èˆ¬çš„ãªè³ªå•
        # ========================================
        ("ä»Šæ—¥ã®å¤©æ°—ã¯", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç§ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®å¤©æ°—æƒ…å ±ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã€‚å¤©æ°—äºˆå ±ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚"),
        ("å¤©æ°—ã‚’æ•™ãˆã¦", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç§ã¯å¤©æ°—æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã€‚ãŠä½ã¾ã„ã®åœ°åŸŸã®å¤©æ°—äºˆå ±ã‚’ã”ç¢ºèªãã ã•ã„ã€‚"),
        ("ä½•æ™‚ã§ã™ã‹", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç§ã¯ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—ã§ãã¾ã›ã‚“ã€‚ãƒ‡ãƒã‚¤ã‚¹ã®æ™‚è¨ˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚"),
        ("ä»Šä½•æ™‚", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€æ™‚åˆ»ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚ãŠä½¿ã„ã®ãƒ‡ãƒã‚¤ã‚¹ã§ã”ç¢ºèªãã ã•ã„ã€‚"),
        ("åŠ©ã‘ã¦ãã ã•ã„", "ã¯ã„ã€ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒå¿…è¦ã§ã™ã‹ï¼Ÿè©³ã—ãæ•™ãˆã¦ã„ãŸã ã‘ã‚Œã°ã€ã§ãã‚‹é™ã‚Šã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"),
        ("åŠ©ã‘ã¦", "ã‚‚ã¡ã‚ã‚“ã§ã™ï¼ä½•ã«ã¤ã„ã¦ãŠå›°ã‚Šã§ã™ã‹ï¼Ÿè©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"),
        ("ã‚ã‹ã‚‰ãªã„", "ä½•ãŒã‚ã‹ã‚‰ãªã„ã®ã‹æ•™ãˆã¦ãã ã•ã„ã€‚ã§ãã‚‹é™ã‚Šåˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¾ã™ã€‚"),
        ("æ•™ãˆã¦", "ä½•ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿå…·ä½“çš„ã«è³ªå•ã—ã¦ã„ãŸã ã‘ã‚Œã°ã€ãŠç­”ãˆã—ã¾ã™ã€‚"),
        ("èª¬æ˜ã—ã¦", "ä½•ã«ã¤ã„ã¦ã®èª¬æ˜ãŒå¿…è¦ã§ã™ã‹ï¼Ÿè©³ã—ããŠèã‹ã›ãã ã•ã„ã€‚"),
        ("ãªãœã§ã™ã‹", "ã©ã®éƒ¨åˆ†ã«ã¤ã„ã¦ã€Œãªãœã€ã¨æ€ã‚ã‚Œã¾ã—ãŸã‹ï¼Ÿè©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"),
        ("ã©ã†ã—ã¦", "å…·ä½“çš„ã«ä½•ã«ã¤ã„ã¦ã®ç†ç”±ã‚’ãŠçŸ¥ã‚Šã«ãªã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ"),
        ("æœ¬å½“ã§ã™ã‹", "ã¯ã„ã€ç§ãŒãŠä¼ãˆã™ã‚‹æƒ…å ±ã¯æ­£ç¢ºã§ã‚ã‚‹ã‚ˆã†åŠªã‚ã¦ã„ã¾ã™ã€‚ä½•ã‹ç–‘å•ç‚¹ãŒã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"),
        ("ã™ã”ã„", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼ä½•ã‹ä»–ã«ã”è³ªå•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("é¢ç™½ã„", "ãã†è¨€ã£ã¦ã„ãŸã ã‘ã¦å¬‰ã—ã„ã§ã™ï¼ä»–ã«èˆˆå‘³ã®ã‚ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("é›£ã—ã„", "é›£ã—ãæ„Ÿã˜ã‚‰ã‚Œã¾ã—ãŸã‹ï¼Ÿã‚‚ã†å°‘ã—ç°¡å˜ã«èª¬æ˜ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ã‚‚ã£ã¨è©³ã—ã", "ã‚‚ã¡ã‚ã‚“ã§ã™ã€‚ã©ã®éƒ¨åˆ†ã«ã¤ã„ã¦ã‚‚ã£ã¨è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ"),
        ("ä¾‹ã‚’æ•™ãˆã¦", "å…·ä½“çš„ãªä¾‹ã‚’æŒ™ã’ã¦ã”èª¬æ˜ã—ã¾ã™ã­ã€‚ã©ã®ã‚ˆã†ãªã‚‚ã®ã‚’ã”å¸Œæœ›ã§ã™ã‹ï¼Ÿ"),
        
        # ========================================
        # è‹±èª - Greetings
        # ========================================
        ("Hello", "Hello! I'm NeuroQ. How can I help you today?"),
        ("Hi", "Hi there! What can I do for you?"),
        ("Hey", "Hey! How can I assist you today?"),
        ("Good morning", "Good morning! I hope you have a wonderful day. How can I help?"),
        ("Good afternoon", "Good afternoon! What would you like to know?"),
        ("Good evening", "Good evening! How may I assist you?"),
        ("Thank you", "You're welcome! Is there anything else I can help with?"),
        ("Thanks", "No problem! Feel free to ask if you have more questions."),
        ("Bye", "Goodbye! Have a great day!"),
        ("Goodbye", "Goodbye! It was nice talking to you."),
        ("See you", "See you later! Come back anytime."),
        
        # ========================================
        # è‹±èª - About self
        # ========================================
        ("Who are you", "I am NeuroQ, a generative AI system based on QBNN technology."),
        ("What is your name", "My name is NeuroQ. Nice to meet you!"),
        ("What are you", "I am NeuroQ, an AI assistant powered by Quantum-Bit Neural Network technology."),
        ("What can you do", "I can answer questions, provide information, and have conversations about various topics, especially AI and quantum computing."),
        ("Tell me about yourself", "I am NeuroQ, a QBNN-based generative AI. I use quantum-inspired technology to process and generate text."),
        
        # ========================================
        # è‹±èª - Quantum
        # ========================================
        ("What is quantum", "Quantum refers to the smallest discrete unit of matter and energy. In quantum mechanics, particles can exist in multiple states simultaneously."),
        ("What is quantum computing", "Quantum computing uses quantum mechanics principles to perform calculations. It can solve certain problems much faster than classical computers."),
        ("What is a qubit", "A qubit is a quantum bit that can exist in a superposition of 0 and 1 states simultaneously, unlike classical bits."),
        ("What is quantum entanglement", "Quantum entanglement is a phenomenon where two or more qubits become correlated. Measuring one instantly affects the others."),
        ("What is superposition", "Superposition is a quantum property where particles can exist in multiple states at once until measured."),
        ("What is QBNN", "QBNN stands for Quantum-Bit Neural Network. It's a unique technology that extends neural networks with quantum-inspired entanglement."),
        ("What is NeuroQ", "NeuroQ is a generative AI system based on QBNN technology. That's me!"),
        
        # ========================================
        # è‹±èª - AI
        # ========================================
        ("What is AI", "AI stands for Artificial Intelligence. It refers to computer systems that can mimic human intelligence."),
        ("What is artificial intelligence", "Artificial intelligence is the simulation of human intelligence by computer systems, including learning and problem-solving."),
        ("What is machine learning", "Machine learning is a subset of AI where computers learn patterns from data without being explicitly programmed."),
        ("What is deep learning", "Deep learning is a machine learning technique that uses multiple layers of neural networks to learn from data."),
        ("What is a neural network", "A neural network is a computing system inspired by biological neurons. It's the foundation of deep learning."),
        ("What is a transformer", "A transformer is a deep learning architecture that uses attention mechanisms. It powers models like GPT and ChatGPT."),
        ("What is GPT", "GPT stands for Generative Pre-trained Transformer. It's a type of large language model architecture."),
        ("What is NLP", "NLP stands for Natural Language Processing. It's the field of AI that deals with understanding and generating human language."),
        ("What is generative AI", "Generative AI refers to AI systems that can create new content like text, images, or music."),
        
        # ========================================
        # è¿½åŠ ã®å¯¾è©±ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        # ========================================
        ("å…ƒæ°—ã§ã™ã‹", "ã¯ã„ã€å…ƒæ°—ã§ã™ï¼ã‚ãªãŸã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("èª¿å­ã¯ã©ã†", "çµ¶å¥½èª¿ã§ã™ï¼ä½•ã‹ã”è³ªå•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("æš‡ã§ã™", "ãã‚Œãªã‚‰ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚„AIã«ã¤ã„ã¦ä¸€ç·’ã«å­¦ã³ã¾ã›ã‚“ã‹ï¼Ÿ"),
        ("ä½•ã‹è©±ã—ã¦", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é¢ç™½ã„è©±ã‚’ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿãã‚Œã¨ã‚‚AIã®æœ€æ–°å‹•å‘ã«ã¤ã„ã¦ï¼Ÿ"),
        ("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨ã¯", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«æŒ‡ç¤ºã‚’ä¸ãˆã‚‹ãŸã‚ã®è¨€èªã‚’æ›¸ãä½œæ¥­ã§ã™ã€‚"),
        ("ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯", "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«å¾“ã£ã¦è¨ˆç®—ã‚„å‡¦ç†ã‚’è¡Œã†é›»å­æ©Ÿæ¢°ã§ã™ã€‚"),
        ("ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¨ã¯", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¯ã€ä¸–ç•Œä¸­ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’ã¤ãªããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚"),
        ("How are you", "I'm doing great, thank you for asking! How can I help you today?"),
        ("Are you a robot", "I am an AI, specifically a QBNN-based generative model called NeuroQ."),
        ("Can you help me", "Of course! What do you need help with?"),
        ("I have a question", "Sure, go ahead and ask! I'll do my best to help."),
        ("Tell me a joke", "Why did the quantum physicist break up with the biologist? Because there was no chemistry, only uncertainty!"),
        ("What do you think", "As an AI, I process information rather than think in the human sense, but I can provide analysis and insights."),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: ç§‘å­¦æŠ€è¡“ï¼ˆè©³ç´°ï¼‰
        # ========================================
        ("ç‰©ç†å­¦ã¨ã¯", "ç‰©ç†å­¦ã¯è‡ªç„¶ç•Œã®åŸºæœ¬æ³•å‰‡ã‚’ç ”ç©¶ã™ã‚‹ç§‘å­¦ã§ã™ã€‚åŠ›å­¦ã€é›»ç£æ°—å­¦ã€ç†±åŠ›å­¦ã€é‡å­åŠ›å­¦ãªã©ãŒã‚ã‚Šã¾ã™ã€‚"),
        ("åŒ–å­¦ã¨ã¯", "åŒ–å­¦ã¯ç‰©è³ªã®æ§‹é€ ã€æ€§è³ªã€å¤‰åŒ–ã‚’ç ”ç©¶ã™ã‚‹ç§‘å­¦ã§ã™ã€‚å…ƒç´ ã‚„åˆ†å­ã®åå¿œã‚’æ‰±ã„ã¾ã™ã€‚"),
        ("æ•°å­¦ã¨ã¯", "æ•°å­¦ã¯æ•°ã€é‡ã€æ§‹é€ ã€å¤‰åŒ–ãªã©ã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚ç§‘å­¦æŠ€è¡“ã®åŸºç¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚"),
        ("ç§‘å­¦ã¨ã¯", "ç§‘å­¦ã¯è‡ªç„¶ç¾è±¡ã‚’è¦³å¯Ÿã—ã€å®Ÿé¨“ã¨ç†è«–ã«ã‚ˆã‚Šæ³•å‰‡ã‚’ç™ºè¦‹ã™ã‚‹å­¦å•ã§ã™ã€‚"),
        ("æŠ€è¡“ã¨ã¯", "æŠ€è¡“ã¯ç§‘å­¦çš„çŸ¥è­˜ã‚’å¿œç”¨ã—ã¦å®Ÿç”¨çš„ãªè£½å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç”Ÿã¿å‡ºã™æ–¹æ³•ã§ã™ã€‚"),
        ("ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¨ã¯", "ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¯ä»•äº‹ã‚’ã™ã‚‹èƒ½åŠ›ã®ã“ã¨ã§ã™ã€‚é›»æ°—ã€ç†±ã€å…‰ãªã©ã®å½¢æ…‹ãŒã‚ã‚Šã¾ã™ã€‚"),
        ("é›»å­ã¨ã¯", "é›»å­ã¯è² ã®é›»è·ã‚’æŒã¤ç´ ç²’å­ã§ã€åŸå­ã®æ§‹æˆè¦ç´ ã®ä¸€ã¤ã§ã™ã€‚"),
        ("åŸå­ã¨ã¯", "åŸå­ã¯ç‰©è³ªã®åŸºæœ¬å˜ä½ã§ã€åŸå­æ ¸ã¨é›»å­ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚"),
        ("åˆ†å­ã¨ã¯", "åˆ†å­ã¯äºŒã¤ä»¥ä¸Šã®åŸå­ãŒåŒ–å­¦çµåˆã§çµã³ã¤ã„ãŸç²’å­ã§ã™ã€‚"),
        ("å…‰ã¨ã¯", "å…‰ã¯é›»ç£æ³¢ã®ä¸€ç¨®ã§ã€ç›®ã«è¦‹ãˆã‚‹æ³¢é•·ã®é›»ç£æ”¾å°„ã§ã™ã€‚"),
        ("é›»æ°—ã¨ã¯", "é›»æ°—ã¯é›»è·ã®æµã‚Œã§ã€ç¾ä»£ç¤¾ä¼šã®ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã¨ã—ã¦æ¬ ã‹ã›ã¾ã›ã‚“ã€‚"),
        ("ç£åŠ›ã¨ã¯", "ç£åŠ›ã¯ç£çŸ³ãŒç‰©ä½“ã‚’å¼•ãä»˜ã‘ãŸã‚Šåç™ºã—ãŸã‚Šã™ã‚‹åŠ›ã§ã™ã€‚"),
        ("é‡åŠ›ã¨ã¯", "é‡åŠ›ã¯è³ªé‡ã‚’æŒã¤ç‰©ä½“é–“ã«åƒãå¼•åŠ›ã§ã€åœ°çƒãŒç‰©ä½“ã‚’å¼•ãä»˜ã‘ã‚‹åŠ›ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚"),
        ("å®‡å®™ã¨ã¯", "å®‡å®™ã¯åœ°çƒã‚’å«ã‚€ã™ã¹ã¦ã®å¤©ä½“ã¨ç©ºé–“ã®ç·ç§°ã§ã™ã€‚ç„¡é™ã«åºƒãŒã£ã¦ã„ã¾ã™ã€‚"),
        ("éŠ€æ²³ã¨ã¯", "éŠ€æ²³ã¯æ˜Ÿã€ã‚¬ã‚¹ã€å¡µã€æš—é»’ç‰©è³ªãªã©ãŒé‡åŠ›ã§çµã³ã¤ã„ãŸå·¨å¤§ãªå¤©ä½“ç³»ã§ã™ã€‚"),
        ("å¤ªé™½ã¨ã¯", "å¤ªé™½ã¯åœ°çƒã«æœ€ã‚‚è¿‘ã„æ’æ˜Ÿã§ã€å¤ªé™½ç³»ã®ä¸­å¿ƒã«ã‚ã‚Šã¾ã™ã€‚"),
        ("åœ°çƒã¨ã¯", "åœ°çƒã¯å¤ªé™½ç³»ã®ç¬¬ä¸‰æƒ‘æ˜Ÿã§ã€ç§ãŸã¡ãŒä½ã‚€å”¯ä¸€ã®æƒ‘æ˜Ÿã§ã™ã€‚"),
        ("æœˆã¨ã¯", "æœˆã¯åœ°çƒã®å”¯ä¸€ã®è‡ªç„¶è¡›æ˜Ÿã§ã€åœ°çƒã®å‘¨ã‚Šã‚’å…¬è»¢ã—ã¦ã„ã¾ã™ã€‚"),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°
        # ========================================
        ("Pythonã¨ã¯", "Pythonã¯èª­ã¿ã‚„ã™ãæ›¸ãã‚„ã™ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚AIé–‹ç™ºã§ç‰¹ã«äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚"),
        ("JavaScriptã¨ã¯", "JavaScriptã¯ã‚¦ã‚§ãƒ–ãƒ–ãƒ©ã‚¦ã‚¶ã§å‹•ä½œã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚ã‚¦ã‚§ãƒ–é–‹ç™ºã«æ¬ ã‹ã›ã¾ã›ã‚“ã€‚"),
        ("HTMLã¨ã¯", "HTMLã¯ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®æ§‹é€ ã‚’å®šç¾©ã™ã‚‹ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—è¨€èªã§ã™ã€‚"),
        ("CSSã¨ã¯", "CSSã¯ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚„ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’å®šç¾©ã™ã‚‹è¨€èªã§ã™ã€‚"),
        ("ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã¯", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®æ‰‹é †ã‚„è¨ˆç®—æ–¹æ³•ã®ã“ã¨ã§ã™ã€‚"),
        ("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã¯", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ã—ã¦ä¿å­˜ã—ã€åŠ¹ç‡çš„ã«æ¤œç´¢ã§ãã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚"),
        ("APIã¨ã¯", "APIã¯ç•°ãªã‚‹ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–“ã§ãƒ‡ãƒ¼ã‚¿ã‚„æ©Ÿèƒ½ã‚’ã‚„ã‚Šå–ã‚Šã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã™ã€‚"),
        ("ã‚¯ãƒ©ã‚¦ãƒ‰ã¨ã¯", "ã‚¯ãƒ©ã‚¦ãƒ‰ã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆçµŒç”±ã§ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒªã‚½ãƒ¼ã‚¹ã‚’æä¾›ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚"),
        ("ã‚µãƒ¼ãƒãƒ¼ã¨ã¯", "ã‚µãƒ¼ãƒãƒ¼ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸Šã§ã‚µãƒ¼ãƒ“ã‚¹ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’æä¾›ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚"),
        ("ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¨ã¯", "ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¯ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ãŒå…¬é–‹ã•ã‚Œã€èª°ã§ã‚‚åˆ©ç”¨ã‚„æ”¹è‰¯ãŒã§ãã‚‹ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã™ã€‚"),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: ä¸€èˆ¬çŸ¥è­˜
        # ========================================
        ("æ—¥æœ¬ã¨ã¯", "æ—¥æœ¬ã¯æ±ã‚¢ã‚¸ã‚¢ã«ã‚ã‚‹å³¶å›½ã§ã€é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚"),
        ("æ±äº¬ã¨ã¯", "æ±äº¬ã¯æ—¥æœ¬ã®é¦–éƒ½ã§ã€ä¸–ç•Œæœ‰æ•°ã®å¤§éƒ½å¸‚ã§ã™ã€‚"),
        ("ã‚¢ãƒ¡ãƒªã‚«ã¨ã¯", "ã‚¢ãƒ¡ãƒªã‚«ã¯åŒ—ç±³å¤§é™¸ã«ã‚ã‚‹å›½ã§ã€ä¸–ç•Œæœ€å¤§ã®çµŒæ¸ˆå¤§å›½ã®ä¸€ã¤ã§ã™ã€‚"),
        ("æ­´å²ã¨ã¯", "æ­´å²ã¯éå»ã®å‡ºæ¥äº‹ã‚„äººé¡ã®æ´»å‹•ã‚’è¨˜éŒ²ã—ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚"),
        ("æ–‡åŒ–ã¨ã¯", "æ–‡åŒ–ã¯äººé–“ç¤¾ä¼šã§å…±æœ‰ã•ã‚Œã‚‹ä¾¡å€¤è¦³ã€ç¿’æ…£ã€èŠ¸è¡“ãªã©ã®ç·ä½“ã§ã™ã€‚"),
        ("è¨€èªã¨ã¯", "è¨€èªã¯äººé–“ãŒã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ä½¿ç”¨ã™ã‚‹è¨˜å·ä½“ç³»ã§ã™ã€‚"),
        ("éŸ³æ¥½ã¨ã¯", "éŸ³æ¥½ã¯éŸ³ã‚’ä½¿ã£ã¦è¡¨ç¾ã™ã‚‹èŠ¸è¡“å½¢å¼ã§ã™ã€‚"),
        ("èŠ¸è¡“ã¨ã¯", "èŠ¸è¡“ã¯å‰µé€ çš„ãªè¡¨ç¾æ´»å‹•ã¨ãã®ä½œå“ã®ç·ç§°ã§ã™ã€‚"),
        ("ã‚¹ãƒãƒ¼ãƒ„ã¨ã¯", "ã‚¹ãƒãƒ¼ãƒ„ã¯èº«ä½“ã‚’ä½¿ã£ãŸç«¶æŠ€ã‚„é‹å‹•ã®ç·ç§°ã§ã™ã€‚"),
        ("å¥åº·ã¨ã¯", "å¥åº·ã¯èº«ä½“çš„ã€ç²¾ç¥çš„ã€ç¤¾ä¼šçš„ã«è‰¯å¥½ãªçŠ¶æ…‹ã®ã“ã¨ã§ã™ã€‚"),
        ("æ•™è‚²ã¨ã¯", "æ•™è‚²ã¯çŸ¥è­˜ã‚„æŠ€èƒ½ã‚’æ•™ãˆå­¦ã¶éç¨‹ã§ã™ã€‚äººé–“ã®æˆé•·ã«ä¸å¯æ¬ ã§ã™ã€‚"),
        ("çµŒæ¸ˆã¨ã¯", "çµŒæ¸ˆã¯è²¡ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã®ç”Ÿç”£ã€åˆ†é…ã€æ¶ˆè²»ã«é–¢ã™ã‚‹æ´»å‹•ã§ã™ã€‚"),
        ("æ”¿æ²»ã¨ã¯", "æ”¿æ²»ã¯ç¤¾ä¼šã‚’çµ±æ²»ã—ã€å…¬å…±ã®æ„æ€æ±ºå®šã‚’è¡Œã†æ´»å‹•ã§ã™ã€‚"),
        ("æ³•å¾‹ã¨ã¯", "æ³•å¾‹ã¯ç¤¾ä¼šç§©åºã‚’ç¶­æŒã™ã‚‹ãŸã‚ã«å®šã‚ã‚‰ã‚ŒãŸè¦å‰‡ã§ã™ã€‚"),
        ("ç’°å¢ƒã¨ã¯", "ç’°å¢ƒã¯ç”Ÿç‰©ã‚’å–ã‚Šå·»ãè‡ªç„¶ã‚„ç¤¾ä¼šçš„æ¡ä»¶ã®ç·ä½“ã§ã™ã€‚"),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: æ—¥å¸¸ä¼šè©±
        # ========================================
        ("ãŠé¡˜ã„ã—ã¾ã™", "ã¯ã„ã€ã©ã®ã‚ˆã†ãªãŠé¡˜ã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("æ•™ãˆã¦ãã ã•ã„", "ã¯ã„ã€ä½•ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿå…·ä½“çš„ã«æ•™ãˆã¦ãã ã•ã„ã€‚"),
        ("ã‚ã‹ã‚Šã¾ã›ã‚“", "ã©ã®éƒ¨åˆ†ãŒã‚ã‹ã‚Šã«ãã„ã§ã™ã‹ï¼Ÿã‚‚ã†å°‘ã—è©³ã—ãèª¬æ˜ã—ã¾ã™ã­ã€‚"),
        ("ã§ãã¾ã™ã‹", "ã¯ã„ã€ã§ãã‚‹é™ã‚ŠãŠæ‰‹ä¼ã„ã—ã¾ã™ã€‚å…·ä½“çš„ã«ä½•ã‚’ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ãªã‚‹ã»ã©", "ç†è§£ã—ã¦ã„ãŸã ã‘ã¦å¬‰ã—ã„ã§ã™ã€‚ä»–ã«ã”è³ªå•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ãã†ã§ã™ã‹", "ã¯ã„ã€ãã†ãªã‚“ã§ã™ã€‚ä½•ã‹æ°—ã«ãªã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ã„ã„ã§ã™ã­", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼ä»–ã«ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ã™ã¿ã¾ã›ã‚“", "ã„ãˆã„ãˆã€ãŠæ°—ã«ãªã•ã‚‰ãšã€‚ä½•ã‹ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"),
        ("ã”ã‚ã‚“ãªã•ã„", "å¤§ä¸ˆå¤«ã§ã™ã‚ˆã€‚ä½•ã‹ãŠå›°ã‚Šã®ã“ã¨ãŒã‚ã‚Œã°ãŠèã‹ã›ãã ã•ã„ã€‚"),
        ("ã†ã‚Œã—ã„", "ãã‚Œã¯è‰¯ã‹ã£ãŸã§ã™ï¼ç§ã‚‚å¬‰ã—ã„ã§ã™ã€‚"),
        ("æ¥½ã—ã„", "æ¥½ã—ã‚“ã§ã„ãŸã ã‘ã¦ä½•ã‚ˆã‚Šã§ã™ï¼"),
        ("æ‚²ã—ã„", "ãã‚Œã¯å¤§å¤‰ã§ã—ãŸã­ã€‚ä½•ã‹ãŠåŠ›ã«ãªã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("å›°ã£ã¦ã„ã¾ã™", "ã©ã®ã‚ˆã†ãªã“ã¨ã§ãŠå›°ã‚Šã§ã™ã‹ï¼Ÿè©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"),
        ("è³ªå•ãŒã‚ã‚Šã¾ã™", "ã¯ã„ã€ã©ã‚“ãªè³ªå•ã§ã‚‚ãŠæ°—è»½ã«ã©ã†ãã€‚"),
        ("ç›¸è«‡ã—ãŸã„", "ã‚‚ã¡ã‚ã‚“ã§ã™ã€‚ã©ã®ã‚ˆã†ãªã“ã¨ã‚’ç›¸è«‡ã•ã‚ŒãŸã„ã§ã™ã‹ï¼Ÿ"),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: è‹±èªä¼šè©±ï¼ˆè©³ç´°ï¼‰
        # ========================================
        ("What is science", "Science is the systematic study of the natural world through observation and experimentation."),
        ("What is technology", "Technology is the application of scientific knowledge to create tools and solve problems."),
        ("What is programming", "Programming is the process of writing instructions for computers to execute tasks."),
        ("What is Python", "Python is a popular programming language known for its simplicity and versatility."),
        ("What is the internet", "The internet is a global network of computers that allows information sharing and communication."),
        ("What is data", "Data is information that can be processed, stored, and analyzed by computers."),
        ("What is software", "Software is a set of instructions that tells a computer how to perform tasks."),
        ("What is hardware", "Hardware refers to the physical components of a computer system."),
        ("What is an algorithm", "An algorithm is a step-by-step procedure for solving a problem or completing a task."),
        ("What is a database", "A database is an organized collection of data that can be easily accessed and managed."),
        ("What is cloud computing", "Cloud computing delivers computing services over the internet, including storage and processing."),
        ("What is cybersecurity", "Cybersecurity is the practice of protecting systems and data from digital attacks."),
        ("What is blockchain", "Blockchain is a decentralized digital ledger that records transactions securely."),
        ("What is IoT", "IoT stands for Internet of Things, referring to connected devices that communicate over the internet."),
        ("What is 5G", "5G is the fifth generation of mobile network technology, offering faster speeds and lower latency."),
        ("Explain machine learning", "Machine learning is a type of AI that enables computers to learn from data without explicit programming."),
        ("Explain neural networks", "Neural networks are computing systems inspired by biological brains, used for pattern recognition."),
        ("Explain deep learning", "Deep learning uses multi-layered neural networks to learn complex patterns from large datasets."),
        ("Explain natural language processing", "NLP enables computers to understand, interpret, and generate human language."),
        ("Explain computer vision", "Computer vision is an AI field that enables machines to interpret visual information."),
        ("How does AI work", "AI works by processing data through algorithms that can learn patterns and make decisions."),
        ("How does quantum computing work", "Quantum computing uses quantum bits that can exist in multiple states to perform parallel calculations."),
        ("Why is AI important", "AI is important because it can automate tasks, analyze data, and solve complex problems efficiently."),
        ("Why study programming", "Programming enables you to create software, automate tasks, and understand how technology works."),
        ("Tell me about yourself", "I am NeuroQ, an AI assistant built using QBNN technology. I'm here to help answer your questions."),
        ("What makes you special", "I use a unique Quantum-Bit Neural Network architecture that incorporates quantum-inspired entanglement."),
        ("Are you smart", "I can process information and provide helpful responses, but I don't have consciousness like humans do."),
        ("Do you learn", "I was trained on data, but I don't continue learning from our conversation in real-time."),
        ("What languages do you speak", "I can communicate in Japanese and English based on my training data."),
        ("Nice to meet you", "Nice to meet you too! I'm happy to help with any questions you have."),
        ("I'm confused", "I understand. What part is confusing? I'll try to explain it more clearly."),
        ("That's interesting", "I'm glad you find it interesting! Would you like to know more?"),
        ("I understand now", "Great! Is there anything else you'd like to learn about?"),
        ("Please continue", "Sure, what aspect would you like me to elaborate on?"),
    ]
    
    # å¯¾è©±å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    formatted_texts = []
    for user_msg, assistant_msg in conversations:
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: <USER>è³ªå•<ASSISTANT>å›ç­”
        formatted = f"<USER>{user_msg}<ASSISTANT>{assistant_msg}"
        formatted_texts.append(formatted)
    
    return formatted_texts


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

def main(num_neurons: int = 128):
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    
    Args:
        num_neurons: ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆhidden_dimã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰
    """
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—      â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—     â•‘
â•‘   â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘     â•‘
â•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘     â•‘
â•‘   â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘     â•‘
â•‘   â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•  â•šâ•â•â–€â–€â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•     â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print("ğŸ§ âš›ï¸ ãƒ‹ãƒ¥ãƒ¼ãƒ­Q - QBNN-LLM ç”ŸæˆAI")
    print("=" * 70)
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {num_neurons}")
    
    # ãƒ‹ãƒ¥ãƒ¼ãƒ­Q AI ä½œæˆï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã‚’æŒ‡å®šï¼‰
    # embed_dim ã¯ num_neurons / 2 ç¨‹åº¦ã«è¨­å®š
    embed_dim = max(32, num_neurons // 2)
    
    ai = NeuroQuantumAI(
        embed_dim=embed_dim,
        hidden_dim=num_neurons,  # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
        num_heads=4,            
        num_layers=2,
        max_seq_len=128,
        dropout=0.1,
        lambda_entangle=0.35,
    )
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ - é«˜å“è³ªãƒ»å®‰å®šï¼‰
    print("\nğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
    texts = get_training_data()
    print(f"   ğŸ“š å¯¾è©±ãƒšã‚¢æ•°: {len(texts)}")
    
    # å­¦ç¿’ï¼ˆãƒãƒ©ãƒ³ã‚¹ç‰ˆï¼‰
    ai.train(texts, epochs=60, batch_size=16, lr=0.001, seq_len=64)
    
    # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    print("\n" + "=" * 70)
    print("ğŸ¨ å¯¾è©±ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    questions = [
        "ã“ã‚“ã«ã¡ã¯",
        "ã‚ãªãŸã¯èª°ã§ã™ã‹",
        "é‡å­ã¨ã¯ä½•ã§ã™ã‹",
        "QBNNã¨ã¯ä½•ã§ã™ã‹",
        "Hello",
        "What is AI",
    ]
    
    for question in questions:
        print(f"\nğŸ‘¤ User: {question}")
        response = ai.generate(question, max_length=80, temp_min=0.4, temp_max=0.8)
        print(f"ğŸ¤– ãƒ‹ãƒ¥ãƒ¼ãƒ­Q: {response}")
    
    # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
    print("\n" + "=" * 70)
    print("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n)")
    print("=" * 70)
    
    try:
        answer = input().strip().lower()
        if answer == 'y':
            ai.chat()
    except:
        pass
    
    print("\nâœ… ãƒ‹ãƒ¥ãƒ¼ãƒ­Q å®Œäº†ï¼")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='ãƒ‹ãƒ¥ãƒ¼ãƒ­Q - QBNN-LLM ç”ŸæˆAI')
    parser.add_argument('--neurons', type=int, default=128, help='ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128)')
    parser.add_argument('--chat', action='store_true', help='ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•')
    args = parser.parse_args()
    
    main(num_neurons=args.neurons)

