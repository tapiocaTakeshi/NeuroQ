#!/usr/bin/env python3
"""
QBNN Generative AI - ç‹¬è‡ªã®Quantum-Bit Neural Networkã‚’ä½¿ã£ãŸç”ŸæˆAI
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from collections import Counter

print("=" * 70)
print("ğŸ§ âš›ï¸ QBNN Generative AI - ç‹¬è‡ªã®é‡å­ãƒ“ãƒƒãƒˆNNã§ç”ŸæˆAI")
print("=" * 70)


# ========================================
# QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆç‹¬è‡ªï¼‰
# ========================================

class QBNNLayer(nn.Module):
    """
    Quantum-Bit Neural Network Layer
    
    æ•°å¼ãƒ¢ãƒ‡ãƒ«:
    1. s^(l) = tanh(h^(l)) âˆˆ [-1, 1]  (æ­£è¦åŒ–)
    2. Î¸^(l)_i = arccos(s^(l)_i)       (Blochè§’)
    3. hÌƒ^(l+1) = W^(l) h^(l) + b^(l)  (ç·šå½¢å¤‰æ›)
    4. Î”^(l+1)_j = Î£_i J^(l)_{ij} s^(l)_i s^(l+1)_{raw,j}  (ã‚‚ã¤ã‚Œè£œæ­£)
    5. Ä¥^(l+1) = hÌƒ^(l+1) + Î»^(l) Î”^(l+1)  (æœ‰åŠ¹å…¥åŠ›)
    6. h^(l+1) = tanh(Ä¥^(l+1))  (æ´»æ€§åŒ–)
    """
    
    def __init__(self, input_dim, output_dim, lambda_val=0.5):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # é€šå¸¸ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹
        self.W = nn.Linear(input_dim, output_dim)
        
        # ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ« Jï¼ˆç‹¬è‡ªï¼‰
        self.J = nn.Parameter(torch.randn(input_dim, output_dim) * 0.1)
        
        # ã‚‚ã¤ã‚Œå¼·åº¦ Î»ï¼ˆç‹¬è‡ªï¼‰
        self.lambda_val = nn.Parameter(torch.tensor(float(lambda_val)))
    
    def forward(self, h_prev):
        # 1. æ­£è¦åŒ–
        s_prev = torch.tanh(h_prev)
        
        # 2. ç·šå½¢å¤‰æ›
        h_tilde = self.W(h_prev)
        
        # 3. æ­£è¦åŒ–ï¼ˆæ¬¡å±¤ã®å€™è£œï¼‰
        s_raw = torch.tanh(h_tilde)
        
        # 4. ã‚‚ã¤ã‚Œè£œæ­£é … Î”
        # Î”_j = Î£_i J_{ij} s^(l)_i s^(l+1)_{raw,j}
        delta = torch.einsum('...i,ij,...j->...j', s_prev, self.J, s_raw)
        
        # 5. æœ‰åŠ¹å…¥åŠ›
        h_hat = h_tilde + self.lambda_val * delta
        
        # 6. æ´»æ€§åŒ–
        return torch.tanh(h_hat)
    
    def get_entanglement_info(self):
        """ã‚‚ã¤ã‚Œæƒ…å ±ã‚’å–å¾—"""
        with torch.no_grad():
            j_mean = self.J.mean().item()
            j_std = self.J.std().item()
            lam = self.lambda_val.item()
        return {'lambda': lam, 'J_mean': j_mean, 'J_std': j_std}


# ========================================
# QBNNè¨€èªãƒ¢ãƒ‡ãƒ«
# ========================================

class QBNNLanguageModel(nn.Module):
    """QBNNãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, 
                 num_layers=3, lambda_val=0.5, dropout=0.1):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # åŸ‹ã‚è¾¼ã¿å±¤
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoding = PositionalEncoding(embed_dim, dropout)
        
        # QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆç‹¬è‡ªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰
        self.qbnn_layers = nn.ModuleList()
        
        # å…¥åŠ›å±¤
        self.qbnn_layers.append(QBNNLayer(embed_dim, hidden_dim, lambda_val))
        
        # ä¸­é–“å±¤
        for _ in range(num_layers - 2):
            self.qbnn_layers.append(QBNNLayer(hidden_dim, hidden_dim, lambda_val))
        
        # å‡ºåŠ›å±¤
        self.qbnn_layers.append(QBNNLayer(hidden_dim, embed_dim, lambda_val))
        
        # å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        self.output_proj = nn.Linear(embed_dim, vocab_size)
        
        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        self.dropout = nn.Dropout(dropout)
        
        print(f"\nğŸ“Š QBNNLanguageModel æ§‹æˆ:")
        print(f"   èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
        print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embed_dim}")
        print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {hidden_dim}")
        print(f"   QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {num_layers}")
        print(f"   ã‚‚ã¤ã‚Œå¼·åº¦ Î»: {lambda_val}")
    
    def forward(self, x):
        # x: (batch, seq_len)
        
        # åŸ‹ã‚è¾¼ã¿
        h = self.embedding(x)  # (batch, seq_len, embed_dim)
        h = self.pos_encoding(h)
        
        # QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é€šã™
        for layer in self.qbnn_layers:
            h = self.dropout(layer(h))
        
        # å‡ºåŠ›
        logits = self.output_proj(h)  # (batch, seq_len, vocab_size)
        
        return logits
    
    def get_entanglement_info(self):
        """å…¨å±¤ã®ã‚‚ã¤ã‚Œæƒ…å ±"""
        info = []
        for i, layer in enumerate(self.qbnn_layers):
            layer_info = layer.get_entanglement_info()
            info.append(f"Layer {i}: Î»={layer_info['lambda']:.3f}")
        return info


class PositionalEncoding(nn.Module):
    """ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
    
    def __init__(self, d_model, dropout=0.1, max_len=512):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


# ========================================
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
# ========================================

class CharTokenizer:
    """æ–‡å­—ãƒ¬ãƒ™ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"""
    
    def __init__(self):
        self.char_to_idx = {}
        self.idx_to_char = {}
        self.vocab_size = 0
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.pad_token = '<PAD>'
        self.unk_token = '<UNK>'
        self.bos_token = '<BOS>'
        self.eos_token = '<EOS>'
    
    def build_vocab(self, texts):
        """èªå½™ã‚’æ§‹ç¯‰"""
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        special_tokens = [self.pad_token, self.unk_token, self.bos_token, self.eos_token]
        
        # å…¨æ–‡å­—ã‚’åé›†
        chars = set()
        for text in texts:
            chars.update(text)
        
        # è¾æ›¸ä½œæˆ
        all_tokens = special_tokens + sorted(list(chars))
        self.char_to_idx = {c: i for i, c in enumerate(all_tokens)}
        self.idx_to_char = {i: c for i, c in enumerate(all_tokens)}
        self.vocab_size = len(all_tokens)
        
        print(f"   èªå½™ã‚µã‚¤ã‚º: {self.vocab_size}")
        return self
    
    def encode(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«å¤‰æ›"""
        tokens = [self.char_to_idx.get(self.bos_token)]
        for char in text:
            tokens.append(self.char_to_idx.get(char, self.char_to_idx[self.unk_token]))
        tokens.append(self.char_to_idx.get(self.eos_token))
        return tokens
    
    def decode(self, tokens):
        """ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        chars = []
        for t in tokens:
            char = self.idx_to_char.get(t, self.unk_token)
            if char not in [self.pad_token, self.bos_token, self.eos_token, self.unk_token]:
                chars.append(char)
        return ''.join(chars)


# ========================================
# ç”ŸæˆAI
# ========================================

class QBNNGenerativeAI:
    """QBNNç”ŸæˆAI"""
    
    def __init__(self, embed_dim=128, hidden_dim=256, num_layers=3, 
                 lambda_val=0.5, dropout=0.1):
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.lambda_val = lambda_val
        self.dropout = dropout
        
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"\nğŸ”§ ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
    
    def train(self, texts, epochs=100, batch_size=32, lr=0.001, seq_len=64):
        """å­¦ç¿’"""
        print("\n" + "=" * 50)
        print("ğŸ“š å­¦ç¿’é–‹å§‹")
        print("=" * 50)
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰
        print("\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰...")
        self.tokenizer = CharTokenizer()
        self.tokenizer.build_vocab(texts)
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        print("\nğŸ§  QBNNãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰...")
        self.model = QBNNLanguageModel(
            vocab_size=self.tokenizer.vocab_size,
            embed_dim=self.embed_dim,
            hidden_dim=self.hidden_dim,
            num_layers=self.num_layers,
            lambda_val=self.lambda_val,
            dropout=self.dropout
        ).to(self.device)
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
        all_tokens = []
        for text in texts:
            tokens = self.tokenizer.encode(text)
            all_tokens.extend(tokens)
        
        all_tokens = torch.tensor(all_tokens, dtype=torch.long)
        print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens)}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        sequences = []
        for i in range(0, len(all_tokens) - seq_len - 1, seq_len // 2):
            x = all_tokens[i:i+seq_len]
            y = all_tokens[i+1:i+seq_len+1]
            if len(x) == seq_len and len(y) == seq_len:
                sequences.append((x, y))
        
        print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences)}")
        
        # å­¦ç¿’
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()
        
        print("\nğŸš€ å­¦ç¿’ãƒ«ãƒ¼ãƒ—...")
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            np.random.shuffle(sequences)
            
            for i in range(0, len(sequences), batch_size):
                batch = sequences[i:i+batch_size]
                if len(batch) == 0:
                    continue
                
                x_batch = torch.stack([s[0] for s in batch]).to(self.device)
                y_batch = torch.stack([s[1] for s in batch]).to(self.device)
                
                optimizer.zero_grad()
                logits = self.model(x_batch)
                
                # (batch, seq, vocab) -> (batch * seq, vocab)
                loss = criterion(
                    logits.view(-1, self.tokenizer.vocab_size),
                    y_batch.view(-1)
                )
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / max(1, len(sequences) // batch_size)
            
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(f"   Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}")
        
        print("\nâœ… å­¦ç¿’å®Œäº†ï¼")
        
        # ã‚‚ã¤ã‚Œæƒ…å ±
        print("\nâš›ï¸ é‡å­ã‚‚ã¤ã‚Œæƒ…å ±:")
        for info in self.model.get_entanglement_info():
            print(f"   {info}")
    
    def generate(self, prompt="", max_length=100, temperature=1.0, 
                 top_k=50, top_p=0.9, repetition_penalty=1.2):
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.model.eval()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        if prompt:
            tokens = self.tokenizer.encode(prompt)[:-1]  # EOSã‚’é™¤ã
        else:
            tokens = [self.tokenizer.char_to_idx[self.tokenizer.bos_token]]
        
        tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)
        
        generated = tokens[0].tolist()
        
        with torch.no_grad():
            for _ in range(max_length):
                # æœ€å¾Œã®64ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨
                input_tokens = tokens[:, -64:] if tokens.size(1) > 64 else tokens
                
                logits = self.model(input_tokens)
                next_logits = logits[0, -1, :]  # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬
                
                # æ¸©åº¦èª¿æ•´
                next_logits = next_logits / temperature
                
                # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
                for token_id in set(generated[-20:]):
                    next_logits[token_id] /= repetition_penalty
                
                # Top-K
                if top_k > 0:
                    indices_to_remove = next_logits < torch.topk(next_logits, top_k)[0][-1]
                    next_logits[indices_to_remove] = float('-inf')
                
                # Top-P (Nucleus)
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                    sorted_indices_to_remove[0] = False
                    
                    indices_to_remove = sorted_indices[sorted_indices_to_remove]
                    next_logits[indices_to_remove] = float('-inf')
                
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                probs = F.softmax(next_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # EOSæ¤œå‡º
                if next_token.item() == self.tokenizer.char_to_idx[self.tokenizer.eos_token]:
                    break
                
                generated.append(next_token.item())
                tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)
        
        return self.tokenizer.decode(generated)
    
    def chat(self):
        """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
        print("\n" + "=" * 50)
        print("ğŸ’¬ QBNN ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰")
        print("=" * 50)
        print("ã‚³ãƒãƒ³ãƒ‰:")
        print("  /quit  - çµ‚äº†")
        print("  /temp <å€¤> - æ¸©åº¦è¨­å®š (0.1-1.0)")
        print("  /len <å€¤> - ç”Ÿæˆé•·ã• (10-500)")
        print("  /info - ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
        print("-" * 50)
        
        temperature = 1.0
        max_length = 100
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ You: ").strip()
                
                if not user_input:
                    continue
                
                if user_input == '/quit':
                    print("ğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼")
                    break
                
                if user_input.startswith('/temp '):
                    try:
                        temperature = float(user_input.split()[1])
                        temperature = max(0.1, min(1.0, temperature))
                        print(f"   æ¸©åº¦ã‚’ {temperature} ã«è¨­å®š")
                    except:
                        print("   ã‚¨ãƒ©ãƒ¼: /temp <æ•°å€¤> (0.1-1.0)")
                    continue
                
                if user_input.startswith('/len '):
                    try:
                        max_length = int(user_input.split()[1])
                        max_length = max(10, min(500, max_length))
                        print(f"   ç”Ÿæˆé•·ã•ã‚’ {max_length} ã«è¨­å®š")
                    except:
                        print("   ã‚¨ãƒ©ãƒ¼: /len <æ•°å€¤>")
                    continue
                
                if user_input == '/info':
                    print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
                    print(f"   èªå½™ã‚µã‚¤ã‚º: {self.tokenizer.vocab_size}")
                    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {self.embed_dim}")
                    print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {self.hidden_dim}")
                    print(f"   QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {self.num_layers}")
                    print(f"   ã‚‚ã¤ã‚Œå¼·åº¦ Î»: {self.lambda_val}")
                    print(f"\nâš›ï¸ é‡å­ã‚‚ã¤ã‚Œæƒ…å ±:")
                    for info in self.model.get_entanglement_info():
                        print(f"   {info}")
                    continue
                
                # ç”Ÿæˆ
                print(f"\nğŸ¤– QBNN: ", end="", flush=True)
                response = self.generate(
                    prompt=user_input,
                    max_length=max_length,
                    temperature=temperature
                )
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
                if response.startswith(user_input):
                    response = response[len(user_input):]
                
                print(response)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                break
            except Exception as e:
                print(f"   ã‚¨ãƒ©ãƒ¼: {e}")


# ========================================
# ãƒ‡ãƒ¢ç”¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
# ========================================

def get_demo_texts():
    """ãƒ‡ãƒ¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"""
    return [
        # æ—¥æœ¬èª
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦è¨ˆç®—ã‚’è¡Œã†ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚",
        "é‡å­ãƒ“ãƒƒãƒˆã¯ã€0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æŒã¤ã“ã¨ãŒã§ãã¾ã™ã€‚",
        "é‡å­ã‚‚ã¤ã‚Œã¯ã€äºŒã¤ã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ã¦ã„ã‚‹çŠ¶æ…‹ã§ã™ã€‚",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€è„³ã®ç¥çµŒç´°èƒã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ãŸæ©Ÿæ¢°å­¦ç¿’ã§ã™ã€‚",
        "äººå·¥çŸ¥èƒ½ã¯ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚",
        "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€äººé–“ã®è¨€èªã‚’ç†è§£ã—ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€æ³¨æ„æ©Ÿæ§‹ã‚’ä½¿ã£ãŸæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚",
        "ç”ŸæˆAIã¯ã€æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆã™ã‚‹äººå·¥çŸ¥èƒ½ã§ã™ã€‚",
        "QBNNã¯ã€é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç•¥ç§°ã§ã™ã€‚",
        "APQBã¯ã€èª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆã®ç•¥ç§°ã§ã™ã€‚",
        "ç›¸é–¢ä¿‚æ•°ã¯ã€äºŒã¤ã®å¤‰æ•°ã®é–¢ä¿‚ã®å¼·ã•ã‚’è¡¨ã™æŒ‡æ¨™ã§ã™ã€‚",
        "ãƒ–ãƒ­ãƒƒãƒ›çƒã¯ã€é‡å­ãƒ“ãƒƒãƒˆã®çŠ¶æ…‹ã‚’è¦–è¦šåŒ–ã™ã‚‹æ–¹æ³•ã§ã™ã€‚",
        "ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¯ã€é‡å­ã‚‚ã¤ã‚Œã¨ã‚‚å‘¼ã°ã‚Œã¾ã™ã€‚",
        
        # è‹±èª
        "Quantum computing uses quantum mechanics for computation.",
        "A qubit can exist in superposition of 0 and 1.",
        "Neural networks are inspired by biological neurons.",
        "Deep learning uses multiple layers of neural networks.",
        "Artificial intelligence aims to mimic human intelligence.",
        "Machine learning algorithms learn patterns from data.",
        "Natural language processing understands human language.",
        "Transformers use attention mechanisms for learning.",
        "Generative AI creates new content automatically.",
        "QBNN stands for Quantum-Bit Neural Network.",
    ]


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

if __name__ == "__main__":
    # ç”ŸæˆAIä½œæˆ
    ai = QBNNGenerativeAI(
        embed_dim=64,
        hidden_dim=128,
        num_layers=4,
        lambda_val=0.5,
        dropout=0.1
    )
    
    # ãƒ‡ãƒ¢ãƒ†ã‚­ã‚¹ãƒˆã§å­¦ç¿’
    texts = get_demo_texts()
    ai.train(texts, epochs=100, batch_size=8, lr=0.002, seq_len=32)
    
    # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    print("\n" + "=" * 50)
    print("ğŸ¨ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    prompts = [
        "é‡å­",
        "Neural",
        "æ·±å±¤",
        "AI",
        "QBNN"
    ]
    
    for prompt in prompts:
        print(f"\nğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'")
        result = ai.generate(prompt, max_length=50, temperature=0.8)
        print(f"   ç”Ÿæˆ: {result}")
    
    # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
    print("\n" + "=" * 50)
    print("ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n)")
    print("=" * 50)
    
    try:
        answer = input().strip().lower()
        if answer == 'y':
            ai.chat()
    except:
        pass
    
    print("\nâœ… å®Œäº†ï¼")

