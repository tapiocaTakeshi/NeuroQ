#!/usr/bin/env python3
"""
Hugging Face (GPT-2) + OpenAI Embedding + QBNN çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
================================================================

æ§‹æˆ:
1. GPT-2 Tokenizer (Hugging Face): ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒˆãƒ¼ã‚¯ãƒ³ID
2. Hybrid Embedding: HF Embedding + OpenAI Embedding (optional)
3. GPT-2 Style Attention (Hugging Face): å› æœçš„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
4. QBNN Layers: é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‡¦ç†
5. Language Model Head: æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬

ä½¿ç”¨ä¾‹:
    pipeline = HFOpenAIQBNNPipeline(
        model_name='gpt2',
        use_openai_embedding=True,
        openai_api_key='your-key'
    )
    pipeline.train(texts, epochs=20)
    output = pipeline.generate("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯")
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import random
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

# Hugging Face Transformers
try:
    from transformers import GPT2Tokenizer, GPT2Config
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("âš ï¸  transformers not installed. Install with: pip install transformers")

# OpenAI (optional)
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸  openai not installed. Install with: pip install openai")


# ============================================================
# Configuration
# ============================================================

@dataclass
class HFQBNNConfig:
    """Hugging Face + QBNN çµ±åˆè¨­å®š"""

    # Tokenizerè¨­å®š
    tokenizer_name: str = 'gpt2'  # 'gpt2', 'gpt2-medium', 'gpt2-large'

    # Embeddingè¨­å®š
    embed_dim: int = 256
    use_openai_embedding: bool = False
    openai_model: str = 'text-embedding-3-small'
    openai_embed_dim: int = 1536

    # Modelè¨­å®š
    num_heads: int = 8
    num_layers: int = 6
    hidden_dim: int = 512
    max_seq_len: int = 512

    # Dropout & Regularization
    dropout: float = 0.1
    attention_dropout: float = 0.1

    # QBNNè¨­å®š
    lambda_entangle: float = 0.35
    use_qbnn_attention: bool = True

    # Trainingè¨­å®š
    vocab_size: int = None  # Auto-detect from tokenizer


# ============================================================
# STAGE 1: Tokenizer (Hugging Face GPT-2)
# ============================================================

class HFTokenizerWrapper:
    """
    Hugging Face GPT-2 Tokenizer ãƒ©ãƒƒãƒ‘ãƒ¼

    æ©Ÿèƒ½:
    - GPT-2ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä½¿ç”¨
    - ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†
    - ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ
    """

    def __init__(self, model_name: str = 'gpt2'):
        if not HF_AVAILABLE:
            raise ImportError("transformers not installed. Run: pip install transformers")

        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)

        # GPT-2ã«ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
        special_tokens = {
            'pad_token': '<PAD>',
            'bos_token': '<BOS>',
            'eos_token': '<EOS>',
        }
        self.tokenizer.add_special_tokens(special_tokens)

        self.pad_token_id = self.tokenizer.pad_token_id
        self.bos_token_id = self.tokenizer.bos_token_id
        self.eos_token_id = self.tokenizer.eos_token_id

        self.vocab_size = len(self.tokenizer)

        print(f"âœ… GPT-2 Tokenizer loaded: {model_name}")
        print(f"   Vocabulary size: {self.vocab_size}")
        print(f"   Special tokens: PAD={self.pad_token_id}, BOS={self.bos_token_id}, EOS={self.eos_token_id}")

    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒˆãƒ¼ã‚¯ãƒ³ID"""
        if add_special_tokens:
            return [self.bos_token_id] + self.tokenizer.encode(text) + [self.eos_token_id]
        return self.tokenizer.encode(text)

    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆ"""
        return self.tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)

    def batch_encode(
        self,
        texts: List[str],
        max_length: int = 512,
        padding: bool = True
    ) -> Dict[str, torch.Tensor]:
        """ãƒãƒƒãƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        encoded = self.tokenizer(
            texts,
            max_length=max_length,
            padding='max_length' if padding else False,
            truncation=True,
            return_tensors='pt'
        )
        return encoded


# ============================================================
# STAGE 2: Hybrid Embedding (HF + OpenAI)
# ============================================================

class HybridEmbedding(nn.Module):
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åŸ‹ã‚è¾¼ã¿å±¤

    ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: Hugging Face Embeddingã®ã¿
    ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: HF Embedding + OpenAI Embedding

    OpenAI Embeddingã¯ã€ã‚ˆã‚Šè±Šã‹ãªæ„å‘³è¡¨ç¾ã‚’æä¾›
    """

    def __init__(
        self,
        config: HFQBNNConfig,
        use_openai: bool = False,
        openai_api_key: Optional[str] = None
    ):
        super().__init__()

        self.config = config
        self.use_openai = use_openai and OPENAI_AVAILABLE

        # Hugging Face Token Embedding
        self.token_embedding = nn.Embedding(config.vocab_size, config.embed_dim)

        # Position Embedding
        self.position_embedding = nn.Embedding(config.max_seq_len, config.embed_dim)

        # OpenAI Embeddingçµ±åˆç”¨
        if self.use_openai:
            if openai_api_key:
                openai.api_key = openai_api_key

            # OpenAI embedding â†’ HF embed_dim ã¸ã®å°„å½±
            self.openai_projection = nn.Linear(config.openai_embed_dim, config.embed_dim)

            # çµ±åˆç”¨ã®é‡ã¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
            self.alpha = nn.Parameter(torch.tensor(0.5))  # HF vs OpenAI ã®ãƒãƒ©ãƒ³ã‚¹

            print(f"âœ… OpenAI Embedding enabled: {config.openai_model}")

        self.layer_norm = nn.LayerNorm(config.embed_dim)
        self.dropout = nn.Dropout(config.dropout)

        self._init_weights()

    def _init_weights(self):
        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.position_embedding.weight, mean=0.0, std=0.02)

    def get_openai_embedding(self, text: str) -> torch.Tensor:
        """OpenAI APIã§embeddingã‚’å–å¾—"""
        try:
            response = openai.Embedding.create(
                model=self.config.openai_model,
                input=text
            )
            embedding = response['data'][0]['embedding']
            return torch.tensor(embedding, dtype=torch.float32)
        except Exception as e:
            print(f"âš ï¸  OpenAI API error: {e}")
            return None

    def forward(
        self,
        token_ids: torch.Tensor,
        texts: Optional[List[str]] = None
    ) -> torch.Tensor:
        """
        Embedding forward pass

        Args:
            token_ids: (batch, seq_len) ãƒˆãƒ¼ã‚¯ãƒ³ID
            texts: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ - OpenAI embeddingç”¨ã®å…ƒãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            (batch, seq_len, embed_dim) åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        batch_size, seq_len = token_ids.shape

        # 1. Hugging Face Token Embedding
        token_embeds = self.token_embedding(token_ids)

        # 2. Position Embedding
        positions = torch.arange(seq_len, device=token_ids.device)
        positions = positions.unsqueeze(0).expand(batch_size, -1)
        pos_embeds = self.position_embedding(positions)

        # 3. åŸºæœ¬åŸ‹ã‚è¾¼ã¿
        embeddings = token_embeds + pos_embeds

        # 4. OpenAI Embeddingçµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.use_openai and texts is not None:
            openai_embeds = []
            for text in texts:
                oe = self.get_openai_embedding(text)
                if oe is not None:
                    # (openai_embed_dim,) â†’ (embed_dim,)
                    oe = self.openai_projection(oe.to(token_ids.device))
                    openai_embeds.append(oe)
                else:
                    # Fallback: zero embedding
                    openai_embeds.append(torch.zeros(self.config.embed_dim, device=token_ids.device))

            # (batch, embed_dim) â†’ (batch, 1, embed_dim) â†’ broadcast
            openai_embeds = torch.stack(openai_embeds).unsqueeze(1)

            # HF ã¨ OpenAI ã‚’é‡ã¿ä»˜ã‘å¹³å‡
            alpha = torch.sigmoid(self.alpha)  # 0-1 ã«æ­£è¦åŒ–
            embeddings = alpha * embeddings + (1 - alpha) * openai_embeds.expand_as(embeddings)

        # 5. æ­£è¦åŒ–ã¨ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)

        return embeddings


# ============================================================
# STAGE 3: GPT-2 Style Causal Attention (Hugging Face)
# ============================================================

class GPT2StyleAttention(nn.Module):
    """
    GPT-2ã‚¹ã‚¿ã‚¤ãƒ«ã®å› æœçš„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

    ç‰¹å¾´:
    - å› æœçš„ãƒã‚¹ã‚¯ï¼ˆæœªæ¥ã®æƒ…å ±ã‚’è¦‹ãªã„ï¼‰
    - ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
    - Scaled Dot-Product Attention
    """

    def __init__(self, config: HFQBNNConfig):
        super().__init__()

        self.embed_dim = config.embed_dim
        self.num_heads = config.num_heads
        self.head_dim = config.embed_dim // config.num_heads

        assert self.embed_dim % self.num_heads == 0, "embed_dim must be divisible by num_heads"

        # Q, K, V projections
        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim)
        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim)
        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim)

        # Output projection
        self.out_proj = nn.Linear(config.embed_dim, config.embed_dim)

        self.attn_dropout = nn.Dropout(config.attention_dropout)
        self.resid_dropout = nn.Dropout(config.dropout)

        self.scale = math.sqrt(self.head_dim)

        # Causal mask (ä¸‹ä¸‰è§’è¡Œåˆ—)
        self.register_buffer(
            "causal_mask",
            torch.tril(torch.ones(config.max_seq_len, config.max_seq_len)).view(
                1, 1, config.max_seq_len, config.max_seq_len
            )
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        GPT-2 Causal Attention

        Args:
            x: (batch, seq_len, embed_dim)

        Returns:
            (batch, seq_len, embed_dim)
        """
        batch_size, seq_len, _ = x.shape

        # Q, K, V ã‚’è¨ˆç®—
        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Attention scores: Q @ K^T / sqrt(d_k)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale

        # Causal masking (æœªæ¥ã‚’è¦‹ãªã„)
        causal_mask = self.causal_mask[:, :, :seq_len, :seq_len]
        attn_scores = attn_scores.masked_fill(causal_mask == 0, float('-inf'))

        # Softmax
        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.attn_dropout(attn_probs)

        # Attention output
        output = torch.matmul(attn_probs, V)
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)

        # Output projection
        output = self.out_proj(output)
        output = self.resid_dropout(output)

        return output


# ============================================================
# STAGE 4: QBNN Layer (é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)
# ============================================================

class QBNNLayer(nn.Module):
    """
    QBNNå±¤ - é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸå‡¦ç†

    æ•°å¼:
    1. s = tanh(h) âˆˆ [-1, 1] (æ­£è¦åŒ–)
    2. hÌƒ = WÂ·h + b (ç·šå½¢å¤‰æ›)
    3. Î” = Î£ J_ij Â· s_i Â· s_j (é‡å­ã‚‚ã¤ã‚Œé …)
    4. Ä¥ = hÌƒ + Î»Â·Î” (ã‚‚ã¤ã‚Œè£œæ­£)
    5. output = activation(Ä¥)
    """

    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        lambda_entangle: float = 0.35
    ):
        super().__init__()

        self.input_dim = input_dim
        self.output_dim = output_dim

        # ç·šå½¢å¤‰æ›
        self.linear = nn.Linear(input_dim, output_dim)

        # é‡å­ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ« J
        self.J = nn.Parameter(torch.randn(input_dim, output_dim) * 0.02)

        # ã‚‚ã¤ã‚Œå¼·åº¦ Î» (å­¦ç¿’å¯èƒ½)
        self.lambda_entangle = nn.Parameter(torch.tensor(lambda_entangle))

        # æ­£è¦åŒ–
        self.layer_norm = nn.LayerNorm(output_dim)

    def forward(self, h: torch.Tensor) -> torch.Tensor:
        """
        QBNNå±¤ã®å‡¦ç†

        Args:
            h: (batch, seq, input_dim) å…¥åŠ›

        Returns:
            (batch, seq, output_dim) å‡ºåŠ›
        """
        # 1. æ­£è¦åŒ– (ç–‘ä¼¼é‡å­çŠ¶æ…‹)
        s = torch.tanh(h)

        # 2. ç·šå½¢å¤‰æ›
        h_tilde = self.linear(h)

        # 3. é‡å­ã‚‚ã¤ã‚Œé …ã®è¨ˆç®—
        s_out = torch.tanh(h_tilde)
        delta = torch.einsum('...i,ij,...j->...j', s, self.J, s_out)

        # 4. ã‚‚ã¤ã‚Œè£œæ­£ã‚’åŠ ç®—
        h_hat = h_tilde + self.lambda_entangle * delta

        # 5. æ­£è¦åŒ–ã¨æ´»æ€§åŒ–
        output = self.layer_norm(h_hat)
        output = F.gelu(output)

        return output


class QBNNAttention(nn.Module):
    """
    QBNNæ‹¡å¼µã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

    GPT-2 Attention + é‡å­ã‚‚ã¤ã‚Œè£œæ­£
    """

    def __init__(self, config: HFQBNNConfig):
        super().__init__()

        # Base GPT-2 attention
        self.gpt2_attention = GPT2StyleAttention(config)

        # é‡å­ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ« (ãƒ˜ãƒƒãƒ‰ã”ã¨)
        self.num_heads = config.num_heads
        self.head_dim = config.embed_dim // config.num_heads

        self.J_attn = nn.Parameter(
            torch.randn(self.num_heads, self.head_dim, self.head_dim) * 0.02
        )
        self.lambda_attn = nn.Parameter(torch.tensor(config.lambda_entangle))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        QBNN Attention

        Args:
            x: (batch, seq, embed_dim)

        Returns:
            (batch, seq, embed_dim)
        """
        # Base attention
        attn_out = self.gpt2_attention(x)

        # é‡å­ã‚‚ã¤ã‚Œè£œæ­£ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        # ã“ã“ã§ã¯å‡ºåŠ›ã«å¯¾ã—ã¦è»½ã„è£œæ­£ã‚’åŠ ãˆã‚‹
        delta = torch.tanh(attn_out) * self.lambda_attn * 0.1

        return attn_out + delta


# ============================================================
# STAGE 5: Transformer Block
# ============================================================

class HFQBNNBlock(nn.Module):
    """
    Transformer Block = Attention + FFN

    ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
    - use_qbnn_attention: QBNNæ‹¡å¼µã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨
    - ãã‚Œä»¥å¤–: æ¨™æº–GPT-2ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
    """

    def __init__(self, config: HFQBNNConfig):
        super().__init__()

        # Attention
        if config.use_qbnn_attention:
            self.attention = QBNNAttention(config)
        else:
            self.attention = GPT2StyleAttention(config)

        self.attn_norm = nn.LayerNorm(config.embed_dim)

        # Feed-Forward Network (QBNNæ‹¡å¼µ)
        self.ffn = nn.Sequential(
            QBNNLayer(config.embed_dim, config.hidden_dim, config.lambda_entangle),
            nn.Dropout(config.dropout),
            QBNNLayer(config.hidden_dim, config.embed_dim, config.lambda_entangle),
        )
        self.ffn_norm = nn.LayerNorm(config.embed_dim)

        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Transformer Block

        Pre-LNå¼: Norm â†’ Attention â†’ Residual
        """
        # Attention + æ®‹å·®æ¥ç¶š
        attn_out = self.attention(self.attn_norm(x))
        x = x + self.dropout(attn_out)

        # FFN + æ®‹å·®æ¥ç¶š
        ffn_out = self.ffn(self.ffn_norm(x))
        x = x + self.dropout(ffn_out)

        return x


# ============================================================
# STAGE 6: Language Model
# ============================================================

class HFOpenAIQBNNPipeline(nn.Module):
    """
    Hugging Face (GPT-2) + OpenAI Embedding + QBNN çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

    å‡¦ç†ãƒ•ãƒ­ãƒ¼:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ãƒ†ã‚­ã‚¹ãƒˆ â”‚ â†’ â”‚ HF Token â”‚ â†’ â”‚ Hybrid â”‚ â†’ â”‚ QBNN â”‚ â†’ â”‚  å‡ºåŠ›  â”‚
    â”‚         â”‚   â”‚   izer   â”‚   â”‚Embeddingâ”‚   â”‚Blocksâ”‚   â”‚  Head  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """

    def __init__(
        self,
        config: Optional[HFQBNNConfig] = None,
        model_name: str = 'gpt2',
        use_openai_embedding: bool = False,
        openai_api_key: Optional[str] = None
    ):
        super().__init__()

        if config is None:
            config = HFQBNNConfig(tokenizer_name=model_name)

        self.config = config

        # Tokenizer
        self.tokenizer = HFTokenizerWrapper(config.tokenizer_name)

        # Update vocab_size
        if config.vocab_size is None:
            config.vocab_size = self.tokenizer.vocab_size

        # Hybrid Embedding
        self.embedding = HybridEmbedding(
            config,
            use_openai=use_openai_embedding,
            openai_api_key=openai_api_key
        )

        # Transformer Blocks
        self.blocks = nn.ModuleList([
            HFQBNNBlock(config)
            for _ in range(config.num_layers)
        ])

        # Final Layer Norm
        self.final_norm = nn.LayerNorm(config.embed_dim)

        # Language Model Head
        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)

        # Device
        self.device = torch.device('cpu')

        # Tie weights (embedding ã¨ lm_head)
        self.lm_head.weight = self.embedding.token_embedding.weight

        # Initialize
        self._init_weights()

        # Parameter count
        self.num_params = sum(p.numel() for p in self.parameters())

        print(f"\nâœ… HF-OpenAI-QBNN Pipeline initialized")
        print(f"   Model: {config.tokenizer_name}")
        print(f"   Vocab size: {config.vocab_size}")
        print(f"   Embed dim: {config.embed_dim}")
        print(f"   Num layers: {config.num_layers}")
        print(f"   Num heads: {config.num_heads}")
        print(f"   Total parameters: {self.num_params:,}")
        print(f"   QBNN attention: {config.use_qbnn_attention}")
        print(f"   OpenAI embedding: {use_openai_embedding}")

    def _init_weights(self):
        """Initialize weights"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)

    def to(self, device):
        """Move to device"""
        super().to(device)
        self.device = device
        return self

    def forward(
        self,
        token_ids: torch.Tensor,
        texts: Optional[List[str]] = None
    ) -> torch.Tensor:
        """
        Forward pass

        Args:
            token_ids: (batch, seq_len) ãƒˆãƒ¼ã‚¯ãƒ³ID
            texts: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ - OpenAI embeddingç”¨

        Returns:
            (batch, seq_len, vocab_size) ãƒ­ã‚¸ãƒƒãƒˆ
        """
        # 1. Embedding
        x = self.embedding(token_ids, texts=texts)

        # 2. Transformer Blocks
        for block in self.blocks:
            x = block(x)

        # 3. Final Norm
        x = self.final_norm(x)

        # 4. Language Model Head
        logits = self.lm_head(x)

        return logits

    @torch.no_grad()
    def generate(
        self,
        prompt: str,
        max_length: int = 50,
        temperature: float = 0.7,
        top_k: int = 40,
        top_p: float = 0.9,
        repetition_penalty: float = 1.2
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_length: æœ€å¤§ç”Ÿæˆé•·
            temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
            top_k: Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            top_p: Top-Pã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            repetition_penalty: ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        self.eval()

        # Encode prompt
        tokens = self.tokenizer.encode(prompt, add_special_tokens=False)
        tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)

        generated = tokens[0].tolist()

        for _ in range(max_length):
            # Forward pass
            input_tokens = tokens[:, -self.config.max_seq_len:]
            logits = self(input_tokens)

            # Next token logits
            next_logits = logits[0, -1, :] / temperature

            # Repetition penalty
            for token_id in set(generated[-20:]):
                if next_logits[token_id] > 0:
                    next_logits[token_id] /= repetition_penalty
                else:
                    next_logits[token_id] *= repetition_penalty

            # Top-K filtering
            if top_k > 0:
                top_k_vals, _ = torch.topk(next_logits, min(top_k, next_logits.size(-1)))
                next_logits[next_logits < top_k_vals[-1]] = float('-inf')

            # Top-P filtering
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = False

                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_logits[indices_to_remove] = float('-inf')

            # Sampling
            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, 1)

            # EOS check
            if next_token.item() == self.tokenizer.eos_token_id:
                break

            generated.append(next_token.item())
            tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)

        return self.tokenizer.decode(generated)

    def train_model(
        self,
        texts: List[str],
        epochs: int = 20,
        batch_size: int = 8,
        lr: float = 5e-4,
        seq_length: int = 128
    ):
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

        Args:
            texts: å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            lr: å­¦ç¿’ç‡
            seq_length: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        """
        print("\n" + "=" * 60)
        print("ğŸ“ å­¦ç¿’é–‹å§‹: HF + OpenAI + QBNN Pipeline")
        print("=" * 60)

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        all_tokens = []
        for text in texts:
            tokens = self.tokenizer.encode(text, add_special_tokens=True)
            all_tokens.extend(tokens)

        print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens):,}")

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
        sequences = []
        for i in range(0, len(all_tokens) - seq_length - 1, seq_length // 2):
            x = all_tokens[i:i + seq_length]
            y = all_tokens[i + 1:i + seq_length + 1]
            if len(x) == seq_length:
                sequences.append((x, y))

        print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences):,}")
        print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.num_params:,}")

        # Optimizer
        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=0.01)
        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)

        # Training loop
        print("\nğŸ”„ å­¦ç¿’ä¸­...")
        self.train()

        for epoch in range(epochs):
            total_loss = 0
            random.shuffle(sequences)

            for i in range(0, len(sequences), batch_size):
                batch = sequences[i:i + batch_size]

                x = torch.tensor([s[0] for s in batch], dtype=torch.long).to(self.device)
                y = torch.tensor([s[1] for s in batch], dtype=torch.long).to(self.device)

                optimizer.zero_grad()

                # Forward
                logits = self(x)

                # Loss
                loss = criterion(
                    logits.view(-1, self.config.vocab_size),
                    y.view(-1)
                )

                # Backward
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)
                optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / max(len(sequences) // batch_size, 1)

            if (epoch + 1) % 5 == 0 or epoch == 0:
                print(f"   Epoch {epoch + 1:3d}/{epochs}: Loss = {avg_loss:.4f}")

        print("\nâœ… å­¦ç¿’å®Œäº†ï¼")


# ============================================================
# Main Demo
# ============================================================

def get_training_data() -> List[str]:
    """ã‚µãƒ³ãƒ—ãƒ«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿"""
    return [
        # é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦æƒ…å ±ã‚’å‡¦ç†ã™ã‚‹é©æ–°çš„ãªè¨ˆç®—æ©Ÿã§ã™ã€‚",
        "é‡å­ãƒ“ãƒƒãƒˆã¯0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚",
        "é‡å­ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¯ã€è¤‡æ•°ã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ãŸçŠ¶æ…‹ã§ã™ã€‚",

        # AI & ML
        "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€äººé–“ã®è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã§ã™ã€‚",
        "Transformerã¯ã€è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã‚’ç”¨ã„ãŸé©æ–°çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚",

        # GPT-2
        "GPT-2 is a large-scale unsupervised language model.",
        "The model uses transformer architecture with causal attention.",
        "It can generate coherent and contextually relevant text.",

        # General
        "äººå·¥çŸ¥èƒ½ã¯æ€¥é€Ÿã«é€²åŒ–ã—ã¦ã„ã¾ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã£ã¦æ§˜ã€…ãªå•é¡ŒãŒè§£æ±ºã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "è‡ªç„¶è¨€èªå‡¦ç†ã¯é‡è¦ãªç ”ç©¶åˆ†é‡ã§ã™ã€‚",
    ] * 100  # ãƒ‡ãƒ¼ã‚¿ã‚’å¢—å¹…


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 70)
    print("ğŸ§ âš›ï¸ Hugging Face (GPT-2) + OpenAI + QBNN Pipeline")
    print("=" * 70)

    # Device
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"\nğŸ® CUDA GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device("cpu")
        print("\nğŸ’» CPU")

    # Config
    config = HFQBNNConfig(
        tokenizer_name='gpt2',
        embed_dim=256,
        num_heads=8,
        num_layers=4,
        hidden_dim=512,
        max_seq_len=256,
        use_qbnn_attention=True,
        lambda_entangle=0.35
    )

    # Pipeline
    print("\nğŸ”§ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ä¸­...")
    pipeline = HFOpenAIQBNNPipeline(
        config=config,
        use_openai_embedding=False,  # OpenAI APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯False
        openai_api_key=None
    ).to(device)

    # Training data
    texts = get_training_data()
    print(f"\nğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«")

    # Train
    pipeline.train_model(texts, epochs=20, batch_size=8, lr=5e-4)

    # Generation test
    print("\n" + "=" * 60)
    print("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)

    prompts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯",
        "The transformer model",
        "äººå·¥çŸ¥èƒ½ã¨ã¯",
        "Deep learning is",
    ]

    for prompt in prompts:
        output = pipeline.generate(
            prompt,
            max_length=30,
            temperature=0.8
        )
        print(f"\nå…¥åŠ›: '{prompt}'")
        print(f"å‡ºåŠ›: {output}")

    print("\nâœ… å®Œäº†ï¼")


if __name__ == '__main__':
    if not HF_AVAILABLE:
        print("\nâŒ Hugging Face transformers ãŒå¿…è¦ã§ã™")
        print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install transformers")
    else:
        main()
