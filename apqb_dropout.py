#!/usr/bin/env python3
"""
APQB Dropout - Adjustable Pseudo Quantum Bit ãƒ™ãƒ¼ã‚¹ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤

å¾“æ¥ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ: å›ºå®šç¢ºç‡ã§ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ç„¡åŠ¹åŒ–
APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ: é‡å­ç¢ºç‡åˆ†å¸ƒã«åŸºã¥ã„ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ãƒ‰ãƒ­ãƒƒãƒ—

ç›¸é–¢ä¿‚æ•° r ã§ç¢ºç‡åˆ†å¸ƒã‚’åˆ¶å¾¡:
  r = 1  â†’ ã»ã¼ãƒ‰ãƒ­ãƒƒãƒ—ãªã—ï¼ˆã™ã¹ã¦ä¿æŒï¼‰
  r = -1 â†’ ã»ã¼å…¨ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆã™ã¹ã¦ç„¡åŠ¹ï¼‰
  r = 0  â†’ 50%ã®ç¢ºç‡ã§ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆé€šå¸¸ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç›¸å½“ï¼‰
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

# ========== APQB (Adjustable Pseudo Quantum Bit) ==========
class APQB:
    """èª¿æ•´å¯èƒ½ãªæ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆ"""
    
    def __init__(self, r: float = 0.0):
        """
        Args:
            r: ç›¸é–¢ä¿‚æ•° (-1 to 1)
               r = 1  â†’ |0âŸ© (ç¢ºå®Ÿã«0 = ãƒ‰ãƒ­ãƒƒãƒ—)
               r = -1 â†’ |1âŸ© (ç¢ºå®Ÿã«1 = ä¿æŒ)
               r = 0  â†’ ç­‰ç¢ºç‡ã®é‡ã­åˆã‚ã›
        """
        self.r = np.clip(r, -1, 1)
    
    @property
    def theta(self) -> float:
        """ç›¸é–¢ä¿‚æ•°ã‹ã‚‰è§’åº¦ã‚’è¨ˆç®—"""
        return np.pi * (1 - self.r) / 2
    
    @property
    def p_keep(self) -> float:
        """ä¿æŒç¢ºç‡ (|1âŸ©ã®ç¢ºç‡)"""
        return np.sin(self.theta / 2) ** 2
    
    @property
    def p_drop(self) -> float:
        """ãƒ‰ãƒ­ãƒƒãƒ—ç¢ºç‡ (|0âŸ©ã®ç¢ºç‡)"""
        return np.cos(self.theta / 2) ** 2
    
    def measure(self) -> int:
        """é‡å­æ¸¬å®š: 1=ä¿æŒ, 0=ãƒ‰ãƒ­ãƒƒãƒ—"""
        return 1 if np.random.random() < self.p_keep else 0


# ========== APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ (PyTorch) ==========
class APQBDropout(nn.Module):
    """
    APQBãƒ™ãƒ¼ã‚¹ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤
    
    å¾“æ¥ã®Dropoutã¨ã®é•ã„:
    - ç›¸é–¢ä¿‚æ•° r ã§ãƒ‰ãƒ­ãƒƒãƒ—ç¢ºç‡ã‚’åˆ¶å¾¡
    - é‡å­çš„ãªç¢ºç‡åˆ†å¸ƒã‚’ä½¿ç”¨
    - å‹•çš„ã« r ã‚’å¤‰æ›´å¯èƒ½
    """
    
    def __init__(self, r: float = 0.0, learnable: bool = False):
        """
        Args:
            r: åˆæœŸç›¸é–¢ä¿‚æ•° (-1 to 1)
               r = 0 ã¯å¾“æ¥ã® dropout(p=0.5) ã«ç›¸å½“
            learnable: Trueã®å ´åˆã€rã‚’å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã™ã‚‹
        """
        super().__init__()
        
        if learnable:
            # å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ r ã‚’å®šç¾©
            self._r = nn.Parameter(torch.tensor(float(r)))
        else:
            self.register_buffer('_r', torch.tensor(float(r)))
        
        self.learnable = learnable
    
    @property
    def r(self) -> float:
        """ç¾åœ¨ã®ç›¸é–¢ä¿‚æ•°"""
        return float(self._r.clamp(-1, 1))
    
    @r.setter
    def r(self, value: float):
        """ç›¸é–¢ä¿‚æ•°ã‚’è¨­å®š"""
        with torch.no_grad():
            self._r.fill_(np.clip(value, -1, 1))
    
    def get_keep_probability(self) -> float:
        """ç¾åœ¨ã®ä¿æŒç¢ºç‡ã‚’å–å¾—"""
        r = self.r
        theta = np.pi * (1 - r) / 2
        return np.sin(theta / 2) ** 2
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        é †ä¼æ’­
        
        Args:
            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«
        
        Returns:
            APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’é©ç”¨ã—ãŸãƒ†ãƒ³ã‚½ãƒ«
        """
        if not self.training:
            return x
        
        # ç›¸é–¢ä¿‚æ•°ã‹ã‚‰ä¿æŒç¢ºç‡ã‚’è¨ˆç®—
        r = self._r.clamp(-1, 1)
        theta = np.pi * (1 - r.item()) / 2
        p_keep = np.sin(theta / 2) ** 2
        
        # APQBãƒ™ãƒ¼ã‚¹ã®ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
        # å„è¦ç´ ã«ã¤ã„ã¦ç‹¬ç«‹ã«APQBæ¸¬å®šã‚’è¡Œã†
        mask = self._generate_apqb_mask(x.shape, p_keep, x.device)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã¨åŒæ§˜ï¼‰
        if p_keep > 0:
            return x * mask / p_keep
        else:
            return x * 0
    
    def _generate_apqb_mask(self, shape, p_keep, device):
        """APQBãƒ™ãƒ¼ã‚¹ã®ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ"""
        # é‡å­çš„ãªãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æ¨¡å€£
        # è¤‡æ•°ã®APQBæ¸¬å®šã‚’çµ„ã¿åˆã‚ã›ã¦ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ
        
        # åŸºæœ¬çš„ãªä¸€æ§˜ä¹±æ•°
        rand = torch.rand(shape, device=device)
        
        # APQBã®é‡å­ã‚†ã‚‰ãã‚’è¿½åŠ 
        # è¤‡æ•°ã®ã€Œé‡å­ãƒ“ãƒƒãƒˆã€ã‹ã‚‰ã®å¹²æ¸‰åŠ¹æœã‚’æ¨¡å€£
        quantum_noise = torch.zeros(shape, device=device)
        for i in range(4):  # 4ã¤ã®APQBã‹ã‚‰ã®å¹²æ¸‰
            phase = torch.rand(shape, device=device) * 2 * np.pi
            quantum_noise += torch.sin(phase) * 0.1
        
        # å¹²æ¸‰ã‚’åŠ ãˆãŸä¹±æ•°ã§ãƒã‚¹ã‚¯ã‚’æ±ºå®š
        adjusted_rand = rand + quantum_noise
        mask = (adjusted_rand < p_keep).float()
        
        return mask
    
    def extra_repr(self) -> str:
        return f'r={self.r:.3f}, p_keep={self.get_keep_probability():.3f}, learnable={self.learnable}'


# ========== APQBã‚’ä½¿ã£ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ==========
class APQBNeuralNet(nn.Module):
    """APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self, input_size: int, hidden_sizes: list, output_size: int,
                 dropout_r: float = 0.0, learnable_dropout: bool = False):
        """
        Args:
            input_size: å…¥åŠ›æ¬¡å…ƒ
            hidden_sizes: éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºã®ãƒªã‚¹ãƒˆ
            output_size: å‡ºåŠ›æ¬¡å…ƒ
            dropout_r: APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®ç›¸é–¢ä¿‚æ•°
            learnable_dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ã‚’å­¦ç¿’å¯èƒ½ã«ã™ã‚‹ã‹
        """
        super().__init__()
        
        self.layers = nn.ModuleList()
        self.dropouts = nn.ModuleList()
        
        # éš ã‚Œå±¤ã‚’æ§‹ç¯‰
        prev_size = input_size
        for hidden_size in hidden_sizes:
            self.layers.append(nn.Linear(prev_size, hidden_size))
            self.dropouts.append(APQBDropout(r=dropout_r, learnable=learnable_dropout))
            prev_size = hidden_size
        
        # å‡ºåŠ›å±¤
        self.output_layer = nn.Linear(prev_size, output_size)
        
        # æ´»æ€§åŒ–é–¢æ•°
        self.activation = nn.ReLU()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for layer, dropout in zip(self.layers, self.dropouts):
            x = layer(x)
            x = self.activation(x)
            x = dropout(x)
        
        x = self.output_layer(x)
        return x
    
    def set_dropout_r(self, r: float):
        """å…¨APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã®ç›¸é–¢ä¿‚æ•°ã‚’è¨­å®š"""
        for dropout in self.dropouts:
            dropout.r = r
    
    def get_dropout_stats(self) -> dict:
        """ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã®çµ±è¨ˆã‚’å–å¾—"""
        stats = []
        for i, dropout in enumerate(self.dropouts):
            stats.append({
                'layer': i,
                'r': dropout.r,
                'p_keep': dropout.get_keep_probability()
            })
        return stats


# ========== å¾“æ¥ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã¨ã®æ¯”è¼ƒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ==========
class StandardNeuralNet(nn.Module):
    """å¾“æ¥ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self, input_size: int, hidden_sizes: list, output_size: int,
                 dropout_p: float = 0.5):
        super().__init__()
        
        self.layers = nn.ModuleList()
        self.dropouts = nn.ModuleList()
        
        prev_size = input_size
        for hidden_size in hidden_sizes:
            self.layers.append(nn.Linear(prev_size, hidden_size))
            self.dropouts.append(nn.Dropout(p=dropout_p))
            prev_size = hidden_size
        
        self.output_layer = nn.Linear(prev_size, output_size)
        self.activation = nn.ReLU()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for layer, dropout in zip(self.layers, self.dropouts):
            x = layer(x)
            x = self.activation(x)
            x = dropout(x)
        
        x = self.output_layer(x)
        return x


# ========== ãƒ‡ãƒ¢: XORå•é¡Œã§ã®æ¯”è¼ƒ ==========
def demo_xor():
    """XORå•é¡Œã§APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã¨å¾“æ¥ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’æ¯”è¼ƒ"""
    print("=" * 60)
    print("ğŸ§ âš›ï¸ APQB Dropout ãƒ‡ãƒ¢ - XORå•é¡Œ")
    print("=" * 60)
    
    # XORãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)
    y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å¼µï¼ˆãƒã‚¤ã‚ºã‚’è¿½åŠ ï¼‰
    X_train = X.repeat(100, 1) + torch.randn(400, 2) * 0.1
    y_train = y.repeat(100, 1)
    
    dataset = TensorDataset(X_train, y_train)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ
    results = {}
    
    # 1. APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ (r=0, 50%ãƒ‰ãƒ­ãƒƒãƒ—ç›¸å½“)
    print("\nğŸ“Š APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ (r=0.0)")
    apqb_model = APQBNeuralNet(2, [16, 16], 1, dropout_r=0.0)
    apqb_losses = train_model(apqb_model, dataloader, epochs=200)
    apqb_acc = evaluate_xor(apqb_model, X, y)
    results['APQB (r=0.0)'] = {'losses': apqb_losses, 'accuracy': apqb_acc}
    
    # 2. APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ (r=0.5, ä½ãƒ‰ãƒ­ãƒƒãƒ—ç‡)
    print("\nğŸ“Š APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ (r=0.5)")
    apqb_model2 = APQBNeuralNet(2, [16, 16], 1, dropout_r=0.5)
    apqb_losses2 = train_model(apqb_model2, dataloader, epochs=200)
    apqb_acc2 = evaluate_xor(apqb_model2, X, y)
    results['APQB (r=0.5)'] = {'losses': apqb_losses2, 'accuracy': apqb_acc2}
    
    # 3. APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ (r=-0.5, é«˜ãƒ‰ãƒ­ãƒƒãƒ—ç‡)
    print("\nğŸ“Š APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ (r=-0.5)")
    apqb_model3 = APQBNeuralNet(2, [16, 16], 1, dropout_r=-0.5)
    apqb_losses3 = train_model(apqb_model3, dataloader, epochs=200)
    apqb_acc3 = evaluate_xor(apqb_model3, X, y)
    results['APQB (r=-0.5)'] = {'losses': apqb_losses3, 'accuracy': apqb_acc3}
    
    # 4. å¾“æ¥ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
    print("\nğŸ“Š å¾“æ¥ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ (p=0.5)")
    std_model = StandardNeuralNet(2, [16, 16], 1, dropout_p=0.5)
    std_losses = train_model(std_model, dataloader, epochs=200)
    std_acc = evaluate_xor(std_model, X, y)
    results['Standard (p=0.5)'] = {'losses': std_losses, 'accuracy': std_acc}
    
    # 5. ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆãªã—
    print("\nğŸ“Š ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆãªã—")
    no_drop_model = APQBNeuralNet(2, [16, 16], 1, dropout_r=1.0)  # r=1ã§ã»ã¼ãƒ‰ãƒ­ãƒƒãƒ—ãªã—
    no_drop_losses = train_model(no_drop_model, dataloader, epochs=200)
    no_drop_acc = evaluate_xor(no_drop_model, X, y)
    results['No Dropout'] = {'losses': no_drop_losses, 'accuracy': no_drop_acc}
    
    # çµæœã‚’è¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“ˆ çµæœæ¯”è¼ƒ")
    print("=" * 60)
    for name, data in results.items():
        print(f"{name:20s}: ç²¾åº¦ = {data['accuracy']*100:.1f}%, æœ€çµ‚æå¤± = {data['losses'][-1]:.4f}")
    
    # ã‚°ãƒ©ãƒ•ã‚’æç”»
    plot_comparison(results)
    
    return results


def train_model(model, dataloader, epochs=100, lr=0.01):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCEWithLogitsLoss()
    
    losses = []
    model.train()
    
    for epoch in range(epochs):
        epoch_loss = 0
        for X_batch, y_batch in dataloader:
            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / len(dataloader)
        losses.append(avg_loss)
        
        if (epoch + 1) % 50 == 0:
            print(f"   Epoch {epoch+1}: Loss = {avg_loss:.4f}")
    
    return losses


def evaluate_xor(model, X, y):
    """XORå•é¡Œã®ç²¾åº¦ã‚’è©•ä¾¡"""
    model.eval()
    with torch.no_grad():
        output = model(X)
        predictions = (torch.sigmoid(output) > 0.5).float()
        accuracy = (predictions == y).float().mean().item()
    return accuracy


def plot_comparison(results):
    """çµæœã‚’å¯è¦–åŒ–"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # æå¤±æ›²ç·š
    ax1 = axes[0]
    colors = ['#00d4ff', '#ff00ff', '#00ff88', '#ffaa00', '#ff6666']
    for i, (name, data) in enumerate(results.items()):
        ax1.plot(data['losses'], label=name, color=colors[i % len(colors)], linewidth=2)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training Loss Comparison')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_facecolor('#1a1a2e')
    
    # ç²¾åº¦æ¯”è¼ƒ
    ax2 = axes[1]
    names = list(results.keys())
    accuracies = [results[name]['accuracy'] * 100 for name in names]
    bars = ax2.bar(range(len(names)), accuracies, color=colors[:len(names)])
    ax2.set_xticks(range(len(names)))
    ax2.set_xticklabels(names, rotation=45, ha='right')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('Final Accuracy Comparison')
    ax2.set_ylim(0, 105)
    ax2.grid(True, alpha=0.3, axis='y')
    ax2.set_facecolor('#1a1a2e')
    
    # ç²¾åº¦å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, acc in zip(bars, accuracies):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    fig.patch.set_facecolor('#0a0a1a')
    for ax in axes:
        ax.tick_params(colors='white')
        ax.xaxis.label.set_color('white')
        ax.yaxis.label.set_color('white')
        ax.title.set_color('white')
        for spine in ax.spines.values():
            spine.set_color('white')
    
    plt.tight_layout()
    plt.savefig('/Users/yuyahiguchi/Program/Qubit/apqb_dropout_comparison.png', 
                dpi=150, facecolor='#0a0a1a', edgecolor='none')
    print("\nğŸ“Š ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: apqb_dropout_comparison.png")
    plt.close()


# ========== APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®ç¢ºç‡åˆ†å¸ƒã‚’å¯è¦–åŒ– ==========
def visualize_apqb_dropout():
    """APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®ç¢ºç‡åˆ†å¸ƒã‚’å¯è¦–åŒ–"""
    print("\n" + "=" * 60)
    print("ğŸ“Š APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç¢ºç‡åˆ†å¸ƒ")
    print("=" * 60)
    
    r_values = np.linspace(-1, 1, 100)
    p_keeps = []
    
    for r in r_values:
        apqb = APQB(r)
        p_keeps.append(apqb.p_keep)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    ax.plot(r_values, p_keeps, 'c-', linewidth=3, label='P(keep) = sinÂ²(Ï€(1-r)/4)')
    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50% (æ¨™æº–ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ)')
    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)
    
    # ç‰¹å®šã®rå€¤ã‚’ãƒãƒ¼ã‚¯
    special_rs = [-1, -0.5, 0, 0.5, 1]
    for r in special_rs:
        apqb = APQB(r)
        ax.plot(r, apqb.p_keep, 'mo', markersize=10)
        ax.annotate(f'r={r}\nP={apqb.p_keep:.2f}', 
                   xy=(r, apqb.p_keep), xytext=(r+0.1, apqb.p_keep+0.1),
                   fontsize=9, color='white')
    
    ax.set_xlabel('ç›¸é–¢ä¿‚æ•° r', fontsize=12, color='white')
    ax.set_ylabel('ä¿æŒç¢ºç‡ P(keep)', fontsize=12, color='white')
    ax.set_title('APQB Dropout: ç›¸é–¢ä¿‚æ•°ã¨ä¿æŒç¢ºç‡ã®é–¢ä¿‚', fontsize=14, color='white')
    ax.legend(facecolor='#1a1a2e', edgecolor='white', labelcolor='white')
    ax.grid(True, alpha=0.3)
    ax.set_facecolor('#1a1a2e')
    ax.set_xlim(-1.1, 1.1)
    ax.set_ylim(-0.05, 1.05)
    
    ax.tick_params(colors='white')
    for spine in ax.spines.values():
        spine.set_color('white')
    
    fig.patch.set_facecolor('#0a0a1a')
    plt.tight_layout()
    plt.savefig('/Users/yuyahiguchi/Program/Qubit/apqb_dropout_probability.png',
                dpi=150, facecolor='#0a0a1a', edgecolor='none')
    print("ğŸ“Š ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: apqb_dropout_probability.png")
    plt.close()
    
    # ç¢ºç‡è¡¨ã‚’è¡¨ç¤º
    print("\nç›¸é–¢ä¿‚æ•° r ã¨ä¿æŒç¢ºç‡ P(keep) ã®å¯¾å¿œ:")
    print("-" * 40)
    for r in [-1.0, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1.0]:
        apqb = APQB(r)
        bar = 'â–ˆ' * int(apqb.p_keep * 20)
        print(f"r = {r:+.2f}: P(keep) = {apqb.p_keep:.3f} |{bar}")


# ========== ãƒ¡ã‚¤ãƒ³ ==========
if __name__ == "__main__":
    import sys
    
    print("ğŸ§ âš›ï¸ APQB Dropout - é‡å­ç¢ºç‡çš„ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤")
    print()
    
    # APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®ç¢ºç‡åˆ†å¸ƒã‚’å¯è¦–åŒ–
    visualize_apqb_dropout()
    
    # XORå•é¡Œã§ã®ãƒ‡ãƒ¢
    demo_xor()
    
    print("\nâœ… å®Œäº†ï¼")

