#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                        â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—                       â•‘
â•‘   â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘                       â•‘
â•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘                       â•‘
â•‘   â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•                       â•‘    
â•‘   â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•  â•šâ•â•â–€â–€â•â•                        â•‘
â•‘                                                                               â•‘                                                                 â•‘
â•‘                                                                               â•‘
â•‘   neuroQ: Quantum-Bit Neural Network Language Model                           â•‘
â•‘   ç‹¬è‡ªã®é‡å­ã‚‚ã¤ã‚Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ç”ŸæˆAI                                  â•‘
â•‘                                                                               â•‘
â•‘   å‚ç…§å…ƒ: qbnn_layered.py                                                      â•‘
â•‘   - APQB: èª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆ                                                  â•‘
â•‘   - EntanglementOperator: å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«æ¼”ç®—å­                                  â•‘
â•‘   - EQBNNLayer: å±¤çŠ¶QBNNå±¤                                                     â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import json
import os
from collections import Counter
from typing import List, Dict, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# SentencePieceï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ç”¨ï¼‰
try:
    import sentencepiece as spm
    SENTENCEPIECE_AVAILABLE = True
except ImportError:
    SENTENCEPIECE_AVAILABLE = False
    warnings.warn("sentencepieceãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install sentencepiece ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ========================================
# qbnn_layered.py ã‹ã‚‰ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ========================================
try:
    from qbnn_layered import (
        APQB as APQB_Core,                  # APQBç†è«–ã®ã‚³ã‚¢
        EntanglementOperator,               # å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«æ¼”ç®—å­
        QuantumCorrelationMatrix,           # é‡å­ç›¸é–¢è¡Œåˆ—
        EQBNNLayer,                         # E-QBNNå±¤
    )
    QBNN_LAYERED_AVAILABLE = True
    print("âœ… qbnn_layered.py ã‹ã‚‰ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError:
    QBNN_LAYERED_AVAILABLE = False
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãªã®ã§è­¦å‘Šã‚’è¡¨ç¤ºã—ãªã„ï¼ˆå†…è”µã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§å‹•ä½œã—ã¾ã™ï¼‰

# ========================================
# quantum_computer.py ã‹ã‚‰é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ========================================
try:
    # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ quantum_computer.py ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    import sys
    sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
    from quantum_computer import (
        QuantumComputer,
        QuantumCircuit,
        Gates,
        QubitState,
    )
    QUANTUM_COMPUTER_AVAILABLE = True
    print("âœ… quantum_computer.py ã‹ã‚‰é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError:
    QUANTUM_COMPUTER_AVAILABLE = False
    warnings.warn("quantum_computer.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")

# ========================================
# è¨­å®š
# ========================================

class NeuroQuantumConfig:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ­Qè¨­å®š"""
    def __init__(
        self,
        vocab_size: int = 8000,
        embed_dim: int = 256,
        hidden_dim: int = 512,
        num_heads: int = 8,
        num_layers: int = 6,
        max_seq_len: int = 512,
        dropout: float = 0.1,
        lambda_entangle: float = 0.5,  # QBNNã‚‚ã¤ã‚Œå¼·åº¦
    ):
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        self.dropout = dropout
        self.lambda_entangle = lambda_entangle


# ========================================
# Part 1: QBNN Layerï¼ˆç‹¬è‡ªã®é‡å­ã‚‚ã¤ã‚Œå±¤ï¼‰
# ========================================

class QBNNLayer(nn.Module):
    """
    Quantum-Bit Neural Network Layer
    
    qbnn_layered.py ã® EQBNNLayer ã‚’åŸºç›¤ã¨ã—ã¦ä½¿ç”¨å¯èƒ½
    
    ç‹¬è‡ªã®æ•°å¼ãƒ¢ãƒ‡ãƒ«:
    1. s^(l) = tanh(h^(l)) âˆˆ [-1, 1]  (æ­£è¦åŒ– â†’ Blochçƒã®zåº§æ¨™)
    2. hÌƒ^(l+1) = W^(l) h^(l) + b^(l)  (ç·šå½¢å¤‰æ›)
    3. Î”^(l+1)_j = Î£_i J^(l)_{ij} s^(l)_i s^(l+1)_{raw,j}  (ã‚‚ã¤ã‚Œè£œæ­£)
    4. Ä¥^(l+1) = hÌƒ^(l+1) + Î»_eff Î”^(l+1)  (æœ‰åŠ¹å…¥åŠ›)
    5. h^(l+1) = activation(Ä¥^(l+1))  (æ´»æ€§åŒ–)
    
    APQBç†è«–ã«åŸºã¥ãæ”¹è‰¯:
    - Î»ã‚’ç¯„å›²ã§åˆ¶å¾¡ã—ã€Î¸ï¼ˆã‚·ãƒ¼ã‚¿ï¼‰ãŒå‹•çš„ã«å¤‰åŒ–ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    - r = cos(2Î¸), T = |sin(2Î¸)|, rÂ² + TÂ² = 1
    
    å‚ç…§: qbnn_layered.py ã® APQB ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, input_dim: int, output_dim: int, 
                 lambda_min: float = 0.2, lambda_max: float = 0.5,
                 use_qbnn_layered: bool = True):  # qbnn_layered.pyã‚’å‚ç…§
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.use_qbnn_layered = use_qbnn_layered and QBNN_LAYERED_AVAILABLE
        
        # Î»ã®ç¯„å›²ï¼ˆÎ¸ãŒå‹•ã‘ã‚‹ã‚ˆã†ã«ï¼‰
        self.lambda_min = lambda_min
        self.lambda_max = lambda_max
        
        if self.use_qbnn_layered:
            # qbnn_layered.py ã® EQBNNLayer ã‚’å†…éƒ¨ã§ä½¿ç”¨
            self.eqbnn_core = EQBNNLayer(
                input_dim=input_dim,
                output_dim=output_dim,
                prev_output_dim=input_dim,
                entangle_strength=(lambda_min + lambda_max) / 2
            )
            # ã‚³ã‚¢å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‚ç…§
            self.W = self.eqbnn_core.linear
            # W_entangle.weightã®å½¢çŠ¶ã¯(output_dim, input_dim)ãªã®ã§ã€å‚ç…§ã¨ã—ã¦ä¿æŒ
            # forwardå†…ã§è»¢ç½®ã—ã¦ä½¿ç”¨ã™ã‚‹
            self.J_source = self.eqbnn_core.entangle_op.W_entangle.weight
        else:
            # W: é€šå¸¸ã®é‡ã¿è¡Œåˆ—
            self.W = nn.Linear(input_dim, output_dim)
            
            # J: ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ«ï¼ˆç‹¬è‡ªï¼‰
            self.J = nn.Parameter(torch.randn(input_dim, output_dim) * 0.02)
        
        # Î»ãƒ™ãƒ¼ã‚¹å€¤ï¼ˆå­¦ç¿’å¯èƒ½ã€0-1ã«æ­£è¦åŒ–ã—ã¦ã‹ã‚‰ç¯„å›²ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
        self.lambda_base = nn.Parameter(torch.tensor(0.5))
        
        # å±¤æ­£è¦åŒ–
        self.layer_norm = nn.LayerNorm(output_dim)
        
        # å‘¼ã³å‡ºã—ã‚«ã‚¦ãƒ³ã‚¿ï¼ˆå‹•çš„å¤‰åŒ–ç”¨ï¼‰
        self.register_buffer('call_count', torch.tensor(0))
    
    def forward(self, h_prev: torch.Tensor) -> torch.Tensor:
        # 1. æ­£è¦åŒ–ï¼ˆBlochçƒã®zåº§æ¨™ã¨ã—ã¦è§£é‡ˆï¼‰
        s_prev = torch.tanh(h_prev)  # (..., input_dim)
        
        # 2. ç·šå½¢å¤‰æ›
        h_tilde = self.W(h_prev)  # (..., output_dim)
        
        # 3. æ¬¡å±¤ã®å€™è£œã‚’æ­£è¦åŒ–
        s_raw = torch.tanh(h_tilde)  # (..., output_dim)
        
        # 4. ã‚‚ã¤ã‚Œè£œæ­£é … Î”
        # Î”_j = Î£_i J_{ij} s^(l)_i * s^(l+1)_{raw,j}
        # J: (input_dim, output_dim)
        # s_prev: (..., input_dim)
        # s_raw: (..., output_dim)
        # delta: (..., output_dim)
        # å„jã«ã¤ã„ã¦: delta_j = Î£_i (J_{ij} * s_prev_i) * s_raw_j
        
        # Jã®å½¢çŠ¶ã‚’ç¢ºèªã—ã¦è»¢ç½®ï¼ˆuse_qbnn_layeredã®å ´åˆã€J_sourceã¯(output_dim, input_dim)ï¼‰
        if self.use_qbnn_layered:
            J = self.J_source.t()  # (input_dim, output_dim)ã«è»¢ç½®
        else:
            J = self.J  # æ—¢ã«(input_dim, output_dim)
        
        # ã¾ãš J_{ij} * s_prev_i ã‚’è¨ˆç®—: (..., output_dim)
        J_s_prev = torch.einsum('...i,ij->...j', s_prev, J)  # (..., output_dim)
        # æ¬¡ã« s_raw_j ã‚’æ›ã‘ã‚‹
        delta = J_s_prev * s_raw  # (..., output_dim)
        
        # 5. å‹•çš„Î»: Î¸ãŒå‹•ã‘ã‚‹ã‚ˆã†ã«ç¯„å›²å†…ã§å¤‰åŒ–ï¼ˆé‡å­ã‚†ã‚‰ãï¼‰
        # Î»_baseã‚’sigmoidã§0-1ã«åˆ¶é™ã—ã€ç¯„å›²ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        lambda_normalized = torch.sigmoid(self.lambda_base)
        
        # æ¨è«–æ™‚ã®ã¿å‹•çš„å¤‰åŒ–ã‚’è¿½åŠ ï¼ˆå­¦ç¿’æ™‚ã¯å®‰å®šæ€§ã®ãŸã‚å›ºå®šå¯„ã‚Šï¼‰
        if not self.training:
            # sinæ³¢ã§å‹•çš„å¤‰åŒ–ï¼ˆÎ¸ãŒå‹•ã‘ã‚‹ã‚ˆã†ã«ï¼‰
            phase = float(self.call_count) * 0.2
            dynamic_factor = 0.5 + 0.5 * math.sin(phase)
            self.call_count += 1
        else:
            dynamic_factor = 0.5
        
        # æœ‰åŠ¹ãªÎ»ã‚’è¨ˆç®—
        lambda_range = self.lambda_max - self.lambda_min
        lambda_eff = self.lambda_min + lambda_range * (lambda_normalized * 0.7 + dynamic_factor * 0.3)
        
        # 6. æœ‰åŠ¹å…¥åŠ›
        h_hat = h_tilde + lambda_eff * delta
        
        # 7. å±¤æ­£è¦åŒ– + GELUæ´»æ€§åŒ–
        output = self.layer_norm(h_hat)
        output = F.gelu(output)
        
        return output
    
    def get_quantum_info(self) -> Dict:
        """é‡å­æƒ…å ±ã‚’å–å¾—"""
        with torch.no_grad():
            lambda_normalized = torch.sigmoid(self.lambda_base).item()
            lambda_eff = self.lambda_min + (self.lambda_max - self.lambda_min) * lambda_normalized
            
            # Jã®å½¢çŠ¶ã‚’ç¢ºèªï¼ˆuse_qbnn_layeredã®å ´åˆã€è»¢ç½®ãŒå¿…è¦ï¼‰
            if self.use_qbnn_layered:
                J = self.J_source.t()  # (input_dim, output_dim)ã«è»¢ç½®
            else:
                J = self.J  # æ—¢ã«(input_dim, output_dim)
            
            info = {
                'lambda_min': self.lambda_min,
                'lambda_max': self.lambda_max,
                'lambda_eff': lambda_eff,
                'J_mean': J.mean().item(),
                'J_std': J.std().item(),
                'J_max': J.max().item(),
                'source': 'qbnn_layered.py' if self.use_qbnn_layered else 'builtin',
            }
            
            # qbnn_layered.pyä½¿ç”¨æ™‚ã¯è¿½åŠ æƒ…å ±ã‚’å–å¾—
            if self.use_qbnn_layered:
                info['entangle_strength'] = self.eqbnn_core.entangle_op.entangle_strength.item()
            
            return info


# ========================================
# Part 2: QBNN-Attentionï¼ˆé‡å­ã‚‚ã¤ã‚Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰
# ========================================

class QBNNAttention(nn.Module):
    """
    QBNNæ‹¡å¼µSelf-Attention
    
    é€šå¸¸ã®Attentionã«é‡å­ã‚‚ã¤ã‚Œè£œæ­£ã‚’è¿½åŠ 
    """
    
    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1, 
                 lambda_val: float = 0.5):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert self.head_dim * num_heads == embed_dim, "embed_dimã¯num_headsã§å‰²ã‚Šåˆ‡ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
        
        # Q, K, V ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        # QBNNé‡å­ã‚‚ã¤ã‚Œè£œæ­£
        self.J_attn = nn.Parameter(torch.randn(num_heads, self.head_dim, self.head_dim) * 0.02)
        self.lambda_attn = nn.Parameter(torch.tensor(float(lambda_val)))
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.head_dim)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        GPTæ¨™æº–: Multi-Head Causal Self-Attentionï¼ˆQBNNæ‹¡å¼µç‰ˆï¼‰
        
        Args:
            x: (batch, seq, embed_dim)
            mask: Optional attention mask (Noneã®å ´åˆã¯Causal Maskã‚’è‡ªå‹•ç”Ÿæˆ)
        
        Returns:
            (batch, seq, embed_dim)
        """
        batch_size, seq_len, _ = x.shape
        
        # GPTæ¨™æº–: Q, K, V è¨ˆç®—
        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch, heads, seq, head_dim)
        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # GPTæ¨™æº–: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢è¨ˆç®—
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (batch, heads, seq, seq)
        
        # QBNNæ‹¡å¼µ: é‡å­ã‚‚ã¤ã‚Œè£œæ­£
        Q_norm = torch.tanh(Q)
        K_norm = torch.tanh(K)
        # ã‚‚ã¤ã‚Œè£œæ­£é …ï¼ˆãƒ˜ãƒƒãƒ‰ã”ã¨ï¼‰
        delta = torch.einsum('bhid,hde,bhje->bhij', Q_norm, self.J_attn, K_norm)
        attn_scores = attn_scores + self.lambda_attn * delta
        
        # GPTæ¨™æº–: Causal Maské©ç”¨
        if mask is not None:
            # æä¾›ã•ã‚ŒãŸãƒã‚¹ã‚¯ã‚’ä½¿ç”¨
            if mask.dim() == 2:
                mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq, seq)
            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))
        else:
            # Causal Maskè‡ªå‹•ç”Ÿæˆï¼ˆGPTæ¨™æº–ï¼‰
            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()
            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq, seq)
            attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))
        
        # GPTæ¨™æº–: Softmax + Dropout
        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.dropout(attn_probs)
        
        # GPTæ¨™æº–: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é©ç”¨
        output = torch.matmul(attn_probs, V)  # (batch, heads, seq, head_dim)
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        
        # GPTæ¨™æº–: å‡ºåŠ›å°„å½±
        return self.out_proj(output)


# ========================================
# Part 3: QBNN-Transformer Block
# ========================================

class QBNNTransformerBlock(nn.Module):
    """
    GPTãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆQBNNæ‹¡å¼µç‰ˆï¼‰
    
    GPTæ¨™æº–æ§‹é€ :
    1. Pre-norm LayerNorm
    2. Multi-Head Causal Self-Attention (QBNNæ‹¡å¼µ)
    3. Residual Connection
    4. Pre-norm LayerNorm
    5. Feed-Forward Network (æ¨™æº–FFN + QBNNæ‹¡å¼µ)
    6. Residual Connection
    """
    
    def __init__(self, config: NeuroQuantumConfig):
        super().__init__()
        
        # Pre-norm LayerNorm
        self.norm1 = nn.LayerNorm(config.embed_dim)
        self.norm2 = nn.LayerNorm(config.embed_dim)
        
        # QBNN-Attention
        self.attention = QBNNAttention(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            dropout=config.dropout,
            lambda_val=config.lambda_entangle
        )
        
        # GPTæ¨™æº–FFN: Linear â†’ GELU â†’ Linear
        self.ffn_standard = nn.Sequential(
            nn.Linear(config.embed_dim, config.hidden_dim),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.hidden_dim, config.embed_dim),
            nn.Dropout(config.dropout)
        )
        
        # QBNNæ‹¡å¼µFFNï¼ˆå€‹åˆ¥ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€Sequentialã§ã¯ãªãå€‹åˆ¥ã«å®šç¾©ï¼‰
        self.ffn_qbnn_layer1 = QBNNLayer(
            config.embed_dim, config.hidden_dim, 
            lambda_min=config.lambda_entangle * 0.5,
            lambda_max=config.lambda_entangle * 1.5
        )
        self.ffn_qbnn_dropout = nn.Dropout(config.dropout)
        self.ffn_qbnn_layer2 = QBNNLayer(
            config.hidden_dim, config.embed_dim,
            lambda_min=config.lambda_entangle * 0.5,
            lambda_max=config.lambda_entangle * 1.5
        )
        
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        GPTãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        
        Args:
            x: (batch, seq, embed_dim)
            mask: Optional attention mask
        
        Returns:
            (batch, seq, embed_dim)
        """
        # 1. Pre-norm + Multi-Head Causal Self-Attention + Residual
        residual = x
        x = self.norm1(x)
        attn_out = self.attention(x, mask)
        x = residual + self.dropout(attn_out)
        
        # 2. Pre-norm + Feed-Forward Network + Residual
        residual = x
        x = self.norm2(x)
        
        # æ¨™æº–FFN + QBNNæ‹¡å¼µï¼ˆãƒ–ãƒ¬ãƒ³ãƒ‰ï¼‰
        ffn_standard_out = self.ffn_standard(x)
        # QBNNæ‹¡å¼µFFN
        ffn_qbnn_out = self.ffn_qbnn_layer1(x)
        ffn_qbnn_out = self.ffn_qbnn_dropout(ffn_qbnn_out)
        ffn_qbnn_out = self.ffn_qbnn_layer2(ffn_qbnn_out)
        
        # ãƒ–ãƒ¬ãƒ³ãƒ‰æ¯”ç‡: æ¨™æº–FFN 70% + QBNNæ‹¡å¼µ 30%
        ffn_out = 0.7 * ffn_standard_out + 0.3 * ffn_qbnn_out
        
        x = residual + ffn_out
        
        return x


# ========================================
# Part 4: Embeddingï¼ˆåŸ‹ã‚è¾¼ã¿å±¤ï¼‰
# ========================================

class NeuroQuantumEmbedding(nn.Module):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q åŸ‹ã‚è¾¼ã¿å±¤
    
    Token â†’ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    """
    
    def __init__(self, config: NeuroQuantumConfig):
        super().__init__()
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        self.text_embedding = nn.Embedding(config.vocab_size, config.embed_dim)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.position_embedding = nn.Embedding(config.max_seq_len, config.embed_dim)
        
        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        self.dropout = nn.Dropout(config.dropout)
        
        # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
        self.embed_dim = config.embed_dim
    
    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len = token_ids.shape
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        text_embeds = self.text_embedding(token_ids)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0).expand(batch_size, -1)
        pos_embeds = self.position_embedding(positions)
        
        # åˆæˆ
        embeds = text_embeds + pos_embeds
        embeds = self.dropout(embeds)
        
        return embeds


# ========================================
# Part 5: Output Headï¼ˆå‡ºåŠ›å±¤ï¼‰
# ========================================

class NeuroQuantumHead(nn.Module):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q å‡ºåŠ›ãƒ˜ãƒƒãƒ‰
    
    ãƒ™ã‚¯ãƒˆãƒ« â†’ èªå½™ç¢ºç‡ã¸ã®å¤‰æ›
    """
    
    def __init__(self, config: NeuroQuantumConfig):
        super().__init__()
        
        # æœ€çµ‚æ­£è¦åŒ–
        self.norm = nn.LayerNorm(config.embed_dim)
        
        # èªå½™ã¸ã®ç·šå½¢å¤‰æ›
        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        return logits


# ========================================
# Part 6: ãƒ‹ãƒ¥ãƒ¼ãƒ­Q ãƒ¢ãƒ‡ãƒ«æœ¬ä½“
# ========================================

class NeuroQuantum(nn.Module):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q: GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformerï¼ˆQBNNæ‹¡å¼µç‰ˆï¼‰
    
    å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆå›³2-4ã«æº–æ‹ ï¼‰:
    1. å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åŒ– â†’ ãƒˆãƒ¼ã‚¯ãƒ³ID
    2. ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆText Embedding + Position Embeddingï¼‰
    3. ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° â†’ GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformerï¼ˆNå€‹ã®Decoder Blocksï¼‰
    4. Transformerå‡ºåŠ› â†’ å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆFinal LayerNorm + Output Headï¼‰
    5. å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ— â†’ å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰
    
    GPTæ¨™æº–æ§‹é€ :
    - Text Embedding + Position Embeddingï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
    - Dropout
    - Nå€‹ã®GPT Decoder Blocksï¼ˆPre-norm + Attention + FFNï¼‰
    - Final LayerNorm
    - Output Head (Linear to vocab_size)
    
    ç‹¬è‡ªè¦ç´ :
    - QBNNLayer: é‡å­ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ« J ã«ã‚ˆã‚‹è£œæ­£
    - QBNN-Attention: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã¸ã®é‡å­è£œæ­£
    - å­¦ç¿’å¯èƒ½ãª Î»ï¼ˆã‚‚ã¤ã‚Œå¼·åº¦ï¼‰
    """
    
    def __init__(self, config: NeuroQuantumConfig):
        super().__init__()
        self.config = config
        
        # GPTæ¨™æº–: Text Embedding + Position Embedding
        self.text_embedding = nn.Embedding(config.vocab_size, config.embed_dim)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        self.position_embedding = nn.Embedding(config.max_seq_len, config.embed_dim)
        
        # GPT Decoder Blocks
        self.transformer_blocks = nn.ModuleList([
            QBNNTransformerBlock(config) for _ in range(config.num_layers)
        ])
        
        # GPTæ¨™æº–: Final LayerNorm
        self.final_norm = nn.LayerNorm(config.embed_dim)
        
        # GPTæ¨™æº–: Output Head (weight tyingå¯èƒ½ã ãŒã€ã“ã“ã§ã¯ç‹¬ç«‹)
        self.output_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)
        
        # GPTæ¨™æº–: Embedding Dropout
        self.dropout = nn.Dropout(config.dropout)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–ï¼ˆGPTæ¨™æº–ï¼‰
        self.apply(self._init_weights)
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        self.num_params = sum(p.numel() for p in self.parameters())
    
    def _init_weights(self, module):
        """GPTæ¨™æº–ã®é‡ã¿åˆæœŸåŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, token_ids: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformer ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ï¼ˆå›³2-4ã®ãƒ•ãƒ­ãƒ¼ã«æº–æ‹ ï¼‰
        
        å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—:
        1. ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆText Embedding + Position Embeddingï¼‰
        2. ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° â†’ GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformer
        3. Transformerå‡ºåŠ› â†’ å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆFinal LayerNorm + Output Headï¼‰
        4. å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ— â†’ ãƒ­ã‚¸ãƒƒãƒˆï¼ˆå‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ï¼‰
        
        Args:
            token_ids: (batch, seq) ãƒˆãƒ¼ã‚¯ãƒ³IDï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            mask: Optional attention mask (Noneã®å ´åˆã¯Causal Maskã‚’è‡ªå‹•ç”Ÿæˆ)
        
        Returns:
            (batch, seq, vocab_size) ãƒ­ã‚¸ãƒƒãƒˆï¼ˆå¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—å¾Œã®å‡ºåŠ›ï¼‰
        """
        batch, seq = token_ids.shape
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        # Text Embedding: ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
        text_embeds = self.text_embedding(token_ids)  # (batch, seq, embed_dim)
        # Position Embedding: ä½ç½®æƒ…å ±ã‚’è¿½åŠ 
        positions = torch.arange(seq, device=token_ids.device).unsqueeze(0).expand(batch, -1)
        pos_embeds = self.position_embedding(positions)  # (batch, seq, embed_dim)
        # åŸ‹ã‚è¾¼ã¿ã®åˆæˆ + Dropout
        hidden_states = self.dropout(text_embeds + pos_embeds)
        
        # Causal Maskç”Ÿæˆï¼ˆmaskãŒNoneã®å ´åˆï¼‰
        if mask is None:
            mask = torch.tril(torch.ones(seq, seq, device=token_ids.device)).unsqueeze(0).unsqueeze(0)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° â†’ GPTå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿ã®Transformer
        # Nå€‹ã®GPT Decoder Blocksï¼ˆPre-norm + Multi-Head Causal Self-Attention + FFNï¼‰
        for block in self.transformer_blocks:
            hidden_states = block(hidden_states, mask)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: Transformerå‡ºåŠ› â†’ å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—
        # Final LayerNorm
        hidden_states = self.final_norm(hidden_states)
        # Output Head: ãƒ™ã‚¯ãƒˆãƒ« â†’ èªå½™ç¢ºç‡ã¸ã®å¤‰æ›
        logits = self.output_head(hidden_states)  # (batch, seq, vocab_size)
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ­ã‚¸ãƒƒãƒˆï¼ˆå‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ï¼‰
        return logits
    
    def get_quantum_info(self) -> List[Dict]:
        """å…¨å±¤ã®é‡å­æƒ…å ±ã‚’å–å¾—"""
        info = []
        for i, block in enumerate(self.transformer_blocks):
            block_info = {
                'block': i,
                'attn_lambda': block.attention.lambda_attn.item(),
            }
            info.append(block_info)
        return info


# ========================================
# Part 7: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
# ========================================

class NeuroQuantumTokenizer:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå›³2-4ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¹ãƒ†ãƒƒãƒ—ã«æº–æ‹ ï¼‰

    SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨:
    - èªå½™ã‚µã‚¤ã‚ºã‚’æŒ‡å®šã—ã¦å­¦ç¿’å¯èƒ½ï¼ˆ8000-32000æ¨å¥¨ï¼‰
    - BPEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ï¼ˆæ—¥æœ¬èªã«é©ã—ã¦ã„ã‚‹ï¼‰
    - ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãŒå¯èƒ½
    - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆSentencePieceæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ï¼‰

    å‡¦ç†ãƒ•ãƒ­ãƒ¼:
    1. å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚’å€‹ã€…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ï¼‰
    2. ãƒˆãƒ¼ã‚¯ãƒ³ â†’ ãƒˆãƒ¼ã‚¯ãƒ³IDï¼ˆå„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•°å€¤IDã«å¤‰æ›ï¼‰
    """

    def __init__(self, vocab_size: int = 8000, model_file: str = None):
        """
        Args:
            vocab_size: èªå½™ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8000ï¼‰
            model_file: æ—¢å­˜ã®SentencePieceãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯æ–°è¦å­¦ç¿’ï¼‰
        """
        self.vocab_size = vocab_size
        self.actual_vocab_size = None
        self.model_file = model_file
        self.sp_model = None

        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.pad_token = '<pad>'
        self.unk_token = '<unk>'
        self.bos_token = '<s>'
        self.eos_token = '</s>'

        # SentencePieceã‚’ä½¿ç”¨
        if SENTENCEPIECE_AVAILABLE:
            if model_file and os.path.exists(model_file):
                # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
                try:
                    self.sp_model = spm.SentencePieceProcessor()
                    self.sp_model.load(model_file)
                    self.actual_vocab_size = self.sp_model.get_piece_size()
                    self.vocab_size = self.actual_vocab_size
                    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—
                    self.pad_id = self.sp_model.pad_id()
                    self.unk_id = self.sp_model.unk_id()
                    self.bos_id = self.sp_model.bos_id()
                    self.eos_id = self.sp_model.eos_id()
                    print(f"   âœ… SentencePieceãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_file} (èªå½™ã‚µã‚¤ã‚º: {self.actual_vocab_size})")
                except Exception as e:
                    warnings.warn(f"SentencePieceãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}ã€‚æ–°è¦å­¦ç¿’ã—ã¾ã™ã€‚")
                    self.sp_model = None
        else:
            # SentencePieceæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
            self.sp_model = None

        # SentencePieceãŒä½¿ãˆãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if self.sp_model is None:
            self._init_fallback()

    def _init_fallback(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–"""
        self.char_to_idx = {'<PAD>': 0, '<UNK>': 1, '<BOS>': 2, '<EOS>': 3}
        self.idx_to_char = {0: '<PAD>', 1: '<UNK>', 2: '<BOS>', 3: '<EOS>'}
        # vocab_sizeã¯è¨­å®šã•ã‚Œã¦ã„ã‚‹å€¤ã‚’ä¿æŒï¼ˆä¸Šæ›¸ãã—ãªã„ï¼‰
        if not hasattr(self, 'vocab_size') or self.vocab_size is None:
            self.vocab_size = 4
        # actual_vocab_sizeã‚’ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åˆæœŸåŒ–ï¼ˆbuild_vocab()ã§æ›´æ–°ã•ã‚Œã‚‹ï¼‰
        self.actual_vocab_size = len(self.char_to_idx)

        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ID
        self.pad_id = 0
        self.unk_id = 1
        self.bos_id = 2
        self.eos_id = 3

    def build_vocab(self, texts: List[str], character_coverage: float = 0.9995, model_prefix: str = "spm_model_layered", min_freq: int = 1):
        """
        SentencePieceã§èªå½™ã‚’å­¦ç¿’

        Args:
            texts: å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            character_coverage: æ–‡å­—ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆ0.9995ãŒæ¨å¥¨ã€æ—¥æœ¬èªã®å ´åˆã¯0.9995-0.99995ï¼‰
            model_prefix: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            min_freq: æœ€å°é »åº¦ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        """
        if not SENTENCEPIECE_AVAILABLE:
            warnings.warn("SentencePieceãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self._build_vocab_fallback(texts, min_freq)
            return self

        # vocab_sizeãŒ4ä»¥ä¸‹ï¼ˆç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼‰ã®å ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        actual_vocab_size = max(self.vocab_size, 8000) if self.vocab_size <= 4 else self.vocab_size
        print(f"   ğŸ”¤ SentencePieceã§èªå½™å­¦ç¿’ä¸­... (ç›®æ¨™èªå½™ã‚µã‚¤ã‚º: {actual_vocab_size})")

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            temp_file = f.name
            for text in texts:
                f.write(text + '\n')

        try:
            # SentencePieceå­¦ç¿’
            spm.SentencePieceTrainer.train(
                input=temp_file,
                model_prefix=model_prefix,
                vocab_size=actual_vocab_size,
                character_coverage=character_coverage,
                model_type='bpe',  # BPEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                pad_id=0,
                unk_id=1,
                bos_id=2,
                eos_id=3,
                pad_piece=self.pad_token,
                unk_piece=self.unk_token,
                bos_piece=self.bos_token,
                eos_piece=self.eos_token,
            )

            # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            model_file_path = model_prefix + '.model'
            self.sp_model = spm.SentencePieceProcessor()
            self.sp_model.load(model_file_path)
            self.actual_vocab_size = self.sp_model.get_piece_size()
            self.vocab_size = self.actual_vocab_size
            self.model_file = model_file_path
            # vocab_sizeã‚’æ›´æ–°
            if hasattr(self, 'vocab_size') and self.vocab_size <= 4:
                self.vocab_size = self.actual_vocab_size

            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—
            self.pad_id = self.sp_model.pad_id()
            self.unk_id = self.sp_model.unk_id()
            self.bos_id = self.sp_model.bos_id()
            self.eos_id = self.sp_model.eos_id()

            print(f"   âœ… SentencePieceèªå½™å­¦ç¿’å®Œäº† (èªå½™ã‚µã‚¤ã‚º: {self.actual_vocab_size})")

        except Exception as e:
            warnings.warn(f"SentencePieceå­¦ç¿’ã«å¤±æ•—: {e}ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self._build_vocab_fallback(texts, min_freq)
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(temp_file):
                os.unlink(temp_file)
            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä»¥å¤–ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            for ext in ['.vocab']:
                temp_file_ext = model_prefix + ext
                if os.path.exists(temp_file_ext):
                    os.unlink(temp_file_ext)

        return self

    def _build_vocab_fallback(self, texts: List[str], min_freq: int = 1):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç°¡æ˜“èªå½™æ§‹ç¯‰"""
        print(f"   ğŸ”¤ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯èªå½™æ§‹ç¯‰ä¸­...")
        char_counts = Counter()
        for text in texts:
            char_counts.update(list(text))

        # vocab_sizeãŒ4ä»¥ä¸‹ï¼ˆç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼‰ã®å ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        target_vocab_size = max(self.vocab_size, 8000) if self.vocab_size <= 4 else self.vocab_size

        for char, count in char_counts.most_common(target_vocab_size - 4):
            if char not in self.char_to_idx and count >= min_freq:
                idx = len(self.char_to_idx)
                self.char_to_idx[char] = idx
                self.idx_to_char[idx] = char

        self.vocab_size = len(self.char_to_idx)
        self.actual_vocab_size = self.vocab_size
        print(f"   âœ… èªå½™ã‚µã‚¤ã‚º: {self.vocab_size}")

    def encode(self, text: str, add_special: bool = False) -> List[int]:
        """
        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå›³2-4ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¹ãƒ†ãƒƒãƒ—ï¼‰

        å‡¦ç†:
        1. å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚’å€‹ã€…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ï¼‰
        2. ãƒˆãƒ¼ã‚¯ãƒ³ â†’ ãƒˆãƒ¼ã‚¯ãƒ³IDï¼ˆå„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•°å€¤IDã«å¤‰æ›ï¼‰

        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¾‹: "This is an example."ï¼‰
            add_special: ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆBOS/EOSï¼‰ã‚’è¿½åŠ ã™ã‚‹ã‹

        Returns:
            ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: [40134, 2052, 133, 389, 12]ï¼‰
        """
        if self.sp_model is not None:
            # SentencePieceä½¿ç”¨
            if add_special:
                return self.sp_model.encode(text, out_type=int, add_bos=True, add_eos=True)
            else:
                return self.sp_model.encode(text, out_type=int, add_bos=False, add_eos=False)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœ€é•·ãƒãƒƒãƒæ–¹å¼
            tokens = []
            if add_special:
                tokens.append(self.bos_id)

            i = 0
            text_len = len(text)
            while i < text_len:
                matched = False
                for length in range(min(8, text_len - i), 0, -1):
                    substr = text[i:i+length]
                    if substr in self.char_to_idx:
                        tokens.append(self.char_to_idx[substr])
                        i += length
                        matched = True
                        break

                if not matched:
                    tokens.append(self.char_to_idx.get(text[i], self.unk_id))
                    i += 1

            if add_special:
                tokens.append(self.eos_id)

            return tokens

    def decode(self, token_ids: List[int], skip_special: bool = True) -> str:
        """ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        if self.sp_model is not None:
            # SentencePieceä½¿ç”¨
            return self.sp_model.decode(token_ids)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            chars = []
            special_ids = {self.pad_id, self.unk_id, self.bos_id, self.eos_id}

            for t in token_ids:
                if skip_special and t in special_ids:
                    continue
                char = self.idx_to_char.get(t, self.unk_token)
                if char not in ['<PAD>', '<UNK>', '<BOS>', '<EOS>']:
                    chars.append(char)

            return ''.join(chars)

    def save(self, path: str):
        """ä¿å­˜"""
        if self.sp_model is not None:
            # SentencePieceãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹
            print(f"   âœ… SentencePieceãƒ¢ãƒ‡ãƒ«ä¿å­˜æ¸ˆã¿: {self.model_file}")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ä¿å­˜
            data = {
                'char_to_idx': self.char_to_idx,
                'vocab_size': self.vocab_size,
            }
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self, path: str):
        """èª­ã¿è¾¼ã¿"""
        if path.endswith('.model'):
            # SentencePieceãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            if SENTENCEPIECE_AVAILABLE:
                try:
                    self.sp_model = spm.SentencePieceProcessor()
                    self.sp_model.load(path)
                    self.actual_vocab_size = self.sp_model.get_piece_size()
                    self.vocab_size = self.actual_vocab_size
                    self.model_file = path
                    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—
                    self.pad_id = self.sp_model.pad_id()
                    self.unk_id = self.sp_model.unk_id()
                    self.bos_id = self.sp_model.bos_id()
                    self.eos_id = self.sp_model.eos_id()
                    print(f"   âœ… SentencePieceãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {path} (èªå½™ã‚µã‚¤ã‚º: {self.actual_vocab_size})")
                except Exception as e:
                    warnings.warn(f"SentencePieceãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            else:
                warnings.warn("SentencePieceãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.char_to_idx = data['char_to_idx']
            self.idx_to_char = {int(i): c for c, i in self.char_to_idx.items()}
            self.vocab_size = data['vocab_size']
        return self


# ========================================
# Part 8: ãƒ‹ãƒ¥ãƒ¼ãƒ­Q AIï¼ˆç”ŸæˆAIæœ¬ä½“ï¼‰
# ========================================

class NeuroQuantumAI:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­Q AI
    
    QBNN-LLM ã«ã‚ˆã‚‹ç”ŸæˆAI
    
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆhidden_dimï¼‰ã‚’æŒ‡å®šå¯èƒ½
    """
    
    def __init__(
        self,
        embed_dim: int = 48,
        hidden_dim: int = 96,       # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆFFNå±¤ã®æ¬¡å…ƒï¼‰
        num_heads: int = 4,
        num_layers: int = 2,
        max_seq_len: int = 128,
        dropout: float = 0.1,
        lambda_entangle: float = 0.5,
        use_openai_embedding: bool = False,  # OpenAI Embeddingã‚’ä½¿ç”¨ã™ã‚‹ã‹
        openai_api_key: Optional[str] = None,  # OpenAI APIã‚­ãƒ¼
        openai_model: str = "text-embedding-3-large",  # OpenAI Embeddingãƒ¢ãƒ‡ãƒ«
    ):
        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ: MPS (Apple Silicon) > CUDA > CPU
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print("ğŸ® NVIDIA GPU (CUDA) ã‚’ä½¿ç”¨")
        else:
            self.device = torch.device("cpu")
            print("ğŸ’» CPU ã‚’ä½¿ç”¨")
        
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        self.dropout = dropout
        self.lambda_entangle = lambda_entangle
        self.use_openai_embedding = use_openai_embedding
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.openai_model = openai_model
        
        self.tokenizer: Optional[NeuroQuantumTokenizer] = None
        self.model: Optional[NeuroQuantum] = None
        self.config: Optional[NeuroQuantumConfig] = None

        # é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼
        self.quantum_computer: Optional[QuantumComputer] = None
        self.use_quantum_simulation = QUANTUM_COMPUTER_AVAILABLE
        if self.use_quantum_simulation:
            self.quantum_computer = QuantumComputer("NeuroQuantum-QC")
            print("âš›ï¸  é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    def train(self, texts: List[str], epochs: int = 50, batch_size: int = 16,
              lr: float = 0.001, seq_len: int = 64):
        """å­¦ç¿’"""
        print("\n" + "=" * 70)
        print("ğŸ“š ãƒ‹ãƒ¥ãƒ¼ãƒ­Q å­¦ç¿’é–‹å§‹")
        print("=" * 70)

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰
        print("\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰...")

        # æ—¢å­˜ã®SentencePieceãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
        tokenizer_model_paths = [
            "neuroq_tokenizer.model",  # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ¨å¥¨ï¼‰
            "neuroq_tokenizer_8k.model",  # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ—§åç§°ï¼‰
            "../neuroq_tokenizer.model",  # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            "../neuroq_tokenizer_8k.model",  # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ—§åç§°ï¼‰
            os.path.join(os.path.dirname(__file__), "neuroq_tokenizer.model"),  # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            os.path.join(os.path.dirname(__file__), "neuroq_tokenizer_8k.model"),  # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ—§åç§°ï¼‰
            os.path.join(os.path.dirname(os.path.dirname(__file__)), "neuroq_tokenizer.model"),  # è¦ªã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            os.path.join(os.path.dirname(os.path.dirname(__file__)), "neuroq_tokenizer_8k.model"),  # è¦ªã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ—§åç§°ï¼‰
        ]

        existing_model = None
        for path in tokenizer_model_paths:
            if os.path.exists(path):
                existing_model = path
                break

        if existing_model:
            # æ—¢å­˜ã®SentencePieceãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            print(f"   æ—¢å­˜ã®SentencePieceãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {existing_model}")
            self.tokenizer = NeuroQuantumTokenizer(vocab_size=8000, model_file=existing_model)
        else:
            # æ–°è¦ã«èªå½™ã‚’æ§‹ç¯‰
            print("   æ–°è¦ã«èªå½™ã‚’æ§‹ç¯‰ã—ã¾ã™...")
            self.tokenizer = NeuroQuantumTokenizer(vocab_size=8000)
            self.tokenizer.build_vocab(texts)

        # èªå½™ã‚µã‚¤ã‚ºã‚’å–å¾—ï¼ˆactual_vocab_sizeãŒNoneã®å ´åˆã¯vocab_sizeã‚’ä½¿ç”¨ï¼‰
        effective_vocab_size = self.tokenizer.actual_vocab_size
        if effective_vocab_size is None or effective_vocab_size <= 4:
            effective_vocab_size = self.tokenizer.vocab_size
        if effective_vocab_size is None or effective_vocab_size <= 4:
            effective_vocab_size = 8000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        print(f"   èªå½™ã‚µã‚¤ã‚º: {effective_vocab_size}")
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        print("\nğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ­Qãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰...")
        self.config = NeuroQuantumConfig(
            vocab_size=effective_vocab_size,
            embed_dim=self.embed_dim,
            hidden_dim=self.hidden_dim,
            num_heads=self.num_heads,
            num_layers=self.num_layers,
            max_seq_len=self.max_seq_len,
            dropout=self.dropout,
            lambda_entangle=self.lambda_entangle,
        )
        
        self.model = NeuroQuantum(self.config).to(self.device)
        
        print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ§‹æˆ:")
        print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {self.embed_dim}")
        print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {self.hidden_dim}")
        print(f"   ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰: {self.num_heads}")
        print(f"   Transformerãƒ–ãƒ­ãƒƒã‚¯: {self.num_layers}")
        print(f"   ã‚‚ã¤ã‚Œå¼·åº¦ Î»: {self.lambda_entangle}")
        print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model.num_params:,}")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
        all_tokens = []
        for text in texts:
            tokens = self.tokenizer.encode(text)
            all_tokens.extend(tokens)
        
        all_tokens = torch.tensor(all_tokens, dtype=torch.long)
        print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens):,}")
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
        sequences = []
        for i in range(0, len(all_tokens) - seq_len - 1, seq_len // 2):
            x = all_tokens[i:i+seq_len]
            y = all_tokens[i+1:i+seq_len+1]
            if len(x) == seq_len and len(y) == seq_len:
                sequences.append((x, y))
        
        print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences):,}")
        
        # å­¦ç¿’
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.CrossEntropyLoss()
        
        print("\nğŸš€ å­¦ç¿’ãƒ«ãƒ¼ãƒ—...")
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            np.random.shuffle(sequences)
            
            for i in range(0, len(sequences), batch_size):
                batch = sequences[i:i+batch_size]
                if len(batch) == 0:
                    continue
                
                x_batch = torch.stack([s[0] for s in batch]).to(self.device)
                y_batch = torch.stack([s[1] for s in batch]).to(self.device)
                
                optimizer.zero_grad()
                logits = self.model(x_batch)
                
                loss = criterion(
                    logits.view(-1, self.config.vocab_size),
                    y_batch.view(-1)
                )
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
            
            scheduler.step()
            avg_loss = total_loss / max(1, len(sequences) // batch_size)
            
            if (epoch + 1) % 5 == 0 or epoch == 0:
                print(f"   Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}")
        
        print("\nâœ… å­¦ç¿’å®Œäº†ï¼")
        
        # é‡å­æƒ…å ±
        print("\nâš›ï¸ é‡å­ã‚‚ã¤ã‚Œæƒ…å ±:")
        for info in self.model.get_quantum_info():
            print(f"   Block {info['block']}: Î»_attn = {info['attn_lambda']:.4f}")

    def train_on_texts(self, texts: List[str], epochs: int = 50, batch_size: int = 16,
                       lr: float = 0.001, seq_len: int = 64):
        """
        train()ã¸ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
        
        Args:
            texts: å­¦ç¿’ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            lr: å­¦ç¿’ç‡
            seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        """
        return self.train(texts, epochs=epochs, batch_size=batch_size, lr=lr, seq_len=seq_len)

    def _quantum_circuit_influence(self, logits: torch.Tensor, step: int) -> torch.Tensor:
        """
        é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦logitsã«å½±éŸ¿ã‚’ä¸ãˆã‚‹

        Args:
            logits: æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã®logits
            step: ç¾åœ¨ã®ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ—

        Returns:
            é‡å­çš„ã«èª¿æ•´ã•ã‚ŒãŸlogits
        """
        if not self.use_quantum_simulation or self.quantum_computer is None:
            return logits

        # 3é‡å­ãƒ“ãƒƒãƒˆå›è·¯ã‚’ä½œæˆï¼ˆã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå›è·¯ã‚’æ§‹ç¯‰ï¼‰
        n_qubits = 3
        circuit_name = f"generation_step_{step}"
        qc = self.quantum_computer.create_circuit(circuit_name, n_qubits)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ã«åŸºã¥ã„ã¦é‡å­ã‚²ãƒ¼ãƒˆã‚’é©ç”¨
        # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«å¿œã˜ã¦ç•°ãªã‚‹é‡å­å›è·¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ
        if step % 4 == 0:
            # ãƒ™ãƒ«çŠ¶æ…‹ã‚’ä½œæˆ
            qc.h(0).cnot(0, 1).cnot(1, 2)
        elif step % 4 == 1:
            # GHZçŠ¶æ…‹ã‚’ä½œæˆ
            qc.h(0).cnot(0, 1).cnot(0, 2)
        elif step % 4 == 2:
            # WçŠ¶æ…‹é¢¨ã®é‡ã­åˆã‚ã›
            qc.h(0).h(1).cnot(0, 2)
        else:
            # å›è»¢ã‚²ãƒ¼ãƒˆã‚’ä½¿ç”¨
            theta = step * 0.1
            qc.ry(0, theta).ry(1, theta * 1.5).cnot(0, 1)

        # é‡å­å›è·¯ã‚’æ¸¬å®šï¼ˆè¤‡æ•°å›å®Ÿè¡Œã—ã¦çµ±è¨ˆã‚’å–å¾—ï¼‰
        results = qc.run(shots=100)

        # æ¸¬å®šçµæœã®ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—
        total_shots = sum(results.values())
        probs = {state: count / total_shots for state, count in results.items()}

        # é‡å­æ¸¬å®šçµæœã‚’logitsã®èª¿æ•´ã«ä½¿ç”¨
        # ãƒ“ãƒƒãƒˆåˆ—ã‚’æ•°å€¤ã«å¤‰æ›ã—ã€logitsã«é‡å­çš„ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹
        quantum_influence = torch.zeros_like(logits)
        for state, prob in probs.items():
            # ãƒ“ãƒƒãƒˆåˆ—ã‚’æ•´æ•°ã«å¤‰æ›ï¼ˆä¾‹: "101" -> 5ï¼‰
            state_value = int(state, 2)
            # çŠ¶æ…‹å€¤ã‚’ä½¿ã£ã¦logitsã®ä¸€éƒ¨ã‚’èª¿æ•´
            # vocab_sizeã«å¯¾ã—ã¦ãƒ¢ã‚¸ãƒ¥ãƒ­ã‚’å–ã‚Šã€å¾ªç’°çš„ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹
            vocab_size = logits.size(0)
            for i in range(0, vocab_size, 8):  # 8ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«å½±éŸ¿
                idx = (i + state_value) % vocab_size
                quantum_influence[idx] += prob * 0.3  # é‡å­çš„ãªå½±éŸ¿ã®å¼·åº¦

        # å…ƒã®logitsã«é‡å­çš„ãªå½±éŸ¿ã‚’åŠ ç®—
        adjusted_logits = logits + quantum_influence

        return adjusted_logits

    def generate(
        self,
        prompt: str = "",
        max_length: int = 100,
        temp_min: float = 0.4,       # æ¸©åº¦ã®ä¸‹é™
        temp_max: float = 0.8,       # æ¸©åº¦ã®ä¸Šé™
        top_k: int = 40,
        top_p: float = 0.9,
        repetition_penalty: float = 2.0,
        temperature: float = None,   # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼ˆæŒ‡å®šã•ã‚ŒãŸå ´åˆtemp_min/temp_maxã‚’è‡ªå‹•è¨ˆç®—ï¼‰
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆå¯¾è©±å½¢å¼ï¼‰
        
        æ¸©åº¦ã‚’ç¯„å›²ã§æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€Î¸ï¼ˆã‚·ãƒ¼ã‚¿ï¼‰ãŒå‹•çš„ã«å¤‰åŒ–ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
        APQBç†è«–: r = cos(2Î¸), T = |sin(2Î¸)|, rÂ² + TÂ² = 1
        æ¸©åº¦TãŒå›ºå®šã ã¨Î¸ãŒå›ºå®šã•ã‚Œã€é‡å­çš„ã‚†ã‚‰ããŒãªããªã‚‹ã€‚
        
        Args:
            temperature: å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®å˜ä¸€æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
                        æŒ‡å®šã•ã‚ŒãŸå ´åˆã€temp_min = temperature * 0.8, 
                        temp_max = temperature * 1.2 ã«è‡ªå‹•å¤‰æ›ã•ã‚Œã¾ã™ã€‚
        """
        # å¾Œæ–¹äº’æ›æ€§: temperature ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã€temp_min/temp_max ã«å¤‰æ›
        if temperature is not None:
            temp_min = temperature * 0.8
            temp_max = temperature * 1.2
        
        if self.model is None:
            # è‡ªå‹•çš„ã«ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã‚’å®Ÿè¡Œ
            print("âš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒæœªå­¦ç¿’ã§ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§è‡ªå‹•å­¦ç¿’ã‚’é–‹å§‹...")
            sample_data = [
                "äººå·¥çŸ¥èƒ½ã¯ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®æŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã„ã¾ã™ã€‚",
                "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸæ¬¡ä¸–ä»£ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã¯è§£ã‘ãªã„è¤‡é›‘ãªå•é¡Œã‚’é«˜é€Ÿã«è§£ãã“ã¨ãŒã§ãã¾ã™ã€‚",
                "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®è¨€èªã‚’ç†è§£ã—ã€ç”Ÿæˆã™ã‚‹ãŸã‚ã®æŠ€è¡“ã§ã™ã€‚ç¿»è¨³ã€è¦ç´„ã€è³ªå•å¿œç­”ãªã©ã®ã‚¿ã‚¹ã‚¯ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚",
                "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€äººé–“ã®è„³ã®ç¥çµŒç´°èƒã®åƒãã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å±¤çŠ¶ã«æ¥ç¶šã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã§æ§‹æˆã•ã‚Œã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´ã‚’å­¦ç¿’ã—ã¾ã™ã€‚",
                "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«æŒ‡ç¤ºã‚’ä¸ãˆã‚‹ãŸã‚ã®è¨€èªã‚’ä½¿ã£ã¦ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚’ä½œæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚Pythonã€JavaScriptã€Javaãªã©å¤šãã®è¨€èªãŒã‚ã‚Šã¾ã™ã€‚",
                "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¯ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã€ãƒ“ã‚¸ãƒã‚¹ã‚„ç ”ç©¶ã«æ´»ç”¨ã™ã‚‹å­¦å•åˆ†é‡ã§ã™ã€‚çµ±è¨ˆå­¦ã€æ©Ÿæ¢°å­¦ç¿’ã€å¯è¦–åŒ–ãªã©ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚",
                "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆçµŒç”±ã§ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒªã‚½ãƒ¼ã‚¹ã‚’æä¾›ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚AWSã€Azureã€GCPãªã©ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãŒä»£è¡¨çš„ã§ã™ã€‚",
                "ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ã¯ã€åˆ†æ•£å‹å°å¸³æŠ€è¡“ã®ä¸€ç¨®ã§ã€ãƒ‡ãƒ¼ã‚¿ã®æ”¹ã–ã‚“ã‚’é˜²ãä»•çµ„ã¿ã‚’æŒã£ã¦ã„ã¾ã™ã€‚æš—å·é€šè²¨ã‚„å¥‘ç´„ç®¡ç†ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            ]
            try:
                self.train(sample_data, epochs=3)  # è»½é‡ãªå­¦ç¿’
                print("âœ… è‡ªå‹•å­¦ç¿’å®Œäº†")
            except Exception as e:
                print(f"âš ï¸ è‡ªå‹•å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                raise ValueError(f"ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
        # å­¦ç¿’å¾Œã‚‚ãƒ¢ãƒ‡ãƒ«ãŒNoneã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.model.eval()
        
        # å¯¾è©±å½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        dialogue_prompt = f"<USER>{prompt}<ASSISTANT>"
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        tokens = self.tokenizer.encode(dialogue_prompt, add_special=True)[:-1]
        
        tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)
        generated = tokens[0].tolist()
        
        with torch.no_grad():
            for step in range(max_length):
                # æœ€æ–°ã®max_seq_lenãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨
                input_tokens = tokens[:, -self.max_seq_len:] if tokens.size(1) > self.max_seq_len else tokens
                
                logits = self.model(input_tokens)
                next_logits = logits[0, -1, :]

                # âš›ï¸ é‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å½±éŸ¿ã‚’é©ç”¨
                next_logits = self._quantum_circuit_influence(next_logits, step)

                # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæ¸©åº¦èª¿æ•´ã®å‰ã«é©ç”¨ï¼‰
                # ã‚ˆã‚Šé•·ã„ç¯„å›²ï¼ˆ100ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’è¦‹ã¦ã€ã‚ˆã‚Šå¼·ã„ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’é©ç”¨
                recent_tokens = set(generated[-100:])
                for token_id in recent_tokens:
                    next_logits[token_id] /= repetition_penalty

                # å‹•çš„æ¸©åº¦: Î¸ãŒå‹•ã‘ã‚‹ã‚ˆã†ã«ç¯„å›²å†…ã§å¤‰åŒ–ã•ã›ã‚‹
                # sinæ³¢ã§æ»‘ã‚‰ã‹ã«å¤‰å‹•ï¼ˆé‡å­çš„ãªæŒ¯å‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
                theta_phase = step * 0.3  # ä½ç›¸
                temperature = temp_min + (temp_max - temp_min) * (0.5 + 0.5 * math.sin(theta_phase))

                # æ¸©åº¦èª¿æ•´
                next_logits = next_logits / temperature
                
                # Top-K
                if top_k > 0:
                    top_k_vals, _ = torch.topk(next_logits, min(top_k, next_logits.size(-1)))
                    indices_to_remove = next_logits < top_k_vals[-1]
                    next_logits[indices_to_remove] = float('-inf')
                
                # Top-P
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                    sorted_indices_to_remove[0] = False
                    
                    indices_to_remove = sorted_indices[sorted_indices_to_remove]
                    next_logits[indices_to_remove] = float('-inf')
                
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                probs = F.softmax(next_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # EOSæ¤œå‡º
                if next_token.item() == self.tokenizer.eos_id:
                    break
                
                # <USER>ãƒˆãƒ¼ã‚¯ãƒ³ãŒå‡ºãŸã‚‰çµ‚äº†ï¼ˆæ¬¡ã®è³ªå•ã«å…¥ã‚‰ãªã„ã‚ˆã†ã«ï¼‰
                generated.append(next_token.item())
                decoded_so_far = self.tokenizer.decode(generated)
                if "<USER>" in decoded_so_far.split("<ASSISTANT>")[-1]:
                    break
                
                tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)
        
        # å¿œç­”éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
        full_text = self.tokenizer.decode(generated)
        
        # <ASSISTANT>ä»¥é™ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        if "<ASSISTANT>" in full_text:
            response = full_text.split("<ASSISTANT>")[-1]
            # <USER>ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰é™¤å»
            if "<USER>" in response:
                response = response.split("<USER>")[0]
            return response.strip()
        
        return full_text
    
    def chat(self):
        """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
        print("\n" + "=" * 70)
        print("ğŸ’¬ ãƒ‹ãƒ¥ãƒ¼ãƒ­Q ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰")
        print("=" * 70)
        print("\nã‚³ãƒãƒ³ãƒ‰:")
        print("  /quit         - çµ‚äº†")
        print("  /temp <min> <max> - æ¸©åº¦ç¯„å›² (ä¾‹: /temp 0.4 0.8)")
        print("  /len <å€¤>     - ç”Ÿæˆé•·ã• (10-500)")
        print("  /info         - ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
        print("  /quantum      - é‡å­ã‚‚ã¤ã‚Œæƒ…å ±")
        print("-" * 70)
        
        temp_min = 0.4  # æ¸©åº¦ã®ä¸‹é™
        temp_max = 0.8  # æ¸©åº¦ã®ä¸Šé™
        max_length = 100
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ You: ").strip()
                
                if not user_input:
                    continue
                
                if user_input == '/quit':
                    print("ğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼")
                    break
                
                if user_input.startswith('/temp '):
                    try:
                        parts = user_input.split()
                        if len(parts) >= 3:
                            temp_min = float(parts[1])
                            temp_max = float(parts[2])
                            temp_min = max(0.1, min(1.0, temp_min))
                            temp_max = max(0.1, min(1.0, temp_max))
                            if temp_min > temp_max:
                                temp_min, temp_max = temp_max, temp_min
                            print(f"   æ¸©åº¦ç¯„å›²ã‚’ {temp_min:.2f} - {temp_max:.2f} ã«è¨­å®šï¼ˆÎ¸ãŒå‹•ã‘ã‚‹ï¼‰")
                        else:
                            print("   ä½¿ã„æ–¹: /temp <æœ€å°> <æœ€å¤§> (ä¾‹: /temp 0.4 0.8)")
                    except:
                        print("   ã‚¨ãƒ©ãƒ¼: /temp <æœ€å°> <æœ€å¤§>")
                    continue
                
                if user_input.startswith('/len '):
                    try:
                        max_length = int(user_input.split()[1])
                        max_length = max(10, min(500, max_length))
                        print(f"   ç”Ÿæˆé•·ã•ã‚’ {max_length} ã«è¨­å®š")
                    except:
                        print("   ã‚¨ãƒ©ãƒ¼: /len <æ•°å€¤>")
                    continue
                
                if user_input == '/info':
                    print(f"\nğŸ“Š ãƒ‹ãƒ¥ãƒ¼ãƒ­Q ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
                    vocab_size = self.config.vocab_size if self.config else (self.tokenizer.actual_vocab_size or self.tokenizer.vocab_size)
                    print(f"   èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
                    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {self.embed_dim}")
                    print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {self.hidden_dim}")
                    print(f"   ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰: {self.num_heads}")
                    print(f"   Transformerãƒ–ãƒ­ãƒƒã‚¯: {self.num_layers}")
                    print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model.num_params:,}")
                    continue
                
                if user_input == '/quantum':
                    print(f"\nâš›ï¸ é‡å­ã‚‚ã¤ã‚Œæƒ…å ±:")
                    for info in self.model.get_quantum_info():
                        print(f"   Block {info['block']}: Î»_attn = {info['attn_lambda']:.4f}")
                    continue
                
                # ç”Ÿæˆ
                print(f"\nğŸ¤– ãƒ‹ãƒ¥ãƒ¼ãƒ­Q: ", end="", flush=True)
                response = self.generate(
                    prompt=user_input,
                    max_length=max_length,
                    temp_min=temp_min,
                    temp_max=temp_max
                )
                
                print(response)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                break
            except Exception as e:
                print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)
        
        torch.save({
            'model_state': self.model.state_dict(),
            'config': self.config.__dict__,
        }, path + '.pt')
        
        self.tokenizer.save(path + '_tokenizer.json')
        print(f"âœ… ä¿å­˜: {path}")
    
    def load(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        self.tokenizer = NeuroQuantumTokenizer()
        self.tokenizer.load(path + '_tokenizer.json')
        
        # ãƒ¢ãƒ‡ãƒ«
        checkpoint = torch.load(path + '.pt', map_location=self.device)
        self.config = NeuroQuantumConfig(**checkpoint['config'])
        self.model = NeuroQuantum(self.config).to(self.device)
        self.model.load_state_dict(checkpoint['model_state'])
        
        print(f"âœ… èª­ã¿è¾¼ã¿: {path}")
        return self


# ========================================
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
# ========================================

def load_huggingface_data(max_samples: int = 500) -> List[str]:
    """Hugging Faceã‹ã‚‰å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    print("\nğŸ“¥ Hugging Faceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    
    try:
        from datasets import load_dataset
    except ImportError:
        print("   âš ï¸ datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚ã‚Šã¾ã›ã‚“ã€‚pip install datasetsã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return []
    
    formatted_texts = []
    
    # 1. OpenAssistant/oasst1 - é«˜å“è³ªãªå¯¾è©±ãƒ‡ãƒ¼ã‚¿
    try:
        print("   ğŸ“š OpenAssistant/oasst1 ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        dataset = load_dataset("OpenAssistant/oasst1", split="train", trust_remote_code=True)
        
        # å¯¾è©±ãƒ„ãƒªãƒ¼ã‹ã‚‰è³ªå•-å›ç­”ãƒšã‚¢ã‚’æŠ½å‡º
        messages_by_parent = {}
        root_messages = []
        
        for item in dataset:
            parent_id = item.get('parent_id')
            msg_id = item.get('message_id')
            text = item.get('text', '')
            role = item.get('role', '')
            lang = item.get('lang', '')
            
            if not text or len(text) < 5:
                continue
            
            if parent_id is None:
                root_messages.append(item)
            else:
                if parent_id not in messages_by_parent:
                    messages_by_parent[parent_id] = []
                messages_by_parent[parent_id].append(item)
        
        # ãƒ«ãƒ¼ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆè³ªå•ï¼‰ã«å¯¾ã™ã‚‹å›ç­”ã‚’å–å¾—
        count = 0
        for root in root_messages:
            if count >= max_samples // 2:
                break
            
            root_id = root.get('message_id')
            root_text = root.get('text', '')
            root_lang = root.get('lang', '')
            
            # æ—¥æœ¬èªã¾ãŸã¯è‹±èªã®ã¿
            if root_lang not in ['ja', 'en']:
                continue
            
            # å›ç­”ã‚’å–å¾—
            if root_id in messages_by_parent:
                responses = messages_by_parent[root_id]
                if responses:
                    # æœ€åˆã®å›ç­”ã‚’ä½¿ç”¨
                    response = responses[0]
                    response_text = response.get('text', '')
                    
                    if len(root_text) < 200 and len(response_text) < 300:
                        formatted = f"<USER>{root_text}<ASSISTANT>{response_text}"
                        formatted_texts.append(formatted)
                        count += 1
        
        print(f"   âœ… OpenAssistant: {count} ãƒšã‚¢å–å¾—")
        
    except Exception as e:
        print(f"   âš ï¸ OpenAssistantèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 2. kunishou/databricks-dolly-15k-ja - æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿
    try:
        print("   ğŸ“š databricks-dolly-15k-ja ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        dataset = load_dataset("kunishou/databricks-dolly-15k-ja", split="train", trust_remote_code=True)
        
        count = 0
        for item in dataset:
            if count >= max_samples // 4:
                break
            
            instruction = item.get('instruction', '')
            output = item.get('output', '')
            
            if instruction and output and len(instruction) < 150 and len(output) < 300:
                formatted = f"<USER>{instruction}<ASSISTANT>{output}"
                formatted_texts.append(formatted)
                count += 1
        
        print(f"   âœ… dolly-ja: {count} ãƒšã‚¢å–å¾—")
        
    except Exception as e:
        print(f"   âš ï¸ dolly-jaèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 3. databricks/databricks-dolly-15k - è‹±èªãƒ‡ãƒ¼ã‚¿
    try:
        print("   ğŸ“š databricks-dolly-15k ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        dataset = load_dataset("databricks/databricks-dolly-15k", split="train", trust_remote_code=True)
        
        count = 0
        for item in dataset:
            if count >= max_samples // 4:
                break
            
            instruction = item.get('instruction', '')
            response = item.get('response', '')
            
            if instruction and response and len(instruction) < 150 and len(response) < 300:
                formatted = f"<USER>{instruction}<ASSISTANT>{response}"
                formatted_texts.append(formatted)
                count += 1
        
        print(f"   âœ… dolly-en: {count} ãƒšã‚¢å–å¾—")
        
    except Exception as e:
        print(f"   âš ï¸ dolly-enèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"\nğŸ“Š åˆè¨ˆ: {len(formatted_texts)} å¯¾è©±ãƒšã‚¢å–å¾—å®Œäº†")
    
    return formatted_texts


def get_training_data() -> List[str]:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆå¯¾è©±å½¢å¼ï¼‰- æ‹¡å¼µç‰ˆ"""
    
    # å¯¾è©±ãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    conversations = [
        # ========================================
        # æ—¥æœ¬èª - æŒ¨æ‹¶ï¼ˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³å¤šæ•°ï¼‰
        # ========================================
        ("ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ã“ã‚“ã«ã¡ã¯ï¼", "ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ã“ã‚“ã«ã¡ã‚", "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ã”è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã‚‚è‰¯ã„ä¸€æ—¥ã«ãªã‚Šã¾ã™ã‚ˆã†ã«ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ã“ã‚“ã°ã‚“ã¯", "ã“ã‚“ã°ã‚“ã¯ï¼å¤œé…ãã¾ã§ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ã‚„ã‚", "ã‚„ã‚ï¼ä½•ã‹è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼ŸãŠæ°—è»½ã«ã©ã†ãã€‚"),
        ("ãƒãƒ­ãƒ¼", "ãƒãƒ­ãƒ¼ï¼ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚ä½•ã§ã‚‚èã„ã¦ãã ã•ã„ï¼"),
        ("ã©ã†ã‚‚", "ã©ã†ã‚‚ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ã‚ã‚ŠãŒã¨ã†", "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ãŠå½¹ã«ç«‹ã¦ã¦å¬‰ã—ã„ã§ã™ã€‚"),
        ("ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™", "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ä»–ã«ã”è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ã©ã†ãã€‚"),
        ("ã•ã‚ˆã†ãªã‚‰", "ã•ã‚ˆã†ãªã‚‰ï¼ã¾ãŸãŠä¼šã„ã—ã¾ã—ã‚‡ã†ã€‚"),
        ("ãƒã‚¤ãƒã‚¤", "ãƒã‚¤ãƒã‚¤ï¼ã¾ãŸæ¥ã¦ãã ã•ã„ã­ã€‚"),
        ("ã¾ãŸä¼šãŠã†", "ã¯ã„ã€ã¾ãŸãŠä¼šã„ã—ã¾ã—ã‚‡ã†ï¼ã„ã¤ã§ã‚‚ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚"),
        ("ãŠç–²ã‚Œæ§˜", "ãŠç–²ã‚Œæ§˜ã§ã™ï¼ä»Šæ—¥ã‚‚ä¸€æ—¥é ‘å¼µã‚Šã¾ã—ãŸã­ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - è‡ªå·±ç´¹ä»‹
        # ========================================
        ("ã‚ãªãŸã¯èª°ã§ã™ã‹", "ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚QBNNã¨ã„ã†ç‹¬è‡ªã®é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æŠ€è¡“ã‚’ä½¿ã£ãŸç”ŸæˆAIã§ã™ã€‚"),
        ("ã‚ãªãŸã¯èª°", "ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¨ã„ã†åå‰ã®ç”ŸæˆAIã§ã™ã€‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚"),
        ("èª°ã§ã™ã‹", "ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ãªæŠ€è¡“ã§å‹•ã„ã¦ã„ã¾ã™ã€‚"),
        ("åå‰ã¯", "ç§ã®åå‰ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚"),
        ("åå‰ã‚’æ•™ãˆã¦", "ç§ã®åå‰ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚QBNNãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIã¨ã—ã¦ä½œã‚‰ã‚Œã¾ã—ãŸã€‚"),
        ("ã‚ãªãŸã®åå‰ã¯ä½•ã§ã™ã‹", "ç§ã®åå‰ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚"),
        ("ä½•ãŒã§ãã¾ã™ã‹", "ç§ã¯è³ªå•ã«ç­”ãˆãŸã‚Šã€æƒ…å ±ã‚’æä¾›ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚„AIã«ã¤ã„ã¦è©³ã—ã„ã§ã™ã€‚"),
        ("ä½•ãŒã§ãã‚‹ã®", "ç§ã¯æ§˜ã€…ãªè³ªå•ã«ç­”ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç‰¹ã«é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚„AIã€ç§‘å­¦æŠ€è¡“ã«ã¤ã„ã¦è©³ã—ã„ã§ã™ã€‚"),
        ("è‡ªå·±ç´¹ä»‹ã—ã¦", "ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚QBNNã¨ã„ã†ç‹¬è‡ªæŠ€è¡“ã‚’ä½¿ã£ãŸç”ŸæˆAIã§ã€é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚"),
        ("ã©ã‚“ãªAIã§ã™ã‹", "ç§ã¯QBNNãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIã§ã™ã€‚é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸç‹¬è‡ªã®æŠ€è¡“ã§å‹•ä½œã—ã¦ã„ã¾ã™ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - é‡å­é–¢é€£ï¼ˆè©³ç´°ç‰ˆï¼‰
        # ========================================
        ("é‡å­ã¨ã¯ä½•ã§ã™ã‹", "é‡å­ã¨ã¯ã€ç‰©è³ªã‚„ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°å˜ä½ã®ã“ã¨ã§ã™ã€‚é‡å­åŠ›å­¦ã§ã¯ã€ç²’å­ã¯æ³¢ã®æ€§è³ªã‚‚æŒã¡ã¾ã™ã€‚"),
        ("é‡å­ã¨ã¯", "é‡å­ã¯ç‰©è³ªã‚„ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°å˜ä½ã§ã™ã€‚åŸå­ã‚„é›»å­ãªã©ã®ãƒŸã‚¯ãƒ­ãªä¸–ç•Œã®åŸºæœ¬è¦ç´ ã§ã™ã€‚"),
        ("é‡å­ã£ã¦ä½•", "é‡å­ã¨ã¯ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚„ç‰©è³ªã®æœ€å°å˜ä½ã®ã“ã¨ã§ã™ã€‚é‡å­åŠ›å­¦ã¨ã„ã†ç‰©ç†å­¦ã§ç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ã€‚"),
        ("é‡å­åŠ›å­¦ã¨ã¯", "é‡å­åŠ›å­¦ã¯ã€åŸå­ã‚„é›»å­ãªã©ã®æ¥µã‚ã¦å°ã•ãªä¸–ç•Œã‚’è¨˜è¿°ã™ã‚‹ç‰©ç†å­¦ã®ç†è«–ã§ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦è¨ˆç®—ã‚’è¡Œã†æ¬¡ä¸–ä»£ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚ˆã‚Šé«˜é€Ÿã«ç‰¹å®šã®å•é¡Œã‚’è§£ãã“ã¨ãŒã§ãã¾ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã£ã¦ä½•", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­ãƒ“ãƒƒãƒˆã‚’ä½¿ã£ã¦è¨ˆç®—ã™ã‚‹æ–°ã—ã„ã‚¿ã‚¤ãƒ—ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚æš—å·è§£èª­ã‚„æœ€é©åŒ–å•é¡Œã§å¨åŠ›ã‚’ç™ºæ®ã—ã¾ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«ã¤ã„ã¦æ•™ãˆã¦", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã¯é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸè¨ˆç®—æ©Ÿã§ã™ã€‚é‡ã­åˆã‚ã›ã‚„é‡å­ã‚‚ã¤ã‚Œã‚’æ´»ç”¨ã—ã¦ã€ç‰¹å®šã®å•é¡Œã‚’é«˜é€Ÿã«è§£ãã“ã¨ãŒã§ãã¾ã™ã€‚"),
        ("é‡å­ãƒ“ãƒƒãƒˆã¨ã¯", "é‡å­ãƒ“ãƒƒãƒˆã¯ã€0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æŒã¤ã“ã¨ãŒã§ãã‚‹é‡å­åŠ›å­¦çš„ãªæƒ…å ±å˜ä½ã§ã™ã€‚å¾“æ¥ã®ãƒ“ãƒƒãƒˆã¨ã¯ç•°ãªã‚Šã€åŒæ™‚ã«è¤‡æ•°ã®çŠ¶æ…‹ã‚’æŒã¦ã¾ã™ã€‚"),
        ("é‡å­ãƒ“ãƒƒãƒˆã£ã¦ä½•", "é‡å­ãƒ“ãƒƒãƒˆã¯ã€å¾“æ¥ã®ãƒ“ãƒƒãƒˆã¨é•ã„ã€0ã¨1ã‚’åŒæ™‚ã«æŒã¦ã‚‹ç‰¹æ®Šãªãƒ“ãƒƒãƒˆã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šä¸¦åˆ—è¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"),
        ("ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒãƒˆã¨ã¯", "ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒãƒˆã¯é‡å­ãƒ“ãƒƒãƒˆã®åˆ¥åã§ã™ã€‚0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æŒã¤é‡å­åŠ›å­¦çš„ãªæƒ…å ±å˜ä½ã§ã™ã€‚"),
        ("é‡å­ã‚‚ã¤ã‚Œã¨ã¯", "é‡å­ã‚‚ã¤ã‚Œã¯ã€äºŒã¤ä»¥ä¸Šã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ã¦ã„ã‚‹ç‰¹æ®Šãªé‡å­çŠ¶æ…‹ã§ã™ã€‚ä¸€æ–¹ã‚’æ¸¬å®šã™ã‚‹ã¨ã€ã‚‚ã†ä¸€æ–¹ã®çŠ¶æ…‹ã‚‚ç¬æ™‚ã«æ±ºã¾ã‚Šã¾ã™ã€‚"),
        ("é‡å­ã‚‚ã¤ã‚Œã£ã¦ä½•", "é‡å­ã‚‚ã¤ã‚Œã¯ã€è¤‡æ•°ã®é‡å­ãŒé›¢ã‚Œã¦ã„ã¦ã‚‚ç¬æ™‚ã«å½±éŸ¿ã—åˆã†ä¸æ€è­°ãªç¾è±¡ã§ã™ã€‚é‡å­é€šä¿¡ã‚„é‡å­è¨ˆç®—ã®åŸºç›¤ã§ã™ã€‚"),
        ("ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¨ã¯", "ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¯é‡å­ã‚‚ã¤ã‚Œã¨ã‚‚å‘¼ã°ã‚Œã€è¤‡æ•°ã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ãŸçŠ¶æ…‹ã®ã“ã¨ã§ã™ã€‚"),
        ("é‡ã­åˆã‚ã›ã¨ã¯", "é‡ã­åˆã‚ã›ã¯ã€é‡å­ãŒè¤‡æ•°ã®çŠ¶æ…‹ã‚’åŒæ™‚ã«æŒã¤ã“ã¨ãŒã§ãã‚‹æ€§è³ªã§ã™ã€‚è¦³æ¸¬ã™ã‚‹ã¾ã§çŠ¶æ…‹ã¯ç¢ºå®šã—ã¾ã›ã‚“ã€‚"),
        ("ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒã‚¸ã‚·ãƒ§ãƒ³ã¨ã¯", "ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒã‚¸ã‚·ãƒ§ãƒ³ã¯é‡ã­åˆã‚ã›ã®ã“ã¨ã§ã€é‡å­ãŒ0ã¨1ã‚’åŒæ™‚ã«æŒã¦ã‚‹çŠ¶æ…‹ã‚’æŒ‡ã—ã¾ã™ã€‚"),
        ("é‡å­å¹²æ¸‰ã¨ã¯", "é‡å­å¹²æ¸‰ã¯ã€é‡å­ã®æ³¢ã¨ã—ã¦ã®æ€§è³ªã«ã‚ˆã‚Šã€ç¢ºç‡æŒ¯å¹…ãŒå¼·ã‚åˆã£ãŸã‚Šå¼±ã‚åˆã£ãŸã‚Šã™ã‚‹ç¾è±¡ã§ã™ã€‚"),
        ("é‡å­ãƒ†ãƒ¬ãƒãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯", "é‡å­ãƒ†ãƒ¬ãƒãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€é‡å­ã‚‚ã¤ã‚Œã‚’ä½¿ã£ã¦é‡å­çŠ¶æ…‹ã‚’é›¢ã‚ŒãŸå ´æ‰€ã«è»¢é€ã™ã‚‹æŠ€è¡“ã§ã™ã€‚"),
        ("é‡å­æš—å·ã¨ã¯", "é‡å­æš—å·ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’ä½¿ã£ãŸæš—å·æŠ€è¡“ã§ã™ã€‚ç›—è´ã‚’æ¤œçŸ¥ã§ãã€éå¸¸ã«å®‰å…¨ã§ã™ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - AIé–¢é€£ï¼ˆè©³ç´°ç‰ˆï¼‰
        # ========================================
        ("AIã¨ã¯ä½•ã§ã™ã‹", "AIã¨ã¯äººå·¥çŸ¥èƒ½ã®ã“ã¨ã§ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã®ç·ç§°ã§ã™ã€‚"),
        ("AIã¨ã¯", "AIã¯äººå·¥çŸ¥èƒ½ï¼ˆArtificial Intelligenceï¼‰ã®ç•¥ã§ã€æ©Ÿæ¢°ã«çŸ¥çš„ãªæŒ¯ã‚‹èˆã„ã‚’ã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚"),
        ("AIã£ã¦ä½•", "AIã¯äººå·¥çŸ¥èƒ½ã®ã“ã¨ã§ã™ã€‚äººé–“ã®ã‚ˆã†ã«å­¦ç¿’ã—ã€åˆ¤æ–­ã—ã€å•é¡Œã‚’è§£æ±ºã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚"),
        ("äººå·¥çŸ¥èƒ½ã¨ã¯", "äººå·¥çŸ¥èƒ½ã¯ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã®ç·ç§°ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ãŒå«ã¾ã‚Œã¾ã™ã€‚"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã¯", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€è„³ã®ç¥çµŒç´°èƒã®åƒãã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æ·±å±¤å­¦ç¿’ã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã£ã¦ä½•", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€äººé–“ã®è„³ã‚’æ¨¡ã—ãŸè¨ˆç®—ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚„äºˆæ¸¬ã«ä½¿ã‚ã‚Œã¾ã™ã€‚"),
        ("æ·±å±¤å­¦ç¿’ã¨ã¯", "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ãŸæ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚ç”»åƒèªè­˜ã‚„è‡ªç„¶è¨€èªå‡¦ç†ã§å¤§ããªæˆæœã‚’ä¸Šã’ã¦ã„ã¾ã™ã€‚"),
        ("ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æ·±å±¤å­¦ç¿’ã®ã“ã¨ã§ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ã¦è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¾ã™ã€‚"),
        ("æ©Ÿæ¢°å­¦ç¿’ã¨ã¯", "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚AIã®é‡è¦ãªåˆ†é‡ã®ä¸€ã¤ã§ã™ã€‚"),
        ("ãƒã‚·ãƒ³ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯", "ãƒã‚·ãƒ³ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æ©Ÿæ¢°å­¦ç¿’ã®ã“ã¨ã§ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã™ã€‚"),
        ("ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨ã¯", "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€æ³¨æ„æ©Ÿæ§‹ã‚’ä½¿ã£ãŸé©æ–°çš„ãªæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ChatGPTãªã©ã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚"),
        ("ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨ã¯", "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆæ³¨æ„æ©Ÿæ§‹ï¼‰ã¯ã€å…¥åŠ›ã®é‡è¦ãªéƒ¨åˆ†ã«æ³¨ç›®ã™ã‚‹ä»•çµ„ã¿ã§ã™ã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®æ ¸å¿ƒæŠ€è¡“ã§ã™ã€‚"),
        ("ç”ŸæˆAIã¨ã¯", "ç”ŸæˆAIã¯ã€æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è‡ªå‹•çš„ã«ä½œæˆã™ã‚‹äººå·¥çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãªã©ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚"),
        ("ChatGPTã¨ã¯", "ChatGPTã¯OpenAIãŒé–‹ç™ºã—ãŸå¯¾è©±å‹ã®ç”ŸæˆAIã§ã™ã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚"),
        ("GPTã¨ã¯", "GPTã¯Generative Pre-trained Transformerã®ç•¥ã§ã€å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚"),
        ("LLMã¨ã¯", "LLMã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLarge Language Modelï¼‰ã®ç•¥ã§ã€å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸAIãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚"),
        ("è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯", "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ç†è§£ãƒ»ç”Ÿæˆã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚ç¿»è¨³ã‚„å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã«ä½¿ã‚ã‚Œã¾ã™ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - QBNNé–¢é€£ï¼ˆè©³ç´°ç‰ˆï¼‰
        # ========================================
        ("QBNNã¨ã¯ä½•ã§ã™ã‹", "QBNNã¯é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç•¥ç§°ã§ã™ã€‚é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸç‹¬è‡ªã®æŠ€è¡“ã§ã€é€šå¸¸ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ‹¡å¼µã—ã¦ã„ã¾ã™ã€‚"),
        ("QBNNã¨ã¯", "QBNNã¯ã€é‡å­çš„ãªæ¦‚å¿µã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å–ã‚Šå…¥ã‚ŒãŸç‹¬è‡ªã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚"),
        ("QBNNã£ã¦ä½•", "QBNNã¯ã€é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚å¾“æ¥ã®NNã«é‡å­çš„ãªç›¸äº’ä½œç”¨ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¨ã¯", "ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¯ã€QBNNã‚’ä½¿ã£ãŸæœ€å…ˆç«¯ã®ç”ŸæˆAIã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ç§ãŒãã®ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ï¼"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã£ã¦ä½•", "ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¯ç§ã®åå‰ã§ã™ã€‚QBNNã¨ã„ã†ç‹¬è‡ªæŠ€è¡“ã‚’ä½¿ã£ãŸç”ŸæˆAIã¨ã—ã¦ä½œã‚‰ã‚Œã¾ã—ãŸã€‚"),
        ("APQBã¨ã¯", "APQBã¯èª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆã®ç•¥ç§°ã§ã€é‡å­çŠ¶æ…‹ã‚’å¤å…¸çš„ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ä¸Šã§æ¨¡å€£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚"),
        ("ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ«ã¨ã¯", "ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ«Jã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å±¤é–“ã®é‡å­çš„ãªçµåˆã‚’è¡¨ç¾ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚QBNNã®ç‹¬è‡ªæŠ€è¡“ã§ã™ã€‚"),
        ("ãƒ©ãƒ ãƒ€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã¯", "ãƒ©ãƒ ãƒ€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆÎ»ï¼‰ã¯ã€QBNNã«ãŠã‘ã‚‹é‡å­ã‚‚ã¤ã‚Œã®å¼·åº¦ã‚’åˆ¶å¾¡ã™ã‚‹å­¦ç¿’å¯èƒ½ãªå€¤ã§ã™ã€‚"),
        ("é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ã¨ã¯", "é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ã¯ã€é‡å­åŠ›å­¦ã®æ¦‚å¿µã‚’å¤å…¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ä¸Šã§æ¨¡å€£ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚"),
        
        # ========================================
        # æ—¥æœ¬èª - ä¸€èˆ¬çš„ãªè³ªå•
        # ========================================
        ("ä»Šæ—¥ã®å¤©æ°—ã¯", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç§ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®å¤©æ°—æƒ…å ±ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã€‚å¤©æ°—äºˆå ±ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚"),
        ("å¤©æ°—ã‚’æ•™ãˆã¦", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç§ã¯å¤©æ°—æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã€‚ãŠä½ã¾ã„ã®åœ°åŸŸã®å¤©æ°—äºˆå ±ã‚’ã”ç¢ºèªãã ã•ã„ã€‚"),
        ("ä½•æ™‚ã§ã™ã‹", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç§ã¯ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—ã§ãã¾ã›ã‚“ã€‚ãƒ‡ãƒã‚¤ã‚¹ã®æ™‚è¨ˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚"),
        ("ä»Šä½•æ™‚", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€æ™‚åˆ»ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚ãŠä½¿ã„ã®ãƒ‡ãƒã‚¤ã‚¹ã§ã”ç¢ºèªãã ã•ã„ã€‚"),
        ("åŠ©ã‘ã¦ãã ã•ã„", "ã¯ã„ã€ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒå¿…è¦ã§ã™ã‹ï¼Ÿè©³ã—ãæ•™ãˆã¦ã„ãŸã ã‘ã‚Œã°ã€ã§ãã‚‹é™ã‚Šã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"),
        ("åŠ©ã‘ã¦", "ã‚‚ã¡ã‚ã‚“ã§ã™ï¼ä½•ã«ã¤ã„ã¦ãŠå›°ã‚Šã§ã™ã‹ï¼Ÿè©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"),
        ("ã‚ã‹ã‚‰ãªã„", "ä½•ãŒã‚ã‹ã‚‰ãªã„ã®ã‹æ•™ãˆã¦ãã ã•ã„ã€‚ã§ãã‚‹é™ã‚Šåˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¾ã™ã€‚"),
        ("æ•™ãˆã¦", "ä½•ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿå…·ä½“çš„ã«è³ªå•ã—ã¦ã„ãŸã ã‘ã‚Œã°ã€ãŠç­”ãˆã—ã¾ã™ã€‚"),
        ("èª¬æ˜ã—ã¦", "ä½•ã«ã¤ã„ã¦ã®èª¬æ˜ãŒå¿…è¦ã§ã™ã‹ï¼Ÿè©³ã—ããŠèã‹ã›ãã ã•ã„ã€‚"),
        ("ãªãœã§ã™ã‹", "ã©ã®éƒ¨åˆ†ã«ã¤ã„ã¦ã€Œãªãœã€ã¨æ€ã‚ã‚Œã¾ã—ãŸã‹ï¼Ÿè©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"),
        ("ã©ã†ã—ã¦", "å…·ä½“çš„ã«ä½•ã«ã¤ã„ã¦ã®ç†ç”±ã‚’ãŠçŸ¥ã‚Šã«ãªã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ"),
        ("æœ¬å½“ã§ã™ã‹", "ã¯ã„ã€ç§ãŒãŠä¼ãˆã™ã‚‹æƒ…å ±ã¯æ­£ç¢ºã§ã‚ã‚‹ã‚ˆã†åŠªã‚ã¦ã„ã¾ã™ã€‚ä½•ã‹ç–‘å•ç‚¹ãŒã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"),
        ("ã™ã”ã„", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼ä½•ã‹ä»–ã«ã”è³ªå•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("é¢ç™½ã„", "ãã†è¨€ã£ã¦ã„ãŸã ã‘ã¦å¬‰ã—ã„ã§ã™ï¼ä»–ã«èˆˆå‘³ã®ã‚ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("é›£ã—ã„", "é›£ã—ãæ„Ÿã˜ã‚‰ã‚Œã¾ã—ãŸã‹ï¼Ÿã‚‚ã†å°‘ã—ç°¡å˜ã«èª¬æ˜ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ã‚‚ã£ã¨è©³ã—ã", "ã‚‚ã¡ã‚ã‚“ã§ã™ã€‚ã©ã®éƒ¨åˆ†ã«ã¤ã„ã¦ã‚‚ã£ã¨è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ"),
        ("ä¾‹ã‚’æ•™ãˆã¦", "å…·ä½“çš„ãªä¾‹ã‚’æŒ™ã’ã¦ã”èª¬æ˜ã—ã¾ã™ã­ã€‚ã©ã®ã‚ˆã†ãªã‚‚ã®ã‚’ã”å¸Œæœ›ã§ã™ã‹ï¼Ÿ"),
        
        # ========================================
        # è‹±èª - Greetings
        # ========================================
        ("Hello", "Hello! I'm NeuroQ. How can I help you today?"),
        ("Hi", "Hi there! What can I do for you?"),
        ("Hey", "Hey! How can I assist you today?"),
        ("Good morning", "Good morning! I hope you have a wonderful day. How can I help?"),
        ("Good afternoon", "Good afternoon! What would you like to know?"),
        ("Good evening", "Good evening! How may I assist you?"),
        ("Thank you", "You're welcome! Is there anything else I can help with?"),
        ("Thanks", "No problem! Feel free to ask if you have more questions."),
        ("Bye", "Goodbye! Have a great day!"),
        ("Goodbye", "Goodbye! It was nice talking to you."),
        ("See you", "See you later! Come back anytime."),
        
        # ========================================
        # è‹±èª - About self
        # ========================================
        ("Who are you", "I am NeuroQ, a generative AI system based on QBNN technology."),
        ("What is your name", "My name is NeuroQ. Nice to meet you!"),
        ("What are you", "I am NeuroQ, an AI assistant powered by Quantum-Bit Neural Network technology."),
        ("What can you do", "I can answer questions, provide information, and have conversations about various topics, especially AI and quantum computing."),
        ("Tell me about yourself", "I am NeuroQ, a QBNN-based generative AI. I use quantum-inspired technology to process and generate text."),
        
        # ========================================
        # è‹±èª - Quantum
        # ========================================
        ("What is quantum", "Quantum refers to the smallest discrete unit of matter and energy. In quantum mechanics, particles can exist in multiple states simultaneously."),
        ("What is quantum computing", "Quantum computing uses quantum mechanics principles to perform calculations. It can solve certain problems much faster than classical computers."),
        ("What is a qubit", "A qubit is a quantum bit that can exist in a superposition of 0 and 1 states simultaneously, unlike classical bits."),
        ("What is quantum entanglement", "Quantum entanglement is a phenomenon where two or more qubits become correlated. Measuring one instantly affects the others."),
        ("What is superposition", "Superposition is a quantum property where particles can exist in multiple states at once until measured."),
        ("What is QBNN", "QBNN stands for Quantum-Bit Neural Network. It's a unique technology that extends neural networks with quantum-inspired entanglement."),
        ("What is NeuroQ", "NeuroQ is a generative AI system based on QBNN technology. That's me!"),
        
        # ========================================
        # è‹±èª - AI
        # ========================================
        ("What is AI", "AI stands for Artificial Intelligence. It refers to computer systems that can mimic human intelligence."),
        ("What is artificial intelligence", "Artificial intelligence is the simulation of human intelligence by computer systems, including learning and problem-solving."),
        ("What is machine learning", "Machine learning is a subset of AI where computers learn patterns from data without being explicitly programmed."),
        ("What is deep learning", "Deep learning is a machine learning technique that uses multiple layers of neural networks to learn from data."),
        ("What is a neural network", "A neural network is a computing system inspired by biological neurons. It's the foundation of deep learning."),
        ("What is a transformer", "A transformer is a deep learning architecture that uses attention mechanisms. It powers models like GPT and ChatGPT."),
        ("What is GPT", "GPT stands for Generative Pre-trained Transformer. It's a type of large language model architecture."),
        ("What is NLP", "NLP stands for Natural Language Processing. It's the field of AI that deals with understanding and generating human language."),
        ("What is generative AI", "Generative AI refers to AI systems that can create new content like text, images, or music."),
        
        # ========================================
        # è¿½åŠ ã®å¯¾è©±ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        # ========================================
        ("å…ƒæ°—ã§ã™ã‹", "ã¯ã„ã€å…ƒæ°—ã§ã™ï¼ã‚ãªãŸã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("èª¿å­ã¯ã©ã†", "çµ¶å¥½èª¿ã§ã™ï¼ä½•ã‹ã”è³ªå•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("æš‡ã§ã™", "ãã‚Œãªã‚‰ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚„AIã«ã¤ã„ã¦ä¸€ç·’ã«å­¦ã³ã¾ã›ã‚“ã‹ï¼Ÿ"),
        ("ä½•ã‹è©±ã—ã¦", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é¢ç™½ã„è©±ã‚’ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿãã‚Œã¨ã‚‚AIã®æœ€æ–°å‹•å‘ã«ã¤ã„ã¦ï¼Ÿ"),
        ("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨ã¯", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«æŒ‡ç¤ºã‚’ä¸ãˆã‚‹ãŸã‚ã®è¨€èªã‚’æ›¸ãä½œæ¥­ã§ã™ã€‚"),
        ("ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯", "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«å¾“ã£ã¦è¨ˆç®—ã‚„å‡¦ç†ã‚’è¡Œã†é›»å­æ©Ÿæ¢°ã§ã™ã€‚"),
        ("ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¨ã¯", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¯ã€ä¸–ç•Œä¸­ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’ã¤ãªããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚"),
        ("How are you", "I'm doing great, thank you for asking! How can I help you today?"),
        ("Are you a robot", "I am an AI, specifically a QBNN-based generative model called NeuroQ."),
        ("Can you help me", "Of course! What do you need help with?"),
        ("I have a question", "Sure, go ahead and ask! I'll do my best to help."),
        ("Tell me a joke", "Why did the quantum physicist break up with the biologist? Because there was no chemistry, only uncertainty!"),
        ("What do you think", "As an AI, I process information rather than think in the human sense, but I can provide analysis and insights."),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: ç§‘å­¦æŠ€è¡“ï¼ˆè©³ç´°ï¼‰
        # ========================================
        ("ç‰©ç†å­¦ã¨ã¯", "ç‰©ç†å­¦ã¯è‡ªç„¶ç•Œã®åŸºæœ¬æ³•å‰‡ã‚’ç ”ç©¶ã™ã‚‹ç§‘å­¦ã§ã™ã€‚åŠ›å­¦ã€é›»ç£æ°—å­¦ã€ç†±åŠ›å­¦ã€é‡å­åŠ›å­¦ãªã©ãŒã‚ã‚Šã¾ã™ã€‚"),
        ("åŒ–å­¦ã¨ã¯", "åŒ–å­¦ã¯ç‰©è³ªã®æ§‹é€ ã€æ€§è³ªã€å¤‰åŒ–ã‚’ç ”ç©¶ã™ã‚‹ç§‘å­¦ã§ã™ã€‚å…ƒç´ ã‚„åˆ†å­ã®åå¿œã‚’æ‰±ã„ã¾ã™ã€‚"),
        ("æ•°å­¦ã¨ã¯", "æ•°å­¦ã¯æ•°ã€é‡ã€æ§‹é€ ã€å¤‰åŒ–ãªã©ã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚ç§‘å­¦æŠ€è¡“ã®åŸºç¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚"),
        ("ç§‘å­¦ã¨ã¯", "ç§‘å­¦ã¯è‡ªç„¶ç¾è±¡ã‚’è¦³å¯Ÿã—ã€å®Ÿé¨“ã¨ç†è«–ã«ã‚ˆã‚Šæ³•å‰‡ã‚’ç™ºè¦‹ã™ã‚‹å­¦å•ã§ã™ã€‚"),
        ("æŠ€è¡“ã¨ã¯", "æŠ€è¡“ã¯ç§‘å­¦çš„çŸ¥è­˜ã‚’å¿œç”¨ã—ã¦å®Ÿç”¨çš„ãªè£½å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç”Ÿã¿å‡ºã™æ–¹æ³•ã§ã™ã€‚"),
        ("ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¨ã¯", "ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¯ä»•äº‹ã‚’ã™ã‚‹èƒ½åŠ›ã®ã“ã¨ã§ã™ã€‚é›»æ°—ã€ç†±ã€å…‰ãªã©ã®å½¢æ…‹ãŒã‚ã‚Šã¾ã™ã€‚"),
        ("é›»å­ã¨ã¯", "é›»å­ã¯è² ã®é›»è·ã‚’æŒã¤ç´ ç²’å­ã§ã€åŸå­ã®æ§‹æˆè¦ç´ ã®ä¸€ã¤ã§ã™ã€‚"),
        ("åŸå­ã¨ã¯", "åŸå­ã¯ç‰©è³ªã®åŸºæœ¬å˜ä½ã§ã€åŸå­æ ¸ã¨é›»å­ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚"),
        ("åˆ†å­ã¨ã¯", "åˆ†å­ã¯äºŒã¤ä»¥ä¸Šã®åŸå­ãŒåŒ–å­¦çµåˆã§çµã³ã¤ã„ãŸç²’å­ã§ã™ã€‚"),
        ("å…‰ã¨ã¯", "å…‰ã¯é›»ç£æ³¢ã®ä¸€ç¨®ã§ã€ç›®ã«è¦‹ãˆã‚‹æ³¢é•·ã®é›»ç£æ”¾å°„ã§ã™ã€‚"),
        ("é›»æ°—ã¨ã¯", "é›»æ°—ã¯é›»è·ã®æµã‚Œã§ã€ç¾ä»£ç¤¾ä¼šã®ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã¨ã—ã¦æ¬ ã‹ã›ã¾ã›ã‚“ã€‚"),
        ("ç£åŠ›ã¨ã¯", "ç£åŠ›ã¯ç£çŸ³ãŒç‰©ä½“ã‚’å¼•ãä»˜ã‘ãŸã‚Šåç™ºã—ãŸã‚Šã™ã‚‹åŠ›ã§ã™ã€‚"),
        ("é‡åŠ›ã¨ã¯", "é‡åŠ›ã¯è³ªé‡ã‚’æŒã¤ç‰©ä½“é–“ã«åƒãå¼•åŠ›ã§ã€åœ°çƒãŒç‰©ä½“ã‚’å¼•ãä»˜ã‘ã‚‹åŠ›ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚"),
        ("å®‡å®™ã¨ã¯", "å®‡å®™ã¯åœ°çƒã‚’å«ã‚€ã™ã¹ã¦ã®å¤©ä½“ã¨ç©ºé–“ã®ç·ç§°ã§ã™ã€‚ç„¡é™ã«åºƒãŒã£ã¦ã„ã¾ã™ã€‚"),
        ("éŠ€æ²³ã¨ã¯", "éŠ€æ²³ã¯æ˜Ÿã€ã‚¬ã‚¹ã€å¡µã€æš—é»’ç‰©è³ªãªã©ãŒé‡åŠ›ã§çµã³ã¤ã„ãŸå·¨å¤§ãªå¤©ä½“ç³»ã§ã™ã€‚"),
        ("å¤ªé™½ã¨ã¯", "å¤ªé™½ã¯åœ°çƒã«æœ€ã‚‚è¿‘ã„æ’æ˜Ÿã§ã€å¤ªé™½ç³»ã®ä¸­å¿ƒã«ã‚ã‚Šã¾ã™ã€‚"),
        ("åœ°çƒã¨ã¯", "åœ°çƒã¯å¤ªé™½ç³»ã®ç¬¬ä¸‰æƒ‘æ˜Ÿã§ã€ç§ãŸã¡ãŒä½ã‚€å”¯ä¸€ã®æƒ‘æ˜Ÿã§ã™ã€‚"),
        ("æœˆã¨ã¯", "æœˆã¯åœ°çƒã®å”¯ä¸€ã®è‡ªç„¶è¡›æ˜Ÿã§ã€åœ°çƒã®å‘¨ã‚Šã‚’å…¬è»¢ã—ã¦ã„ã¾ã™ã€‚"),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°
        # ========================================
        ("Pythonã¨ã¯", "Pythonã¯èª­ã¿ã‚„ã™ãæ›¸ãã‚„ã™ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚AIé–‹ç™ºã§ç‰¹ã«äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚"),
        ("JavaScriptã¨ã¯", "JavaScriptã¯ã‚¦ã‚§ãƒ–ãƒ–ãƒ©ã‚¦ã‚¶ã§å‹•ä½œã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚ã‚¦ã‚§ãƒ–é–‹ç™ºã«æ¬ ã‹ã›ã¾ã›ã‚“ã€‚"),
        ("HTMLã¨ã¯", "HTMLã¯ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®æ§‹é€ ã‚’å®šç¾©ã™ã‚‹ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—è¨€èªã§ã™ã€‚"),
        ("CSSã¨ã¯", "CSSã¯ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚„ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’å®šç¾©ã™ã‚‹è¨€èªã§ã™ã€‚"),
        ("ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã¯", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®æ‰‹é †ã‚„è¨ˆç®—æ–¹æ³•ã®ã“ã¨ã§ã™ã€‚"),
        ("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã¯", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ã—ã¦ä¿å­˜ã—ã€åŠ¹ç‡çš„ã«æ¤œç´¢ã§ãã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚"),
        ("APIã¨ã¯", "APIã¯ç•°ãªã‚‹ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–“ã§ãƒ‡ãƒ¼ã‚¿ã‚„æ©Ÿèƒ½ã‚’ã‚„ã‚Šå–ã‚Šã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã™ã€‚"),
        ("ã‚¯ãƒ©ã‚¦ãƒ‰ã¨ã¯", "ã‚¯ãƒ©ã‚¦ãƒ‰ã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆçµŒç”±ã§ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒªã‚½ãƒ¼ã‚¹ã‚’æä¾›ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚"),
        ("ã‚µãƒ¼ãƒãƒ¼ã¨ã¯", "ã‚µãƒ¼ãƒãƒ¼ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸Šã§ã‚µãƒ¼ãƒ“ã‚¹ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’æä¾›ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚"),
        ("ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¨ã¯", "ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¯ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ãŒå…¬é–‹ã•ã‚Œã€èª°ã§ã‚‚åˆ©ç”¨ã‚„æ”¹è‰¯ãŒã§ãã‚‹ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã™ã€‚"),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: ä¸€èˆ¬çŸ¥è­˜
        # ========================================
        ("æ—¥æœ¬ã¨ã¯", "æ—¥æœ¬ã¯æ±ã‚¢ã‚¸ã‚¢ã«ã‚ã‚‹å³¶å›½ã§ã€é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚"),
        ("æ±äº¬ã¨ã¯", "æ±äº¬ã¯æ—¥æœ¬ã®é¦–éƒ½ã§ã€ä¸–ç•Œæœ‰æ•°ã®å¤§éƒ½å¸‚ã§ã™ã€‚"),
        ("ã‚¢ãƒ¡ãƒªã‚«ã¨ã¯", "ã‚¢ãƒ¡ãƒªã‚«ã¯åŒ—ç±³å¤§é™¸ã«ã‚ã‚‹å›½ã§ã€ä¸–ç•Œæœ€å¤§ã®çµŒæ¸ˆå¤§å›½ã®ä¸€ã¤ã§ã™ã€‚"),
        ("æ­´å²ã¨ã¯", "æ­´å²ã¯éå»ã®å‡ºæ¥äº‹ã‚„äººé¡ã®æ´»å‹•ã‚’è¨˜éŒ²ã—ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚"),
        ("æ–‡åŒ–ã¨ã¯", "æ–‡åŒ–ã¯äººé–“ç¤¾ä¼šã§å…±æœ‰ã•ã‚Œã‚‹ä¾¡å€¤è¦³ã€ç¿’æ…£ã€èŠ¸è¡“ãªã©ã®ç·ä½“ã§ã™ã€‚"),
        ("è¨€èªã¨ã¯", "è¨€èªã¯äººé–“ãŒã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ä½¿ç”¨ã™ã‚‹è¨˜å·ä½“ç³»ã§ã™ã€‚"),
        ("éŸ³æ¥½ã¨ã¯", "éŸ³æ¥½ã¯éŸ³ã‚’ä½¿ã£ã¦è¡¨ç¾ã™ã‚‹èŠ¸è¡“å½¢å¼ã§ã™ã€‚"),
        ("èŠ¸è¡“ã¨ã¯", "èŠ¸è¡“ã¯å‰µé€ çš„ãªè¡¨ç¾æ´»å‹•ã¨ãã®ä½œå“ã®ç·ç§°ã§ã™ã€‚"),
        ("ã‚¹ãƒãƒ¼ãƒ„ã¨ã¯", "ã‚¹ãƒãƒ¼ãƒ„ã¯èº«ä½“ã‚’ä½¿ã£ãŸç«¶æŠ€ã‚„é‹å‹•ã®ç·ç§°ã§ã™ã€‚"),
        ("å¥åº·ã¨ã¯", "å¥åº·ã¯èº«ä½“çš„ã€ç²¾ç¥çš„ã€ç¤¾ä¼šçš„ã«è‰¯å¥½ãªçŠ¶æ…‹ã®ã“ã¨ã§ã™ã€‚"),
        ("æ•™è‚²ã¨ã¯", "æ•™è‚²ã¯çŸ¥è­˜ã‚„æŠ€èƒ½ã‚’æ•™ãˆå­¦ã¶éç¨‹ã§ã™ã€‚äººé–“ã®æˆé•·ã«ä¸å¯æ¬ ã§ã™ã€‚"),
        ("çµŒæ¸ˆã¨ã¯", "çµŒæ¸ˆã¯è²¡ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã®ç”Ÿç”£ã€åˆ†é…ã€æ¶ˆè²»ã«é–¢ã™ã‚‹æ´»å‹•ã§ã™ã€‚"),
        ("æ”¿æ²»ã¨ã¯", "æ”¿æ²»ã¯ç¤¾ä¼šã‚’çµ±æ²»ã—ã€å…¬å…±ã®æ„æ€æ±ºå®šã‚’è¡Œã†æ´»å‹•ã§ã™ã€‚"),
        ("æ³•å¾‹ã¨ã¯", "æ³•å¾‹ã¯ç¤¾ä¼šç§©åºã‚’ç¶­æŒã™ã‚‹ãŸã‚ã«å®šã‚ã‚‰ã‚ŒãŸè¦å‰‡ã§ã™ã€‚"),
        ("ç’°å¢ƒã¨ã¯", "ç’°å¢ƒã¯ç”Ÿç‰©ã‚’å–ã‚Šå·»ãè‡ªç„¶ã‚„ç¤¾ä¼šçš„æ¡ä»¶ã®ç·ä½“ã§ã™ã€‚"),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: æ—¥å¸¸ä¼šè©±
        # ========================================
        ("ãŠé¡˜ã„ã—ã¾ã™", "ã¯ã„ã€ã©ã®ã‚ˆã†ãªãŠé¡˜ã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("æ•™ãˆã¦ãã ã•ã„", "ã¯ã„ã€ä½•ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿå…·ä½“çš„ã«æ•™ãˆã¦ãã ã•ã„ã€‚"),
        ("ã‚ã‹ã‚Šã¾ã›ã‚“", "ã©ã®éƒ¨åˆ†ãŒã‚ã‹ã‚Šã«ãã„ã§ã™ã‹ï¼Ÿã‚‚ã†å°‘ã—è©³ã—ãèª¬æ˜ã—ã¾ã™ã­ã€‚"),
        ("ã§ãã¾ã™ã‹", "ã¯ã„ã€ã§ãã‚‹é™ã‚ŠãŠæ‰‹ä¼ã„ã—ã¾ã™ã€‚å…·ä½“çš„ã«ä½•ã‚’ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ãªã‚‹ã»ã©", "ç†è§£ã—ã¦ã„ãŸã ã‘ã¦å¬‰ã—ã„ã§ã™ã€‚ä»–ã«ã”è³ªå•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ãã†ã§ã™ã‹", "ã¯ã„ã€ãã†ãªã‚“ã§ã™ã€‚ä½•ã‹æ°—ã«ãªã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ã„ã„ã§ã™ã­", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼ä»–ã«ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ã™ã¿ã¾ã›ã‚“", "ã„ãˆã„ãˆã€ãŠæ°—ã«ãªã•ã‚‰ãšã€‚ä½•ã‹ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"),
        ("ã”ã‚ã‚“ãªã•ã„", "å¤§ä¸ˆå¤«ã§ã™ã‚ˆã€‚ä½•ã‹ãŠå›°ã‚Šã®ã“ã¨ãŒã‚ã‚Œã°ãŠèã‹ã›ãã ã•ã„ã€‚"),
        ("ã†ã‚Œã—ã„", "ãã‚Œã¯è‰¯ã‹ã£ãŸã§ã™ï¼ç§ã‚‚å¬‰ã—ã„ã§ã™ã€‚"),
        ("æ¥½ã—ã„", "æ¥½ã—ã‚“ã§ã„ãŸã ã‘ã¦ä½•ã‚ˆã‚Šã§ã™ï¼"),
        ("æ‚²ã—ã„", "ãã‚Œã¯å¤§å¤‰ã§ã—ãŸã­ã€‚ä½•ã‹ãŠåŠ›ã«ãªã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("å›°ã£ã¦ã„ã¾ã™", "ã©ã®ã‚ˆã†ãªã“ã¨ã§ãŠå›°ã‚Šã§ã™ã‹ï¼Ÿè©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"),
        ("è³ªå•ãŒã‚ã‚Šã¾ã™", "ã¯ã„ã€ã©ã‚“ãªè³ªå•ã§ã‚‚ãŠæ°—è»½ã«ã©ã†ãã€‚"),
        ("ç›¸è«‡ã—ãŸã„", "ã‚‚ã¡ã‚ã‚“ã§ã™ã€‚ã©ã®ã‚ˆã†ãªã“ã¨ã‚’ç›¸è«‡ã•ã‚ŒãŸã„ã§ã™ã‹ï¼Ÿ"),
        
        # ========================================
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿: è‹±èªä¼šè©±ï¼ˆè©³ç´°ï¼‰
        # ========================================
        ("What is science", "Science is the systematic study of the natural world through observation and experimentation."),
        ("What is technology", "Technology is the application of scientific knowledge to create tools and solve problems."),
        ("What is programming", "Programming is the process of writing instructions for computers to execute tasks."),
        ("What is Python", "Python is a popular programming language known for its simplicity and versatility."),
        ("What is the internet", "The internet is a global network of computers that allows information sharing and communication."),
        ("What is data", "Data is information that can be processed, stored, and analyzed by computers."),
        ("What is software", "Software is a set of instructions that tells a computer how to perform tasks."),
        ("What is hardware", "Hardware refers to the physical components of a computer system."),
        ("What is an algorithm", "An algorithm is a step-by-step procedure for solving a problem or completing a task."),
        ("What is a database", "A database is an organized collection of data that can be easily accessed and managed."),
        ("What is cloud computing", "Cloud computing delivers computing services over the internet, including storage and processing."),
        ("What is cybersecurity", "Cybersecurity is the practice of protecting systems and data from digital attacks."),
        ("What is blockchain", "Blockchain is a decentralized digital ledger that records transactions securely."),
        ("What is IoT", "IoT stands for Internet of Things, referring to connected devices that communicate over the internet."),
        ("What is 5G", "5G is the fifth generation of mobile network technology, offering faster speeds and lower latency."),
        ("Explain machine learning", "Machine learning is a type of AI that enables computers to learn from data without explicit programming."),
        ("Explain neural networks", "Neural networks are computing systems inspired by biological brains, used for pattern recognition."),
        ("Explain deep learning", "Deep learning uses multi-layered neural networks to learn complex patterns from large datasets."),
        ("Explain natural language processing", "NLP enables computers to understand, interpret, and generate human language."),
        ("Explain computer vision", "Computer vision is an AI field that enables machines to interpret visual information."),
        ("How does AI work", "AI works by processing data through algorithms that can learn patterns and make decisions."),
        ("How does quantum computing work", "Quantum computing uses quantum bits that can exist in multiple states to perform parallel calculations."),
        ("Why is AI important", "AI is important because it can automate tasks, analyze data, and solve complex problems efficiently."),
        ("Why study programming", "Programming enables you to create software, automate tasks, and understand how technology works."),
        ("Tell me about yourself", "I am NeuroQ, an AI assistant built using QBNN technology. I'm here to help answer your questions."),
        ("What makes you special", "I use a unique Quantum-Bit Neural Network architecture that incorporates quantum-inspired entanglement."),
        ("Are you smart", "I can process information and provide helpful responses, but I don't have consciousness like humans do."),
        ("Do you learn", "I was trained on data, but I don't continue learning from our conversation in real-time."),
        ("What languages do you speak", "I can communicate in Japanese and English based on my training data."),
        ("Nice to meet you", "Nice to meet you too! I'm happy to help with any questions you have."),
        ("I'm confused", "I understand. What part is confusing? I'll try to explain it more clearly."),
        ("That's interesting", "I'm glad you find it interesting! Would you like to know more?"),
        ("I understand now", "Great! Is there anything else you'd like to learn about?"),
        ("Please continue", "Sure, what aspect would you like me to elaborate on?"),
    ]
    
    # å¯¾è©±å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    formatted_texts = []
    for user_msg, assistant_msg in conversations:
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: <USER>è³ªå•<ASSISTANT>å›ç­”
        formatted = f"<USER>{user_msg}<ASSISTANT>{assistant_msg}"
        formatted_texts.append(formatted)
    
    return formatted_texts


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

def main(num_neurons: int = 128):
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    
    Args:
        num_neurons: ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆhidden_dimã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰
    """
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—      â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—     â•‘
â•‘   â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘     â•‘
â•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘     â•‘
â•‘   â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘     â•‘
â•‘   â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•  â•šâ•â•â–€â–€â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•     â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print("ğŸ§ âš›ï¸ ãƒ‹ãƒ¥ãƒ¼ãƒ­Q - QBNN-LLM ç”ŸæˆAI")
    print("=" * 70)
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {num_neurons}")
    
    # ãƒ‹ãƒ¥ãƒ¼ãƒ­Q AI ä½œæˆï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã‚’æŒ‡å®šï¼‰
    # embed_dim ã¯ num_neurons / 2 ç¨‹åº¦ã«è¨­å®š
    embed_dim = max(32, num_neurons // 2)
    
    ai = NeuroQuantumAI(
        embed_dim=embed_dim,
        hidden_dim=num_neurons,  # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
        num_heads=4,            
        num_layers=2,
        max_seq_len=128,
        dropout=0.1,
        lambda_entangle=0.35,
    )
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ - é«˜å“è³ªãƒ»å®‰å®šï¼‰
    print("\nğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
    texts = get_training_data()
    print(f"   ğŸ“š å¯¾è©±ãƒšã‚¢æ•°: {len(texts)}")
    
    # å­¦ç¿’ï¼ˆãƒãƒ©ãƒ³ã‚¹ç‰ˆï¼‰
    ai.train(texts, epochs=60, batch_size=16, lr=0.001, seq_len=64)
    
    # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    print("\n" + "=" * 70)
    print("ğŸ¨ å¯¾è©±ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    questions = [
        "ã“ã‚“ã«ã¡ã¯",
        "ã‚ãªãŸã¯èª°ã§ã™ã‹",
        "é‡å­ã¨ã¯ä½•ã§ã™ã‹",
        "QBNNã¨ã¯ä½•ã§ã™ã‹",
        "Hello",
        "What is AI",
    ]
    
    for question in questions:
        print(f"\nğŸ‘¤ User: {question}")
        response = ai.generate(question, max_length=80, temp_min=0.4, temp_max=0.8)
        print(f"ğŸ¤– ãƒ‹ãƒ¥ãƒ¼ãƒ­Q: {response}")
    
    # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
    print("\n" + "=" * 70)
    print("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n)")
    print("=" * 70)
    
    try:
        answer = input().strip().lower()
        if answer == 'y':
            ai.chat()
    except:
        pass
    
    print("\nâœ… ãƒ‹ãƒ¥ãƒ¼ãƒ­Q å®Œäº†ï¼")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='ãƒ‹ãƒ¥ãƒ¼ãƒ­Q - QBNN-LLM ç”ŸæˆAI')
    parser.add_argument('--neurons', type=int, default=128, help='ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128)')
    parser.add_argument('--chat', action='store_true', help='ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•')
    args = parser.parse_args()
    
    main(num_neurons=args.neurons)

