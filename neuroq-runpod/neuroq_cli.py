#!/usr/bin/env python3
"""
NeuroQ CLI - ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
============================================
å­¦ç¿’ãƒ»ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»ç”Ÿæˆã‚’çµ±åˆã—ãŸCLIãƒ„ãƒ¼ãƒ«

æ©Ÿèƒ½:
- Common Crawl ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
- Wikipedia / ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
- Brain Mode / Layered Mode ã§ã®å­¦ç¿’
- ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

ä½¿ç”¨ä¾‹:
python neuroq_cli.py --prompt "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¤ã„ã¦" --mode brain --train --epochs 30
"""

import argparse
import json
import os
import sys
import random
from typing import List, Dict, Optional

import torch
import torch.nn as nn
import numpy as np

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PARENT_DIR not in sys.path:
    sys.path.insert(0, PARENT_DIR)

from neuroq_model import (
    NeuroQModel, 
    NeuroQConfig, 
    NeuroQTokenizer, 
    NeuroQGenerator,
    create_neuroq_brain,
    create_neuroq_layered,
)


# ========================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—: Common Crawl
# ========================================

def fetch_common_crawl_data(
    domains: List[str] = None,
    max_records: int = 100,
    crawl_id: str = "CC-MAIN-2024-10",
    language: str = "ja",
) -> List[str]:
    """
    Common Crawl ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Args:
        domains: å–å¾—ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒªã‚¹ãƒˆï¼ˆãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰å¯¾å¿œï¼‰
        max_records: æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
        crawl_id: ã‚¯ãƒ­ãƒ¼ãƒ«ID
        language: è¨€èªã‚³ãƒ¼ãƒ‰
    """
    print(f"\nğŸ“¡ Common Crawl ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
    print(f"   Crawl ID: {crawl_id}")
    print(f"   Domains: {domains}")
    print(f"   Max Records: {max_records}")
    
    texts = []
    
    try:
        # warcio ã¨ requests ãŒå¿…è¦
        import requests
        
        # Common Crawl Index API
        # æ³¨: å®Ÿéš›ã® Common Crawl ã‚¢ã‚¯ã‚»ã‚¹ã«ã¯è¿½åŠ ã®è¨­å®šãŒå¿…è¦
        # ã“ã“ã§ã¯ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        
        print("   âš ï¸ Common Crawl ã¸ã®ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã¯è¨­å®šãŒå¿…è¦ã§ã™")
        print("   ğŸ“š ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        # ä»£æ›¿: Hugging Face ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        texts = fetch_huggingface_data(max_samples=max_records, language=language)
        
    except ImportError:
        print("   âš ï¸ requests/warcio ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   ğŸ“š çµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
        texts = get_builtin_training_data()
    except Exception as e:
        print(f"   âš ï¸ Common Crawl å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        texts = get_builtin_training_data()
    
    print(f"   âœ… {len(texts)} ã‚µãƒ³ãƒ—ãƒ«å–å¾—å®Œäº†")
    return texts


# ========================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—: Hugging Face
# ========================================

def fetch_huggingface_data(
    max_samples: int = 500,
    language: str = "ja",
    datasets_list: List[str] = None,
) -> List[str]:
    """Hugging Face ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    print(f"\nğŸ“¡ Hugging Face ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
    
    texts = []
    
    try:
        from datasets import load_dataset
    except ImportError:
        print("   âš ï¸ datasets ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚ã‚Šã¾ã›ã‚“")
        return get_builtin_training_data()
    
    # æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    if language == "ja":
        # Wikipedia æ—¥æœ¬èª
        try:
            print("   ğŸ“š Wikipedia æ—¥æœ¬èª...")
            ds = load_dataset("range3/wiki40b-ja", split="train", streaming=True)
            count = 0
            for item in ds:
                if 'text' in item and len(item['text']) > 50:
                    texts.append(item['text'][:500])
                    count += 1
                    if count >= max_samples // 3:
                        break
            print(f"      âœ… {count} ã‚µãƒ³ãƒ—ãƒ«")
        except Exception as e:
            print(f"      âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ—¥æœ¬èªå¯¾è©±ãƒ‡ãƒ¼ã‚¿
        try:
            print("   ğŸ“š databricks-dolly-15k-ja...")
            ds = load_dataset("kunishou/databricks-dolly-15k-ja", split="train")
            count = 0
            for item in ds:
                if count >= max_samples // 3:
                    break
                if 'output' in item and len(item['output']) > 20:
                    texts.append(item['output'])
                    count += 1
            print(f"      âœ… {count} ã‚µãƒ³ãƒ—ãƒ«")
        except Exception as e:
            print(f"      âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
    
    # è‹±èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    else:
        try:
            print("   ğŸ“š wikitext-2...")
            ds = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
            count = 0
            for item in ds:
                if count >= max_samples:
                    break
                if 'text' in item and len(item['text']) > 30:
                    texts.append(item['text'])
                    count += 1
            print(f"      âœ… {count} ã‚µãƒ³ãƒ—ãƒ«")
        except Exception as e:
            print(f"      âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"   âœ… åˆè¨ˆ {len(texts)} ã‚µãƒ³ãƒ—ãƒ«å–å¾—å®Œäº†")
    return texts


# ========================================
# çµ„ã¿è¾¼ã¿å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
# ========================================

def get_builtin_training_data() -> List[str]:
    """çµ„ã¿è¾¼ã¿ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    
    texts = [
        # é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦æƒ…å ±ã‚’å‡¦ç†ã™ã‚‹é©æ–°çš„ãªè¨ˆç®—æ©Ÿã§ã™ã€‚",
        "é‡å­ãƒ“ãƒƒãƒˆã¯0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®æ€§è³ªã«ã‚ˆã‚Šä¸¦åˆ—è¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚",
        "é‡å­ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¯ã€è¤‡æ•°ã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ãŸçŠ¶æ…‹ã§ã™ã€‚é‡å­é€šä¿¡ã‚„é‡å­æš—å·ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯æš—å·è§£èª­ã‚„æœ€é©åŒ–å•é¡Œã§æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "é‡å­è¶…è¶Šæ€§ã¨ã¯ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒå¤å…¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚é«˜é€Ÿã«è¨ˆç®—ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™æ¦‚å¿µã§ã™ã€‚",
        "é‡å­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯ã€ã‚·ãƒ§ã‚¢ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„ã‚°ãƒ­ãƒ¼ãƒãƒ¼ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãªã©ãŒã‚ã‚Šã¾ã™ã€‚",
        "é‡å­èª¤ã‚Šè¨‚æ­£ã¯ã€é‡å­è¨ˆç®—ã®ä¿¡é ¼æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã®é‡è¦ãªæŠ€è¡“ã§ã™ã€‚",
        "è¶…ä¼å°é‡å­ãƒ“ãƒƒãƒˆã‚„ã‚¤ã‚ªãƒ³ãƒˆãƒ©ãƒƒãƒ—é‡å­ãƒ“ãƒƒãƒˆãªã©ã€æ§˜ã€…ãªå®Ÿè£…æ–¹å¼ãŒç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "é‡å­ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ã¯ã€çµ„ã¿åˆã‚ã›æœ€é©åŒ–å•é¡Œã‚’è§£ããŸã‚ã®é‡å­è¨ˆç®—æ‰‹æ³•ã§ã™ã€‚",
        "é‡å­ãƒ†ãƒ¬ãƒãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€é‡å­æƒ…å ±ã‚’é›¢ã‚ŒãŸå ´æ‰€ã«è»¢é€ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        
        # AIãƒ»æ©Ÿæ¢°å­¦ç¿’
        "äººå·¥çŸ¥èƒ½ã¯ã€äººé–“ã®çŸ¥çš„æ´»å‹•ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å®Ÿç¾ã—ã‚ˆã†ã¨ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã¦æ”¹å–„ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã—ã¾ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã§ã™ã€‚",
        "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã‚’ç”¨ã„ãŸé©æ–°çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚",
        "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®è¨€èªã‚’ç†è§£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
        "ç”ŸæˆAIã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚„ç”»åƒã‚’ç”Ÿæˆã§ãã‚‹äººå·¥çŸ¥èƒ½ã§ã™ã€‚",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
        "å¼·åŒ–å­¦ç¿’ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚",
        
        # QBNNé–¢é€£
        "QBNNã¯é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç•¥ç§°ã§ã™ã€‚",
        "APQBã¯èª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆã§ã€é‡å­çŠ¶æ…‹ã‚’å¤å…¸çš„ã«æ¨¡å€£ã—ã¾ã™ã€‚",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¯ã€QBNNãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
        "é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        
        # å¯¾è©±
        "ã“ã‚“ã«ã¡ã¯ã€ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿè©³ã—ãèª¬æ˜ã—ã¾ã™ã­ã€‚",
        "äººå·¥çŸ¥èƒ½ã®ä»•çµ„ã¿ã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "ä½•ã§ã‚‚èã„ã¦ãã ã•ã„ã€‚ã§ãã‚‹é™ã‚ŠãŠç­”ãˆã—ã¾ã™ã€‚",
    ]
    
    # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
    expanded = texts * 20
    
    prefixes = ["å®Ÿéš›ã«ã€", "èˆˆå‘³æ·±ã„ã“ã¨ã«ã€", "é‡è¦ãªã®ã¯ã€", "ç‰¹ã«ã€", "ã•ã‚‰ã«ã€"]
    for text in texts[:20]:
        for prefix in prefixes:
            expanded.append(prefix + text)
    
    return expanded


# ========================================
# å­¦ç¿’
# ========================================

def train_model(
    texts: List[str],
    mode: str = "brain",
    epochs: int = 30,
    batch_size: int = 16,
    learning_rate: float = 0.001,
    num_neurons: int = 100,
    hidden_dim: int = 256,
    embed_dim: int = 128,
    num_layers: int = 3,
    num_heads: int = 4,
    seq_length: int = 64,
    connection_density: float = 0.25,
    lambda_entangle: float = 0.35,
    save_path: str = None,
) -> tuple:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    
    Returns:
        (model, tokenizer, generator)
    """
    print("\n" + "=" * 60)
    print(f"ğŸ“ NeuroQ å­¦ç¿’é–‹å§‹ (Mode: {mode})")
    print("=" * 60)
    
    # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸ® CUDA GPU ã‚’ä½¿ç”¨: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPU ã‚’ä½¿ç”¨")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰
    print("\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰...")
    tokenizer = NeuroQTokenizer(vocab_size=8000)
    tokenizer.build_vocab(texts)
    print(f"   èªå½™ã‚µã‚¤ã‚º: {tokenizer.actual_vocab_size}")
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    print(f"\nğŸ§  NeuroQ ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ({mode} mode)...")
    
    if mode == "brain":
        config = NeuroQConfig(
            mode='brain',
            vocab_size=tokenizer.actual_vocab_size,
            embed_dim=embed_dim,
            hidden_dim=num_neurons * 2,
            num_neurons=num_neurons,
            num_heads=num_heads,
            num_layers=num_layers,
            max_seq_len=256,
            dropout=0.1,
            connection_density=connection_density,
            lambda_entangle=lambda_entangle,
        )
    else:  # layered
        config = NeuroQConfig(
            mode='layered',
            vocab_size=tokenizer.actual_vocab_size,
            embed_dim=embed_dim,
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=num_layers,
            max_seq_len=256,
            dropout=0.1,
            lambda_entangle=lambda_entangle,
        )
    
    model = NeuroQModel(config).to(device)
    
    print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ§‹æˆ:")
    print(f"   Mode: {mode}")
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embed_dim}")
    if mode == "brain":
        print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {num_neurons}")
        print(f"   æ¥ç¶šå¯†åº¦: {connection_density}")
    else:
        print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {hidden_dim}")
    print(f"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {num_layers}")
    print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_params:,}")
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
    all_tokens = []
    for text in texts:
        tokens = tokenizer.encode(text)
        all_tokens.extend(tokens)
    
    print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens):,}")
    
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
    sequences = []
    for i in range(0, len(all_tokens) - seq_length - 1, seq_length // 2):
        x = all_tokens[i:i+seq_length]
        y = all_tokens[i+1:i+seq_length+1]
        if len(x) == seq_length:
            sequences.append((x, y))
    
    print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences):,}")
    
    # å­¦ç¿’
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    criterion = nn.CrossEntropyLoss()
    
    print(f"\nğŸš€ å­¦ç¿’é–‹å§‹ (Epochs: {epochs}, Batch: {batch_size}, LR: {learning_rate})")
    model.train()
    
    for epoch in range(epochs):
        total_loss = 0
        random.shuffle(sequences)
        
        for i in range(0, len(sequences), batch_size):
            batch = sequences[i:i+batch_size]
            if len(batch) == 0:
                continue
            
            x_batch = torch.tensor([s[0] for s in batch], dtype=torch.long).to(device)
            y_batch = torch.tensor([s[1] for s in batch], dtype=torch.long).to(device)
            
            optimizer.zero_grad()
            logits = model(x_batch)
            
            loss = criterion(
                logits.view(-1, tokenizer.actual_vocab_size),
                y_batch.view(-1)
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        scheduler.step()
        avg_loss = total_loss / max(1, len(sequences) // batch_size)
        
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"   Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}")
    
    print("\nâœ… å­¦ç¿’å®Œäº†ï¼")
    
    # Generator ä½œæˆ
    generator = NeuroQGenerator(model, tokenizer, str(device))
    
    # ä¿å­˜
    if save_path:
        print(f"\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {save_path}")
        model.save_checkpoint(save_path)
        tokenizer.save(save_path.replace('.pt', '_tokenizer.json'))
    
    return model, tokenizer, generator


# ========================================
# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
# ========================================

def generate_text(
    generator: NeuroQGenerator,
    prompt: str,
    max_tokens: int = 100,
    temperature: float = 0.7,
    top_k: int = 40,
    top_p: float = 0.9,
) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    print(f"\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ:")
    print(f"   Prompt: {prompt}")
    print(f"   Max Tokens: {max_tokens}")
    print(f"   Temperature: {temperature}")
    
    output = generator.generate(
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
    )
    
    print(f"\nğŸ¤– ç”Ÿæˆçµæœ:")
    print(f"   {output}")
    
    return output


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

def main():
    parser = argparse.ArgumentParser(
        description='NeuroQ CLI - QBNNç”ŸæˆAI',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # Brain Mode ã§å­¦ç¿’ã—ã¦ç”Ÿæˆ
  python neuroq_cli.py --prompt "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¤ã„ã¦" --mode brain --train --epochs 30
  
  # Common Crawl ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
  python neuroq_cli.py --prompt "AIã¨ã¯" --train --data-source common_crawl
  
  # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã®ã¿
  python neuroq_cli.py --prompt "ã“ã‚“ã«ã¡ã¯" --model-path neuroq_model.pt
        """
    )
    
    # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--prompt', type=str, required=True, help='ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ')
    parser.add_argument('--max-tokens', type=int, default=100, help='æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°')
    parser.add_argument('--temperature', type=float, default=0.7, help='ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦')
    parser.add_argument('--top-k', type=int, default=40, help='Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°')
    parser.add_argument('--top-p', type=float, default=0.9, help='Top-P ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°')
    
    # å­¦ç¿’ãƒ•ãƒ©ã‚°
    parser.add_argument('--train', action='store_true', help='å­¦ç¿’ã‚’å®Ÿè¡Œ')
    parser.add_argument('--train-before-generate', action='store_true', help='ç”Ÿæˆå‰ã«å­¦ç¿’')
    
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
    parser.add_argument('--data-source', type=str, nargs='+', 
                        choices=['common_crawl', 'huggingface', 'builtin'],
                        default=['builtin'], help='ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹')
    parser.add_argument('--common-crawl-domains', type=str, nargs='+',
                        default=['*.wikipedia.org'], help='Common Crawl ãƒ‰ãƒ¡ã‚¤ãƒ³')
    parser.add_argument('--common-crawl-max-records', type=int, default=100,
                        help='Common Crawl æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°')
    parser.add_argument('--common-crawl-id', type=str, default='CC-MAIN-2024-10',
                        help='Common Crawl ID')
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    parser.add_argument('--mode', type=str, choices=['brain', 'layered'], 
                        default='brain', help='ãƒ¢ãƒ‡ãƒ«ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--num-neurons', type=int, default=100, help='ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (Brain Mode)')
    parser.add_argument('--hidden-dim', type=int, default=256, help='éš ã‚Œå±¤æ¬¡å…ƒ (Layered Mode)')
    parser.add_argument('--embed-dim', type=int, default=128, help='åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ')
    parser.add_argument('--num-layers', type=int, default=3, help='ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°')
    parser.add_argument('--num-heads', type=int, default=4, help='ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°')
    parser.add_argument('--connection-density', type=float, default=0.25, help='æ¥ç¶šå¯†åº¦')
    parser.add_argument('--lambda-entangle', type=float, default=0.35, help='ã‚‚ã¤ã‚Œå¼·åº¦')
    
    # å­¦ç¿’è¨­å®š
    parser.add_argument('--epochs', type=int, default=30, help='ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch-size', type=int, default=16, help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--learning-rate', type=float, default=0.001, help='å­¦ç¿’ç‡')
    parser.add_argument('--seq-length', type=int, default=64, help='ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·')
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
    parser.add_argument('--model-path', type=str, help='æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--save-path', type=str, help='ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹')
    
    # JSONå…¥åŠ›
    parser.add_argument('--json-input', type=str, help='JSONå½¢å¼ã®å…¥åŠ›')
    
    args = parser.parse_args()
    
    # JSONå…¥åŠ›ãŒã‚ã‚‹å ´åˆã¯è§£æ
    if args.json_input:
        try:
            json_data = json.loads(args.json_input)
            input_data = json_data.get('input', json_data)
            
            # JSON ã‹ã‚‰å¼•æ•°ã‚’æ›´æ–°
            args.prompt = input_data.get('prompt', args.prompt)
            args.train = input_data.get('train_before_generate', args.train)
            args.mode = input_data.get('mode', args.mode)
            args.epochs = input_data.get('epochs', args.epochs)
            args.batch_size = input_data.get('batch_size', args.batch_size)
            args.learning_rate = input_data.get('learning_rate', args.learning_rate)
            args.num_neurons = input_data.get('num_neurons', args.num_neurons)
            args.max_tokens = input_data.get('max_tokens', args.max_tokens)
            args.temperature = input_data.get('temperature', args.temperature)
            
            if 'data_sources' in input_data:
                args.data_source = input_data['data_sources']
            
            if 'common_crawl_config' in input_data:
                cc_config = input_data['common_crawl_config']
                args.common_crawl_domains = cc_config.get('domains', args.common_crawl_domains)
                args.common_crawl_max_records = cc_config.get('max_records', args.common_crawl_max_records)
                args.common_crawl_id = cc_config.get('crawl_id', args.common_crawl_id)
                
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            sys.exit(1)
    
    print("=" * 60)
    print("ğŸ§ âš›ï¸ NeuroQ CLI")
    print("=" * 60)
    print(f"Mode: {args.mode}")
    print(f"Prompt: {args.prompt}")
    print(f"Train: {args.train or args.train_before_generate}")
    
    generator = None
    
    # å­¦ç¿’ãŒå¿…è¦ãªå ´åˆ
    if args.train or args.train_before_generate or not args.model_path:
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        texts = []
        
        for source in args.data_source:
            if source == 'common_crawl':
                cc_texts = fetch_common_crawl_data(
                    domains=args.common_crawl_domains,
                    max_records=args.common_crawl_max_records,
                    crawl_id=args.common_crawl_id,
                )
                texts.extend(cc_texts)
            elif source == 'huggingface':
                hf_texts = fetch_huggingface_data(max_samples=500)
                texts.extend(hf_texts)
            else:  # builtin
                texts.extend(get_builtin_training_data())
        
        print(f"\nğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«")
        
        # å­¦ç¿’
        model, tokenizer, generator = train_model(
            texts=texts,
            mode=args.mode,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            num_neurons=args.num_neurons,
            hidden_dim=args.hidden_dim,
            embed_dim=args.embed_dim,
            num_layers=args.num_layers,
            num_heads=args.num_heads,
            seq_length=args.seq_length,
            connection_density=args.connection_density,
            lambda_entangle=args.lambda_entangle,
            save_path=args.save_path,
        )
    
    # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    elif args.model_path:
        print(f"\nğŸ“¥ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰: {args.model_path}")
        tokenizer_path = args.model_path.replace('.pt', '_tokenizer.json')
        generator = NeuroQGenerator.load(args.model_path, tokenizer_path, "cpu")
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    if generator:
        output = generate_text(
            generator=generator,
            prompt=args.prompt,
            max_tokens=args.max_tokens,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
        )
        
        # çµæœã‚’JSONå½¢å¼ã§å‡ºåŠ›
        result = {
            "prompt": args.prompt,
            "output": output,
            "model_info": generator.get_model_info(),
        }
        
        print("\n" + "=" * 60)
        print("ğŸ“¤ JSONå‡ºåŠ›:")
        print(json.dumps(result, ensure_ascii=False, indent=2))
    
    print("\nâœ… å®Œäº†ï¼")


if __name__ == "__main__":
    main()

