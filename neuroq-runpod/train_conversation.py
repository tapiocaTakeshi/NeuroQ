#!/usr/bin/env python3
"""
ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
==================
conversation_training_data.txt ã‚’ä½¿ã£ã¦ä¼šè©±èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹
"""

import sys
import torch
from pathlib import Path

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
current_dir = str(Path(__file__).parent)
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from neuroquantum_layered import NeuroQuantumAI

def load_conversation_data(file_path: str = "conversation_training_data.txt"):
    """
    ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€

    Returns:
        List[str]: ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆå„è¦ç´ ãŒ1ã¤ã®ä¼šè©±ã‚¿ãƒ¼ãƒ³ï¼‰
    """
    print(f"ğŸ“– ä¼šè©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # 1è¡Œãšã¤ãŒ1ã¤ã®ä¼šè©±ãƒšã‚¢
    # æ”¹è¡Œã§åˆ†å‰²ã—ã¦ç©ºè¡Œã‚’é™¤å¤–
    conversations = [line.strip() for line in content.split('\n') if line.strip()]

    print(f"âœ… {len(conversations)} å€‹ã®ä¼šè©±ãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
    print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3å€‹ï¼‰:")
    for i, conv in enumerate(conversations[:3], 1):
        # æœ€åˆã®100æ–‡å­—ã®ã¿è¡¨ç¤º
        preview = conv[:100] + "..." if len(conv) > 100 else conv
        print(f"   {i}. {preview}")

    return conversations


def train_conversation_model(
    conversation_data,
    model_save_path: str = "neuroq_pretrained.pth",
    epochs: int = 10,
    batch_size: int = 16,
    seq_len: int = 128
):
    """
    ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

    Args:
        conversation_data: ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        model_save_path: ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ
        epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    """
    print("\n" + "=" * 60)
    print("ğŸš€ ä¼šè©±ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")
    print("=" * 60)

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°ãƒ­ãƒ¼ãƒ‰ï¼‰
    print("\nğŸ“¦ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–...")

    if Path(model_save_path).exists():
        print(f"   æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {model_save_path}")
        # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç¶™ç¶šå­¦ç¿’
        try:
            checkpoint = torch.load(model_save_path, map_location='cpu')
            config_dict = checkpoint['config']

            model = NeuroQuantumAI(
                embed_dim=config_dict['embed_dim'],
                hidden_dim=config_dict['hidden_dim'],
                num_heads=config_dict['num_heads'],
                num_layers=config_dict['num_layers'],
                max_seq_len=config_dict['max_seq_len'],
                dropout=config_dict.get('dropout', 0.1),
                lambda_entangle=config_dict.get('lambda_entangle', 0.5),
            )

            # é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
            from neuroquantum_layered import NeuroQuantum, NeuroQuantumConfig
            config = NeuroQuantumConfig(
                vocab_size=config_dict['vocab_size'],
                embed_dim=config_dict['embed_dim'],
                hidden_dim=config_dict['hidden_dim'],
                num_heads=config_dict['num_heads'],
                num_layers=config_dict['num_layers'],
                max_seq_len=config_dict['max_seq_len'],
                dropout=config_dict.get('dropout', 0.1),
                lambda_entangle=config_dict.get('lambda_entangle', 0.5),
            )

            model.model.load_state_dict(checkpoint['model_state_dict'])
            model.config = config
            print(f"   âœ… æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
            print(f"      Vocab size: {config_dict['vocab_size']}")
            print(f"      Embed dim: {config_dict['embed_dim']}")

        except Exception as e:
            print(f"   âš ï¸ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
            print(f"   æ–°è¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™...")
            model = NeuroQuantumAI(
                embed_dim=128,
                hidden_dim=256,
                num_heads=4,
                num_layers=3,
                max_seq_len=256,
                dropout=0.1,
                lambda_entangle=0.5
            )
    else:
        print(f"   æ–°è¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ")
        model = NeuroQuantumAI(
            embed_dim=128,
            hidden_dim=256,
            num_heads=4,
            num_layers=3,
            max_seq_len=256,
            dropout=0.1,
            lambda_entangle=0.5
        )

    print(f"\nğŸ“š å­¦ç¿’è¨­å®š:")
    print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {epochs}")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {seq_len}")
    print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {len(conversation_data)}")

    # å­¦ç¿’å®Ÿè¡Œ
    print(f"\nğŸ”„ å­¦ç¿’é–‹å§‹...\n")
    model.train(
        texts=conversation_data,
        epochs=epochs,
        batch_size=batch_size,
        seq_len=seq_len
    )

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print(f"\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_save_path}")
    model.save(model_save_path)

    print("\n" + "=" * 60)
    print("âœ… å­¦ç¿’å®Œäº†ï¼")
    print("=" * 60)

    # ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ§ª ç°¡æ˜“ãƒ†ã‚¹ãƒˆ:")
    test_prompts = [
        "<USER>ã“ã‚“ã«ã¡ã¯<ASSISTANT>",
        "<USER>äººå·¥çŸ¥èƒ½ã¨ã¯ï¼Ÿ<ASSISTANT>",
        "<USER>ã‚ã‚ŠãŒã¨ã†<ASSISTANT>"
    ]

    for prompt in test_prompts:
        print(f"\n   å…¥åŠ›: {prompt}")
        response = model.generate(
            prompt=prompt,
            max_length=50,
            temp_min=0.4,
            temp_max=0.7
        )
        print(f"   å‡ºåŠ›: {response}")


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸ’¬ ä¼šè©±ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)

    # ä¼šè©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    conversation_data = load_conversation_data("conversation_training_data.txt")

    # å­¦ç¿’å®Ÿè¡Œ
    train_conversation_model(
        conversation_data=conversation_data,
        epochs=10,           # ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
        batch_size=16,       # ãƒãƒƒãƒã‚µã‚¤ã‚º
        seq_len=128          # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    )

    print("\nğŸ‰ ã™ã¹ã¦å®Œäº†ï¼")
