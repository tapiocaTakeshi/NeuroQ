#!/usr/bin/env python3
"""
ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
==================
conversation_training_data.txt ã‚’ä½¿ã£ã¦ä¼šè©±èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹
"""

import sys
import torch
from pathlib import Path

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
current_dir = str(Path(__file__).parent)
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from neuroquantum_layered import NeuroQuantumAI

def load_conversation_data(file_path: str = "conversation_training_data.txt"):
    """
    ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€

    Returns:
        List[str]: ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆå„è¦ç´ ãŒ1ã¤ã®ä¼šè©±ã‚¿ãƒ¼ãƒ³ï¼‰
    """
    print(f"ğŸ“– ä¼šè©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # 1è¡Œãšã¤ãŒ1ã¤ã®ä¼šè©±ãƒšã‚¢
    # æ”¹è¡Œã§åˆ†å‰²ã—ã¦ç©ºè¡Œã‚’é™¤å¤–
    conversations = [line.strip() for line in content.split('\n') if line.strip()]

    print(f"âœ… {len(conversations)} å€‹ã®ä¼šè©±ãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
    print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3å€‹ï¼‰:")
    for i, conv in enumerate(conversations[:3], 1):
        # æœ€åˆã®100æ–‡å­—ã®ã¿è¡¨ç¤º
        preview = conv[:100] + "..." if len(conv) > 100 else conv
        print(f"   {i}. {preview}")

    return conversations


def train_conversation_model(
    conversation_data,
    tokenizer_save_path: str = "neuroq",
    epochs: int = 10,
    batch_size: int = 16,
    seq_len: int = 128
):
    """
    ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

    Args:
        conversation_data: ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        tokenizer_save_path: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜ãƒ‘ã‚¹ï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰
        epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        seq_len: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    """
    print("\n" + "=" * 60)
    print("ğŸš€ ä¼šè©±ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")
    print("=" * 60)

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    print("\nğŸ“¦ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–...")
    print(f"   æ–°è¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ")
    model = NeuroQuantumAI(
        embed_dim=128,
        hidden_dim=256,
        num_heads=4,
        num_layers=3,
        max_seq_len=256,
        dropout=0.1,
        lambda_entangle=0.5
    )

    print(f"\nğŸ“š å­¦ç¿’è¨­å®š:")
    print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {epochs}")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {seq_len}")
    print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {len(conversation_data)}")

    # å­¦ç¿’å®Ÿè¡Œ
    print(f"\nğŸ”„ å­¦ç¿’é–‹å§‹...\n")
    model.train(
        texts=conversation_data,
        epochs=epochs,
        batch_size=batch_size,
        seq_len=seq_len
    )

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜
    print(f"\nğŸ’¾ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜...")
    model.save_tokenizer(tokenizer_save_path)

    print("\n" + "=" * 60)
    print("âœ… å­¦ç¿’å®Œäº†ï¼")
    print("=" * 60)
    print(f"\næ³¨æ„: ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ä¿å­˜ã—ã¦ã„ã¾ã›ã‚“ã€‚")
    print(f"      ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã¿ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™ã€‚")

    # ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ§ª ç°¡æ˜“ãƒ†ã‚¹ãƒˆ:")
    test_prompts = [
        "<USER>ã“ã‚“ã«ã¡ã¯<ASSISTANT>",
        "<USER>äººå·¥çŸ¥èƒ½ã¨ã¯ï¼Ÿ<ASSISTANT>",
        "<USER>ã‚ã‚ŠãŒã¨ã†<ASSISTANT>"
    ]

    for prompt in test_prompts:
        print(f"\n   å…¥åŠ›: {prompt}")
        response = model.generate(
            prompt=prompt,
            max_length=50,
            temp_min=0.4,
            temp_max=0.7
        )
        print(f"   å‡ºåŠ›: {response}")


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸ’¬ ä¼šè©±ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)

    # ä¼šè©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    conversation_data = load_conversation_data("conversation_training_data.txt")

    # å­¦ç¿’å®Ÿè¡Œ
    train_conversation_model(
        conversation_data=conversation_data,
        epochs=10,           # ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
        batch_size=16,       # ãƒãƒƒãƒã‚µã‚¤ã‚º
        seq_len=128          # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    )

    print("\nğŸ‰ ã™ã¹ã¦å®Œäº†ï¼")
