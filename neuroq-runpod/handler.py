#!/usr/bin/env python3
"""
NeuroQ RunPod Serverless Handler - Optimized Version
=====================================================
é«˜é€Ÿèµ·å‹• & å®‰å®šå‹•ä½œã®ãŸã‚ã®æœ€é©åŒ–æ¸ˆã¿ãƒãƒ³ãƒ‰ãƒ©ãƒ¼

ç‰¹å¾´:
- èµ·å‹•æ™‚ã«é‡ã„å‡¦ç†ã‚’ã—ãªã„ï¼ˆé«˜é€Ÿèµ·å‹•ï¼‰
- health checkã¯å³åº§ã«200ã‚’è¿”ã™
- ãƒ¢ãƒ‡ãƒ«ã¯åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã«lazy load
- vocab_sizeã®æ•´åˆæ€§ã‚’ä¿è¨¼
"""

import runpod
import torch
import os
import sys

print("=" * 60)
print("âš›ï¸ NeuroQ RunPod Serverless - Starting...")
print("=" * 60)

# ========================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆèµ·å‹•æ™‚ã¯å…¨ã¦Noneï¼‰
# ========================================
model = None
is_initialized = False

# è¨­å®š
VOCAB_SIZE = 8000
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"ğŸ“Š Device: {DEVICE}")
print(f"ğŸ“Š CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}")


# ========================================
# Lazy Model Loadingï¼ˆåˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®ã¿ï¼‰
# ========================================
def initialize_model():
    """ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆåˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®ã¿å‘¼ã°ã‚Œã‚‹ï¼‰"""
    global model, is_initialized
    
    if is_initialized:
        return True
    
    print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–é–‹å§‹...")
    
    try:
        from neuroquantum_layered import NeuroQuantumAI
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå€‹åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åˆæœŸåŒ–ï¼‰
        model = NeuroQuantumAI(
            embed_dim=128,
            hidden_dim=256,
            num_heads=4,
            num_layers=3,
            max_seq_len=256,
            dropout=0.1,
            lambda_entangle=0.5
        )
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ç¢ºèª
        if os.path.exists("neuroq_tokenizer.model"):
            print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: neuroq_tokenizer.model")
        else:
            print("âš ï¸ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ˆã‚Šå¤šãã®æ–‡ç« ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼‰
        print("ğŸ”„ å­¦ç¿’é–‹å§‹...")
        training_data = [
            # åŸºæœ¬çš„ãªæŒ¨æ‹¶ã¨è‡ªå·±ç´¹ä»‹
            "ã“ã‚“ã«ã¡ã¯ã€‚ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ãŸäººå·¥çŸ¥èƒ½ã§ã™ã€‚",
            "ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¯é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®åŸç†ã‚’æ´»ç”¨ã—ãŸæ¬¡ä¸–ä»£ã®AIã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
            "ç§ã¯æ—¥æœ¬èªã§ä¼šè©±ãŒã§ãã‚‹äººå·¥çŸ¥èƒ½ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä½•ã§ã‚‚èã„ã¦ãã ã•ã„ã€‚",
            
            # é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¤ã„ã¦
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸæ¬¡ä¸–ä»£ã®è¨ˆç®—æ©Ÿã§ã™ã€‚",
            "é‡å­ãƒ“ãƒƒãƒˆã¯é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’å–ã‚‹ã“ã¨ãŒã§ãã€ä¸¦åˆ—è¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚",
            "é‡å­ã‚‚ã¤ã‚Œã¯äºŒã¤ã®é‡å­ãƒ“ãƒƒãƒˆãŒç›¸é–¢ã‚’æŒã¤ç¾è±¡ã§ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®åŸºç¤ã¨ãªã‚Šã¾ã™ã€‚",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯æš—å·è§£èª­ã‚„æœ€é©åŒ–å•é¡Œã§å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’å‡Œé§•ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚",
            
            # äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦
            "äººå·¥çŸ¥èƒ½ã¯äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
            "æ©Ÿæ¢°å­¦ç¿’ã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹AIã®æ‰‹æ³•ã§ã™ã€‚",
            "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å¤šå±¤åŒ–ã—ãŸæŠ€è¡“ã§ã™ã€‚",
            "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®è¨€èªã‚’ç†è§£ã—ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
            "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ç¾ä»£ã®AIã®åŸºç›¤ã¨ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚",
            
            # æŠ€è¡“çš„ãªèª¬æ˜
            "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚",
            "é‡ã¿ä»˜ã‘ã¨ãƒã‚¤ã‚¢ã‚¹ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã¯å­¦ç¿’ã—ã¾ã™ã€‚",
            "æå¤±é–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ãŒæ©Ÿæ¢°å­¦ç¿’ã®ç›®æ¨™ã§ã™ã€‚",
            "å‹¾é…é™ä¸‹æ³•ã¯æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚",
        ]
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç¹°ã‚Šè¿”ã—ã¦é‡ã‚’å¢—ã‚„ã™
        training_data = training_data * 5
        
        model.train(training_data, epochs=10, seq_len=64)
        
        is_initialized = True
        print("âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†!")
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


# ========================================
# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
# ========================================
def generate_text(prompt: str, max_length: int = 100, 
                  temperature: float = 0.7) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    global model
    
    if model is None:
        return "Error: Model not initialized"
    
    try:
        result = model.generate(
            prompt=prompt,
            max_length=max_length,
            temperature=temperature
        )
        return result
    except Exception as e:
        return f"Error: {str(e)}"


# ========================================
# ãƒ¡ã‚¤ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆRunPodç”¨ï¼‰
# ========================================
def handler(job):
    """
    RunPod Serverless Handler
    
    é‡è¦: health checkã¯å³åº§ã«è¿”ã™ï¼
    """
    global is_initialized
    
    job_input = job.get("input", {})
    action = job_input.get("action", "generate")
    
    # ========================================
    # HEALTH CHECKï¼ˆæœ€å„ªå…ˆãƒ»å³åº§ã«è¿”ã™ï¼‰
    # ========================================
    if action == "health":
        return {
            "status": "healthy",
            "device": DEVICE,
            "cuda_available": torch.cuda.is_available(),
            "model_initialized": is_initialized
        }
    
    # ========================================
    # STATUS CHECK
    # ========================================
    if action == "status":
        return {
            "status": "ok",
            "initialized": is_initialized,
            "device": DEVICE,
            "vocab_size": VOCAB_SIZE
        }
    
    # ========================================
    # GENERATEï¼ˆãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ãªå‡¦ç†ï¼‰
    # ========================================
    if action == "generate":
        # Lazy initialization
        if not is_initialized:
            print("ğŸ”„ åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆ - ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
            if not initialize_model():
                return {
                    "status": "error",
                    "error": "Failed to initialize model"
                }
        
        prompt = job_input.get("prompt", "ã“ã‚“ã«ã¡ã¯")
        max_length = job_input.get("max_length", 100)
        temperature = job_input.get("temperature", 0.7)
        
        print(f"ğŸ“ Generate: prompt='{prompt[:30]}...'")
        
        result = generate_text(
            prompt=prompt,
            max_length=max_length,
            temperature=temperature
        )
        
        return {
            "status": "success",
            "prompt": prompt,
            "generated": result
        }
    
    # ========================================
    # TRAINï¼ˆå­¦ç¿’ï¼‰
    # ========================================
    if action == "train":
        if not is_initialized:
            if not initialize_model():
                return {
                    "status": "error",
                    "error": "Failed to initialize model"
                }
        
        texts = job_input.get("texts", [])
        epochs = job_input.get("epochs", 5)
        
        if not texts:
            return {
                "status": "error",
                "error": "No training texts provided"
            }
        
        try:
            model.train(texts, epochs=epochs, seq_len=32)
            return {
                "status": "success",
                "message": f"Training completed ({epochs} epochs)"
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    # ========================================
    # UNKNOWN ACTION
    # ========================================
    return {
        "status": "error",
        "error": f"Unknown action: {action}",
        "available_actions": ["health", "status", "generate", "train"]
    }


# ========================================
# èµ·å‹•ï¼ˆä½•ã‚‚ã—ãªã„ = é«˜é€Ÿèµ·å‹•ï¼‰
# ========================================
print("=" * 60)
print("âœ… NeuroQ Handler Ready")
print("   - Health check: instant response")
print("   - Model loading: lazy (on first request)")
print("=" * 60)

# RunPodèµ·å‹•
runpod.serverless.start({"handler": handler})
