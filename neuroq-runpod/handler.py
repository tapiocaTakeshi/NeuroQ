#!/usr/bin/env python3
"""
NeuroQ RunPod Serverless Handler
=================================
RunPod Serverless APIç”¨ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«

å‚ç…§å…ƒ:
- neuroquantum_layered.py: å±¤çŠ¶QBNN-Transformer
- neuroquantum_brain.py: è„³å‹æ•£åœ¨QBNN

ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:
- /generate: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
- /health: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
"""

import os
import sys
import json
import torch
from typing import Dict, Any, Optional

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆneuroquantum_*.py ã‚’å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
# Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã¯åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã•ã‚Œã‚‹ã®ã§ã€è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‚ç…§ã¯ä¸è¦
# ãŸã ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã§ã®äº’æ›æ€§ã®ãŸã‚æ®‹ã™
PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# ã¾ãšç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ ï¼ˆDockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã¯ã“ã‚Œã§ååˆ†ï¼‰
if CURRENT_DIR not in sys.path:
    sys.path.insert(0, CURRENT_DIR)
# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚è¿½åŠ ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
if PARENT_DIR not in sys.path:
    sys.path.insert(0, PARENT_DIR)

# neuroquantum_layered.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from neuroquantum_layered import (
        NeuroQuantumAI,
        NeuroQuantumTokenizer,
        NeuroQuantumConfig,
        NeuroQuantum,
    )
    NEUROQUANTUM_LAYERED_AVAILABLE = True
    print("âœ… neuroquantum_layered.py ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError as e:
    NEUROQUANTUM_LAYERED_AVAILABLE = False
    print(f"âš ï¸ neuroquantum_layered.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")

# neuroquantum_brain.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from neuroquantum_brain import (
        NeuroQuantumBrainAI,
        BrainTokenizer,
        NeuroQuantumBrain,
    )
    NEUROQUANTUM_BRAIN_AVAILABLE = True
    print("âœ… neuroquantum_brain.py ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError as e:
    NEUROQUANTUM_BRAIN_AVAILABLE = False
    print(f"âš ï¸ neuroquantum_brain.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")

# RunPod SDK
try:
    import runpod
    RUNPOD_AVAILABLE = True
except ImportError:
    RUNPOD_AVAILABLE = False
    print("âš ï¸ runpodãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install runpod ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# OpenAI APIï¼ˆChatGPTã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”¨ï¼‰
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install openai ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ========================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
# ========================================

# ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ä¿æŒï¼‰
model_layered: Optional[NeuroQuantumAI] = None
model_brain: Optional[NeuroQuantumBrainAI] = None

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
if torch.backends.mps.is_available():
    DEVICE = torch.device("mps")
    print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
elif torch.cuda.is_available():
    DEVICE = torch.device("cuda")
    print("ğŸ® NVIDIA GPU (CUDA) ã‚’ä½¿ç”¨")
else:
    DEVICE = torch.device("cpu")
    print("ğŸ’» CPU ã‚’ä½¿ç”¨")


# ========================================
# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
# ========================================

def init_model(mode: str = "layered", **kwargs) -> Dict[str, Any]:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    
    Args:
        mode: 'layered' ã¾ãŸã¯ 'brain'
        **kwargs: ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
        åˆæœŸåŒ–çµæœ
    """
    global model_layered, model_brain
    
    try:
        if mode == "layered":
            if not NEUROQUANTUM_LAYERED_AVAILABLE:
                return {"error": "neuroquantum_layered.py ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            embed_dim = kwargs.get("embed_dim", 64)
            hidden_dim = kwargs.get("hidden_dim", 128)
            num_heads = kwargs.get("num_heads", 4)
            num_layers = kwargs.get("num_layers", 2)
            max_seq_len = kwargs.get("max_seq_len", 128)
            dropout = kwargs.get("dropout", 0.1)
            lambda_entangle = kwargs.get("lambda_entangle", 0.35)
            
            model_layered = NeuroQuantumAI(
                embed_dim=embed_dim,
                hidden_dim=hidden_dim,
                num_heads=num_heads,
                num_layers=num_layers,
                max_seq_len=max_seq_len,
                dropout=dropout,
                lambda_entangle=lambda_entangle,
            )
            model_layered.device = DEVICE
            
            return {
                "status": "success",
                "mode": "layered",
                "message": "Layered mode ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ"
            }
        
        elif mode == "brain":
            if not NEUROQUANTUM_BRAIN_AVAILABLE:
                return {"error": "neuroquantum_brain.py ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            embed_dim = kwargs.get("embed_dim", 128)
            num_heads = kwargs.get("num_heads", 4)
            num_layers = kwargs.get("num_layers", 3)
            num_neurons = kwargs.get("num_neurons", 75)
            max_vocab = kwargs.get("max_vocab", 50000)
            
            model_brain = NeuroQuantumBrainAI(
                embed_dim=embed_dim,
                num_heads=num_heads,
                num_layers=num_layers,
                num_neurons=num_neurons,
                max_vocab=max_vocab,
            )
            model_brain.device = DEVICE
            
            return {
                "status": "success",
                "mode": "brain",
                "message": "Brain mode ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ"
            }
        
        else:
            return {"error": f"ä¸æ˜ãªãƒ¢ãƒ¼ãƒ‰: {mode}"}
    
    except Exception as e:
        return {"error": f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}"}


# ========================================
# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
# ========================================

def generate_text(
    prompt: str,
    mode: str = "layered",
    max_length: int = 100,
    temperature: float = 0.7,
    top_k: int = 40,
    top_p: float = 0.9,
    **kwargs
) -> Dict[str, Any]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    
    Args:
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        mode: 'layered' ã¾ãŸã¯ 'brain'
        max_length: æœ€å¤§ç”Ÿæˆé•·
        temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        top_k: Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        top_p: Top-P ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
        ç”Ÿæˆçµæœ
    """
    global model_layered, model_brain
    
    try:
        if mode == "layered":
            if model_layered is None:
                # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
                init_result = init_model(mode="layered", **kwargs)
                if "error" in init_result:
                    return init_result
            
            if model_layered.model is None:
                return {"error": "ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            generated = model_layered.generate(
                prompt=prompt,
                max_length=max_length,
                temp_min=temperature * 0.8,
                temp_max=temperature * 1.2,
                top_k=top_k,
                top_p=top_p,
            )
            
            return {
                "status": "success",
                "mode": "layered",
                "prompt": prompt,
                "generated": generated,
            }
        
        elif mode == "brain":
            if model_brain is None:
                # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
                init_result = init_model(mode="brain", **kwargs)
                if "error" in init_result:
                    return init_result
            
            if model_brain.model is None:
                return {"error": "ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            generated = model_brain.generate(
                prompt=prompt,
                max_length=max_length,
                temperature_min=temperature * 0.8,
                temperature_max=temperature * 1.2,
                top_k=top_k,
                top_p=top_p,
            )
            
            return {
                "status": "success",
                "mode": "brain",
                "prompt": prompt,
                "generated": generated,
            }
        
        else:
            return {"error": f"ä¸æ˜ãªãƒ¢ãƒ¼ãƒ‰: {mode}"}
    
    except Exception as e:
        return {"error": f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"}


# ========================================
# RunPod Handler
# ========================================

def handler(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    RunPod Serverless Handler
    
    ãƒªã‚¯ã‚¨ã‚¹ãƒˆå½¢å¼:
    {
        "input": {
            "action": "generate" | "init" | "health",
            "mode": "layered" | "brain",
            "prompt": "ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "max_length": 100,
            "temperature": 0.7,
            ...
        }
    }
    """
    try:
        input_data = event.get("input", {})
        action = input_data.get("action", "generate")
        
        if action == "health":
            return {
                "status": "healthy",
                "layered_available": NEUROQUANTUM_LAYERED_AVAILABLE,
                "brain_available": NEUROQUANTUM_BRAIN_AVAILABLE,
                "openai_available": OPENAI_AVAILABLE,
                "device": str(DEVICE),
            }
        
        elif action == "init":
            mode = input_data.get("mode", "layered")
            kwargs = {k: v for k, v in input_data.items() if k != "action" and k != "mode"}
            return init_model(mode=mode, **kwargs)
        
        elif action == "generate":
            prompt = input_data.get("prompt", "")
            if not prompt:
                return {"error": "promptãŒå¿…è¦ã§ã™"}
            
            mode = input_data.get("mode", "layered")
            max_length = input_data.get("max_length", 100)
            temperature = input_data.get("temperature", 0.7)
            top_k = input_data.get("top_k", 40)
            top_p = input_data.get("top_p", 0.9)
            
            kwargs = {
                k: v for k, v in input_data.items()
                if k not in ["action", "prompt", "mode", "max_length", "temperature", "top_k", "top_p"]
            }
            
            return generate_text(
                prompt=prompt,
                mode=mode,
                max_length=max_length,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                **kwargs
            )
        
        else:
            return {"error": f"ä¸æ˜ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {action}"}
    
    except Exception as e:
        return {"error": f"ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}"}


# ========================================
# RunPod Serverless èµ·å‹•
# ========================================

if __name__ == "__main__":
    if RUNPOD_AVAILABLE:
        print("ğŸš€ RunPod Serverless Handler ã‚’èµ·å‹•ã—ã¾ã™...")
        runpod.serverless.start({"handler": handler})
    else:
        print("âš ï¸ RunPod SDKãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
        print("\nãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:")
        print(json.dumps({
            "input": {
                "action": "health"
            }
        }, indent=2))

