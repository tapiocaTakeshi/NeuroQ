#!/usr/bin/env python3
"""
NeuroQ RunPod Serverless Handler
================================
RunPod Serverless Endpointç”¨ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼

ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰:
- Brain Mode: è„³å‹æ•£åœ¨QBNN
- Layered Mode: å±¤çŠ¶QBNN-Transformer

ä½¿ç”¨æ–¹æ³•:
1. ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ neuroq_model.py ã‚’RunPodã«ãƒ‡ãƒ—ãƒ­ã‚¤
2. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (neuroq_model.pt, neuroq_tokenizer.json) ã‚’é…ç½®
3. RunPod Endpoint ã‹ã‚‰å‘¼ã³å‡ºã—

API ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
- ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: prompt, max_tokens, temperature, top_k, top_p
- ãƒ¢ãƒ‡ãƒ«è¨­å®š: mode, num_neurons, hidden_dim, connection_density, etc.
"""

import runpod
import torch
import os
import traceback
import json
import requests
import gzip
import io
import re
import xml.etree.ElementTree as ET
from typing import List, Dict, Optional, Generator
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import argparse

from neuroq_model import (
    NeuroQGenerator, 
    NeuroQModel, 
    NeuroQTokenizer, 
    NeuroQConfig,
    create_neuroq_brain,
    create_neuroq_layered,
)

# ========================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
# ========================================

MODEL_PATH = os.environ.get("NEUROQ_MODEL_PATH", "neuroq_model.pt")
TOKENIZER_PATH = os.environ.get("NEUROQ_TOKENIZER_PATH", "neuroq_tokenizer.json")
DEFAULT_MODE = os.environ.get("NEUROQ_MODE", "layered")  # 'brain' or 'layered'

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«è¨­å®š
DEFAULT_CONFIG = {
    # å…±é€š
    "embed_dim": int(os.environ.get("NEUROQ_EMBED_DIM", "128")),
    "num_layers": int(os.environ.get("NEUROQ_NUM_LAYERS", "3")),
    "dropout": float(os.environ.get("NEUROQ_DROPOUT", "0.1")),
    "max_seq_len": int(os.environ.get("NEUROQ_MAX_SEQ_LEN", "256")),
    
    # Brain Mode
    "num_neurons": int(os.environ.get("NEUROQ_NUM_NEURONS", "100")),
    "connection_density": float(os.environ.get("NEUROQ_CONNECTION_DENSITY", "0.25")),
    "lambda_entangle_brain": float(os.environ.get("NEUROQ_LAMBDA_BRAIN", "0.35")),
    
    # Layered Mode
    "hidden_dim": int(os.environ.get("NEUROQ_HIDDEN_DIM", "256")),
    "num_heads": int(os.environ.get("NEUROQ_NUM_HEADS", "4")),
    "lambda_entangle_layered": float(os.environ.get("NEUROQ_LAMBDA_LAYERED", "0.5")),
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å­¦ç¿’è¨­å®š
DEFAULT_TRAINING_CONFIG = {
    # ã‚¨ãƒãƒƒã‚¯ãƒ»ã‚¹ãƒ†ãƒƒãƒ—é–¢é€£
    "epochs": int(os.environ.get("NEUROQ_EPOCHS", "10")),
    "batch_size": int(os.environ.get("NEUROQ_BATCH_SIZE", "32")),
    "gradient_accumulation_steps": int(os.environ.get("NEUROQ_GRAD_ACCUM_STEPS", "1")),
    
    # å­¦ç¿’ç‡é–¢é€£
    "learning_rate": float(os.environ.get("NEUROQ_LEARNING_RATE", "1e-4")),
    "min_learning_rate": float(os.environ.get("NEUROQ_MIN_LR", "1e-6")),
    "weight_decay": float(os.environ.get("NEUROQ_WEIGHT_DECAY", "0.01")),
    "warmup_steps": int(os.environ.get("NEUROQ_WARMUP_STEPS", "100")),
    "lr_scheduler": os.environ.get("NEUROQ_LR_SCHEDULER", "cosine"),  # cosine, linear, constant
    
    # Temperatureé–¢é€£ï¼ˆç”Ÿæˆæ™‚ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼‰
    "max_temperature": float(os.environ.get("NEUROQ_MAX_TEMPERATURE", "1.5")),
    "min_temperature": float(os.environ.get("NEUROQ_MIN_TEMPERATURE", "0.1")),
    "temperature_decay": os.environ.get("NEUROQ_TEMP_DECAY", "linear"),  # linear, exponential, cosine
    
    # æ­£å‰‡åŒ–ãƒ»æœ€é©åŒ–
    "max_grad_norm": float(os.environ.get("NEUROQ_MAX_GRAD_NORM", "1.0")),
    "label_smoothing": float(os.environ.get("NEUROQ_LABEL_SMOOTHING", "0.1")),
    
    # è©•ä¾¡ãƒ»ä¿å­˜
    "eval_steps": int(os.environ.get("NEUROQ_EVAL_STEPS", "500")),
    "save_steps": int(os.environ.get("NEUROQ_SAVE_STEPS", "1000")),
    "logging_steps": int(os.environ.get("NEUROQ_LOGGING_STEPS", "100")),
    
    # æ—©æœŸåœæ­¢
    "early_stopping_patience": int(os.environ.get("NEUROQ_EARLY_STOPPING", "3")),
    "early_stopping_threshold": float(os.environ.get("NEUROQ_EARLY_STOPPING_THRESHOLD", "0.001")),
    
    # ãƒ‡ãƒ¼ã‚¿é–¢é€£
    "train_split": float(os.environ.get("NEUROQ_TRAIN_SPLIT", "0.9")),
    "shuffle": True,
    "seed": int(os.environ.get("NEUROQ_SEED", "42")),
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š
DEFAULT_DATA_SOURCE_CONFIG = {
    # Common Crawlè¨­å®š
    "common_crawl": {
        "enabled": True,
        "index_url": "https://index.commoncrawl.org",
        "data_url": "https://data.commoncrawl.org",
        "crawl_id": os.environ.get("COMMONCRAWL_ID", "CC-MAIN-2024-10"),  # æœ€æ–°ã®ã‚¯ãƒ­ãƒ¼ãƒ«ID
        "max_records": int(os.environ.get("COMMONCRAWL_MAX_RECORDS", "1000")),
        "languages": ["ja", "en"],  # æ—¥æœ¬èªã¨è‹±èª
        "min_content_length": 100,
        "max_content_length": 10000,
    },
    # PubMedè¨­å®š
    "pubmed": {
        "enabled": True,
        "base_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils",
        "api_key": os.environ.get("PUBMED_API_KEY", ""),  # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç·©å’Œç”¨
        "max_records": int(os.environ.get("PUBMED_MAX_RECORDS", "1000")),
        "search_terms": ["quantum computing", "neural network", "machine learning", "artificial intelligence"],
        "retmax": 100,  # 1å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°
        "include_abstract": True,
        "include_mesh_terms": True,
    },
}


# ========================================
# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹
# ========================================

class CommonCrawlLoader:
    """
    Common Crawlã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãƒ­ãƒ¼ãƒ€ãƒ¼
    https://commoncrawl.org/
    """
    
    def __init__(self, config: dict = None):
        self.config = config or DEFAULT_DATA_SOURCE_CONFIG["common_crawl"]
        self.index_url = self.config["index_url"]
        self.data_url = self.config["data_url"]
        self.crawl_id = self.config["crawl_id"]
        
    def search_index(self, query: str, limit: int = 100) -> List[dict]:
        """
        Common Crawl Indexã‚’æ¤œç´¢
        """
        try:
            url = f"{self.index_url}/{self.crawl_id}-index"
            params = {
                "url": query,
                "output": "json",
                "limit": limit
            }
            response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                results = []
                for line in response.text.strip().split('\n'):
                    if line:
                        try:
                            results.append(json.loads(line))
                        except json.JSONDecodeError:
                            continue
                return results
            return []
        except Exception as e:
            print(f"âš ï¸ Common Crawl Indexæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_warc_record(self, record: dict) -> Optional[str]:
        """
        WARCãƒ¬ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        """
        try:
            filename = record.get("filename")
            offset = int(record.get("offset", 0))
            length = int(record.get("length", 0))
            
            if not filename or length == 0:
                return None
            
            url = f"{self.data_url}/{filename}"
            headers = {"Range": f"bytes={offset}-{offset + length - 1}"}
            
            response = requests.get(url, headers=headers, timeout=60)
            
            if response.status_code in [200, 206]:
                # gzipè§£å‡
                try:
                    decompressed = gzip.decompress(response.content)
                    content = decompressed.decode('utf-8', errors='ignore')
                    
                    # HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    text = self._extract_text_from_html(content)
                    return text
                except:
                    return None
            return None
        except Exception as e:
            print(f"âš ï¸ WARCãƒ¬ã‚³ãƒ¼ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_text_from_html(self, html: str) -> str:
        """
        HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        """
        # WARCãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if "\r\n\r\n" in html:
            parts = html.split("\r\n\r\n")
            if len(parts) > 2:
                html = "\r\n\r\n".join(parts[2:])
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
        html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
        
        # HTMLã‚¿ã‚°ã‚’é™¤å»
        text = re.sub(r'<[^>]+>', ' ', html)
        
        # ç‰¹æ®Šæ–‡å­—ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        text = re.sub(r'&nbsp;', ' ', text)
        text = re.sub(r'&amp;', '&', text)
        text = re.sub(r'&lt;', '<', text)
        text = re.sub(r'&gt;', '>', text)
        text = re.sub(r'&quot;', '"', text)
        
        # ä½™åˆ†ãªç©ºç™½ã‚’æ•´ç†
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def load_data(self, domains: List[str] = None, max_records: int = None) -> List[str]:
        """
        Common Crawlã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
        
        Args:
            domains: æ¤œç´¢ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: ["*.wikipedia.org", "*.news.yahoo.co.jp"]ï¼‰
            max_records: å–å¾—ã™ã‚‹æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
        """
        if max_records is None:
            max_records = self.config["max_records"]
        
        if domains is None:
            domains = ["*.wikipedia.org", "*.news.yahoo.co.jp", "*.nhk.or.jp"]
        
        print(f"ğŸ“¥ Common Crawlã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        print(f"   Crawl ID: {self.crawl_id}")
        print(f"   å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³: {domains}")
        print(f"   æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {max_records}")
        
        texts = []
        records_per_domain = max_records // len(domains)
        
        for domain in domains:
            print(f"   ğŸ” æ¤œç´¢ä¸­: {domain}")
            records = self.search_index(domain, limit=records_per_domain)
            
            for record in records[:records_per_domain]:
                text = self.fetch_warc_record(record)
                if text:
                    # é•·ã•ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    if self.config["min_content_length"] <= len(text) <= self.config["max_content_length"]:
                        texts.append(text)
                
                if len(texts) >= max_records:
                    break
            
            if len(texts) >= max_records:
                break
            
            time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        print(f"âœ… Common Crawl: {len(texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—")
        return texts


class PubMedLoader:
    """
    PubMedã‹ã‚‰è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãƒ­ãƒ¼ãƒ€ãƒ¼
    https://pubmed.ncbi.nlm.nih.gov/
    """
    
    def __init__(self, config: dict = None):
        self.config = config or DEFAULT_DATA_SOURCE_CONFIG["pubmed"]
        self.base_url = self.config["base_url"]
        self.api_key = self.config.get("api_key", "")
        
    def search(self, term: str, retmax: int = 100) -> List[str]:
        """
        PubMedã‚’æ¤œç´¢ã—ã¦PMIDã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        """
        try:
            url = f"{self.base_url}/esearch.fcgi"
            params = {
                "db": "pubmed",
                "term": term,
                "retmax": retmax,
                "retmode": "json",
                "sort": "relevance"
            }
            if self.api_key:
                params["api_key"] = self.api_key
            
            response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                return data.get("esearchresult", {}).get("idlist", [])
            return []
        except Exception as e:
            print(f"âš ï¸ PubMedæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_abstracts(self, pmids: List[str]) -> List[dict]:
        """
        PMIDãƒªã‚¹ãƒˆã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—
        """
        if not pmids:
            return []
        
        try:
            url = f"{self.base_url}/efetch.fcgi"
            params = {
                "db": "pubmed",
                "id": ",".join(pmids),
                "rettype": "abstract",
                "retmode": "xml"
            }
            if self.api_key:
                params["api_key"] = self.api_key
            
            response = requests.get(url, params=params, timeout=60)
            
            if response.status_code == 200:
                return self._parse_pubmed_xml(response.text)
            return []
        except Exception as e:
            print(f"âš ï¸ PubMedè«–æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _parse_pubmed_xml(self, xml_text: str) -> List[dict]:
        """
        PubMed XMLã‚’ãƒ‘ãƒ¼ã‚¹
        """
        articles = []
        try:
            root = ET.fromstring(xml_text)
            
            for article in root.findall(".//PubmedArticle"):
                try:
                    # ã‚¿ã‚¤ãƒˆãƒ«
                    title_elem = article.find(".//ArticleTitle")
                    title = title_elem.text if title_elem is not None and title_elem.text else ""
                    
                    # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ
                    abstract_parts = []
                    for abstract_text in article.findall(".//AbstractText"):
                        if abstract_text.text:
                            label = abstract_text.get("Label", "")
                            if label:
                                abstract_parts.append(f"{label}: {abstract_text.text}")
                            else:
                                abstract_parts.append(abstract_text.text)
                    abstract = " ".join(abstract_parts)
                    
                    # MeSH Terms
                    mesh_terms = []
                    if self.config.get("include_mesh_terms", True):
                        for mesh in article.findall(".//MeshHeading/DescriptorName"):
                            if mesh.text:
                                mesh_terms.append(mesh.text)
                    
                    # PMID
                    pmid_elem = article.find(".//PMID")
                    pmid = pmid_elem.text if pmid_elem is not None else ""
                    
                    # è‘—è€…
                    authors = []
                    for author in article.findall(".//Author"):
                        lastname = author.find("LastName")
                        forename = author.find("ForeName")
                        if lastname is not None and lastname.text:
                            name = lastname.text
                            if forename is not None and forename.text:
                                name = f"{forename.text} {lastname.text}"
                            authors.append(name)
                    
                    # å‡ºç‰ˆå¹´
                    year_elem = article.find(".//PubDate/Year")
                    year = year_elem.text if year_elem is not None else ""
                    
                    if title or abstract:
                        articles.append({
                            "pmid": pmid,
                            "title": title,
                            "abstract": abstract,
                            "authors": authors,
                            "year": year,
                            "mesh_terms": mesh_terms
                        })
                except Exception as e:
                    continue
                    
        except ET.ParseError as e:
            print(f"âš ï¸ XMLè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return articles
    
    def load_data(self, search_terms: List[str] = None, max_records: int = None) -> List[str]:
        """
        PubMedã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
        
        Args:
            search_terms: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            max_records: å–å¾—ã™ã‚‹æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
        """
        if max_records is None:
            max_records = self.config["max_records"]
        
        if search_terms is None:
            search_terms = self.config["search_terms"]
        
        print(f"ğŸ“¥ PubMedã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        print(f"   æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {search_terms}")
        print(f"   æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {max_records}")
        
        texts = []
        records_per_term = max_records // len(search_terms)
        retmax = min(self.config["retmax"], records_per_term)
        
        for term in search_terms:
            print(f"   ğŸ” æ¤œç´¢ä¸­: {term}")
            pmids = self.search(term, retmax=retmax)
            
            if pmids:
                articles = self.fetch_abstracts(pmids)
                
                for article in articles:
                    # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’çµåˆã—ã¦ãƒ†ã‚­ã‚¹ãƒˆåŒ–
                    text_parts = []
                    
                    if article["title"]:
                        text_parts.append(f"Title: {article['title']}")
                    
                    if article["abstract"] and self.config.get("include_abstract", True):
                        text_parts.append(f"Abstract: {article['abstract']}")
                    
                    if article["mesh_terms"] and self.config.get("include_mesh_terms", True):
                        text_parts.append(f"Keywords: {', '.join(article['mesh_terms'][:10])}")
                    
                    if text_parts:
                        texts.append("\n".join(text_parts))
                
                if len(texts) >= max_records:
                    break
            
            time.sleep(0.34)  # NCBI APIåˆ¶é™: 1ç§’ã‚ãŸã‚Š3ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
        print(f"âœ… PubMed: {len(texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—")
        return texts[:max_records]


class DataSourceManager:
    """
    è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ç®¡ç†ã™ã‚‹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    """
    
    def __init__(self, config: dict = None):
        self.config = config or DEFAULT_DATA_SOURCE_CONFIG
        self.loaders = {}
        
        # ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
        if self.config.get("common_crawl", {}).get("enabled", True):
            self.loaders["common_crawl"] = CommonCrawlLoader(self.config.get("common_crawl"))
        
        if self.config.get("pubmed", {}).get("enabled", True):
            self.loaders["pubmed"] = PubMedLoader(self.config.get("pubmed"))
    
    def load_all(self, sources: List[str] = None, **kwargs) -> Dict[str, List[str]]:
        """
        æŒ‡å®šã•ã‚ŒãŸã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
        
        Args:
            sources: ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆï¼ˆNone=å…¨ã¦ï¼‰
            **kwargs: å„ãƒ­ãƒ¼ãƒ€ãƒ¼ã¸ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            ã‚½ãƒ¼ã‚¹åã‚’ã‚­ãƒ¼ã¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®è¾æ›¸
        """
        if sources is None:
            sources = list(self.loaders.keys())
        
        results = {}
        
        for source in sources:
            if source in self.loaders:
                try:
                    loader = self.loaders[source]
                    source_kwargs = kwargs.get(source, {})
                    texts = loader.load_data(**source_kwargs)
                    results[source] = texts
                except Exception as e:
                    print(f"âš ï¸ {source}ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
                    results[source] = []
        
        return results
    
    def load_combined(self, sources: List[str] = None, **kwargs) -> List[str]:
        """
        ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦çµåˆ
        """
        all_data = self.load_all(sources, **kwargs)
        combined = []
        for source, texts in all_data.items():
            combined.extend(texts)
        return combined
    
    def get_available_sources(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return list(self.loaders.keys())


# ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
if torch.cuda.is_available():
    DEVICE = "cuda"
    print(f"ğŸ® CUDA GPU ã‚’ä½¿ç”¨: {torch.cuda.get_device_name(0)}")
elif torch.backends.mps.is_available():
    DEVICE = "mps"
    print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
else:
    DEVICE = "cpu"
    print("ğŸ’» CPU ã‚’ä½¿ç”¨")

# ========================================
# ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
# ========================================

# ãƒ¢ãƒ¼ãƒ‰ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
model_cache = {}

def get_config_key(config_params: dict) -> str:
    """è¨­å®šã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
    return json.dumps(config_params, sort_keys=True)


def load_model(mode: str = None, config_params: dict = None):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
    
    Args:
        mode: 'brain' or 'layered' (Noneã®å ´åˆã¯DEFAULT_MODEã‚’ä½¿ç”¨)
        config_params: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ï¼‰
    """
    global model_cache
    
    if mode is None:
        mode = DEFAULT_MODE
    
    # è¨­å®šã‚’ãƒãƒ¼ã‚¸
    params = DEFAULT_CONFIG.copy()
    if config_params:
        params.update(config_params)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    cache_key = f"{mode}_{get_config_key(params)}"
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ã‚Œã°ãã‚Œã‚’è¿”ã™
    if cache_key in model_cache:
        return model_cache[cache_key]
    
    print(f"ğŸ“¥ NeuroQ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    print(f"   Mode: {mode}")
    print(f"   Config: {json.dumps(params, indent=2)}")
    print(f"   Device: {DEVICE}")
    
    # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    mode_model_path = f"neuroq_{mode}_model.pt"
    actual_model_path = mode_model_path if os.path.exists(mode_model_path) else MODEL_PATH
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if not os.path.exists(actual_model_path):
        print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {actual_model_path}")
        print("   ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™")
        
        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        if mode == 'brain':
            config = NeuroQConfig(
                mode='brain',
                vocab_size=2000,
                embed_dim=params['embed_dim'],
                num_neurons=params['num_neurons'],
                hidden_dim=params['num_neurons'] * 2,
                num_heads=params.get('num_heads', 4),
                num_layers=params['num_layers'],
                max_seq_len=params['max_seq_len'],
                dropout=params['dropout'],
                connection_density=params['connection_density'],
                lambda_entangle=params['lambda_entangle_brain'],
            )
        else:  # layered
            config = NeuroQConfig(
                mode='layered',
                vocab_size=2000,
                embed_dim=params['embed_dim'],
                hidden_dim=params['hidden_dim'],
                num_heads=params['num_heads'],
                num_layers=params['num_layers'],
                max_seq_len=params['max_seq_len'],
                dropout=params['dropout'],
                lambda_entangle=params['lambda_entangle_layered'],
            )
        
        model = NeuroQModel(config)
        tokenizer = NeuroQTokenizer(vocab_size=2000)
        
        # åŸºæœ¬çš„ãªèªå½™ã‚’æ§‹ç¯‰
        basic_texts = [
            "ã“ã‚“ã«ã¡ã¯ã€ç§ã¯NeuroQã§ã™ã€‚é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIã§ã™ã€‚",
            "Hello, I am NeuroQ. A generative AI based on Quantum-Bit Neural Network.",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯æ¬¡ä¸–ä»£ã®è¨ˆç®—æŠ€è¡“ã§ã™ã€‚",
            "äººå·¥çŸ¥èƒ½ã¯ç§ãŸã¡ã®ç”Ÿæ´»ã‚’å¤‰é©ã—ã¦ã„ã¾ã™ã€‚",
            "QBNNã¯é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚",
        ]
        tokenizer.build_vocab(basic_texts)
        
        generator = NeuroQGenerator(model, tokenizer, DEVICE)
        print(f"âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº† (Mode: {mode})")
    else:
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        generator = NeuroQGenerator.load(actual_model_path, TOKENIZER_PATH, DEVICE)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†: {actual_model_path}")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    info = generator.get_model_info()
    print(f"   ãƒ¢ãƒ¼ãƒ‰: {info.get('mode', 'unknown')}")
    print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info['num_params']:,}")
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {info['embed_dim']}")
    print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {info['hidden_dim']}")
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {info.get('num_neurons', 'N/A')}")
    print(f"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {info['num_layers']}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    model_cache[cache_key] = generator
    
    return generator


# ========================================
# RunPod Handler
# ========================================

def handler(job):
    """
    RunPod Serverless ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    å…¥åŠ›JSONå½¢å¼:
    {
        "input": {
            // === ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
            "prompt": "ç”Ÿæˆã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",  // å¿…é ˆ
            "max_tokens": 128,        // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰
            "temperature": 0.7,       // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰
            "top_k": 40,              // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 40ï¼‰
            "top_p": 0.9,             // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.9ï¼‰
            "repetition_penalty": 1.2 // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.2ï¼‰
            
            // === ãƒ¢ãƒ‡ãƒ«è¨­å®š ===
            "mode": "brain",          // ã‚ªãƒ—ã‚·ãƒ§ãƒ³: "brain" or "layered"
            
            // Brain Mode å°‚ç”¨
            "num_neurons": 100,       // ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
            "connection_density": 0.25, // æ¥ç¶šå¯†åº¦ (0.0-1.0)
            "lambda_entangle": 0.35,  // é‡å­ã‚‚ã¤ã‚Œå¼·åº¦
            
            // Layered Mode å°‚ç”¨
            "hidden_dim": 256,        // éš ã‚Œå±¤æ¬¡å…ƒ
            "num_heads": 4,           // ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
            
            // å…±é€š
            "embed_dim": 128,         // åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
            "num_layers": 3,          // ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            
            // === å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
            "epochs": 10,             // ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
            "batch_size": 32,         // ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 32ï¼‰
            "learning_rate": 1e-4,    // å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-4ï¼‰
            "min_learning_rate": 1e-6, // æœ€å°å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-6ï¼‰
            "weight_decay": 0.01,     // é‡ã¿æ¸›è¡°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.01ï¼‰
            "warmup_steps": 100,      // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰
            "lr_scheduler": "cosine", // LRã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼: cosine, linear, constant
            "max_temperature": 1.5,   // æœ€å¤§æ¸©åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.5ï¼‰
            "min_temperature": 0.1,   // æœ€å°æ¸©åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰
            "temperature_decay": "linear", // æ¸©åº¦æ¸›è¡°: linear, exponential, cosine
            "max_grad_norm": 1.0,     // å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰
            "label_smoothing": 0.1,   // ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰
            "gradient_accumulation_steps": 1, // å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—
            "eval_steps": 500,        // è©•ä¾¡é–“éš”ã‚¹ãƒ†ãƒƒãƒ—
            "save_steps": 1000,       // ä¿å­˜é–“éš”ã‚¹ãƒ†ãƒƒãƒ—
            "logging_steps": 100,     // ãƒ­ã‚°é–“éš”ã‚¹ãƒ†ãƒƒãƒ—
            "early_stopping_patience": 3,    // æ—©æœŸåœæ­¢patience
            "early_stopping_threshold": 0.001, // æ—©æœŸåœæ­¢é–¾å€¤
            "train_split": 0.9,       // å­¦ç¿’/æ¤œè¨¼åˆ†å‰²æ¯”ç‡
            "seed": 42,               // ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        }
    }
    
    å‡ºåŠ›JSONå½¢å¼:
    {
        "prompt": "å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        "output": "ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ",
        "model_info": {
            "mode": "brain" or "layered",
            "num_neurons": 100,
            "num_params": 123456,
            ...
        }
    }
    """
    try:
        # å…¥åŠ›ã‚’å–å¾—
        job_input = job.get("input", {})
        
        # ãƒ¢ãƒ¼ãƒ‰è¨­å®š
        mode = job_input.get("mode", DEFAULT_MODE)
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        config_params = {}
        
        # å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if "embed_dim" in job_input:
            config_params["embed_dim"] = int(job_input["embed_dim"])
        if "num_layers" in job_input:
            config_params["num_layers"] = int(job_input["num_layers"])
        if "dropout" in job_input:
            config_params["dropout"] = float(job_input["dropout"])
        if "max_seq_len" in job_input:
            config_params["max_seq_len"] = int(job_input["max_seq_len"])
        
        # Brain Mode å°‚ç”¨
        if "num_neurons" in job_input:
            config_params["num_neurons"] = int(job_input["num_neurons"])
        if "connection_density" in job_input:
            config_params["connection_density"] = float(job_input["connection_density"])
        if "lambda_entangle" in job_input and mode == "brain":
            config_params["lambda_entangle_brain"] = float(job_input["lambda_entangle"])
        
        # Layered Mode å°‚ç”¨
        if "hidden_dim" in job_input:
            config_params["hidden_dim"] = int(job_input["hidden_dim"])
        if "num_heads" in job_input:
            config_params["num_heads"] = int(job_input["num_heads"])
        if "lambda_entangle" in job_input and mode == "layered":
            config_params["lambda_entangle_layered"] = float(job_input["lambda_entangle"])
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        training_params = {}
        
        # ã‚¨ãƒãƒƒã‚¯ãƒ»ãƒãƒƒãƒé–¢é€£
        if "epochs" in job_input:
            training_params["epochs"] = int(job_input["epochs"])
        if "batch_size" in job_input:
            training_params["batch_size"] = int(job_input["batch_size"])
        if "gradient_accumulation_steps" in job_input:
            training_params["gradient_accumulation_steps"] = int(job_input["gradient_accumulation_steps"])
        
        # å­¦ç¿’ç‡é–¢é€£
        if "learning_rate" in job_input:
            training_params["learning_rate"] = float(job_input["learning_rate"])
        if "min_learning_rate" in job_input:
            training_params["min_learning_rate"] = float(job_input["min_learning_rate"])
        if "weight_decay" in job_input:
            training_params["weight_decay"] = float(job_input["weight_decay"])
        if "warmup_steps" in job_input:
            training_params["warmup_steps"] = int(job_input["warmup_steps"])
        if "lr_scheduler" in job_input:
            training_params["lr_scheduler"] = str(job_input["lr_scheduler"])
        
        # Temperatureé–¢é€£
        if "max_temperature" in job_input:
            training_params["max_temperature"] = float(job_input["max_temperature"])
        if "min_temperature" in job_input:
            training_params["min_temperature"] = float(job_input["min_temperature"])
        if "temperature_decay" in job_input:
            training_params["temperature_decay"] = str(job_input["temperature_decay"])
        
        # æ­£å‰‡åŒ–ãƒ»æœ€é©åŒ–
        if "max_grad_norm" in job_input:
            training_params["max_grad_norm"] = float(job_input["max_grad_norm"])
        if "label_smoothing" in job_input:
            training_params["label_smoothing"] = float(job_input["label_smoothing"])
        
        # è©•ä¾¡ãƒ»ä¿å­˜
        if "eval_steps" in job_input:
            training_params["eval_steps"] = int(job_input["eval_steps"])
        if "save_steps" in job_input:
            training_params["save_steps"] = int(job_input["save_steps"])
        if "logging_steps" in job_input:
            training_params["logging_steps"] = int(job_input["logging_steps"])
        
        # æ—©æœŸåœæ­¢
        if "early_stopping_patience" in job_input:
            training_params["early_stopping_patience"] = int(job_input["early_stopping_patience"])
        if "early_stopping_threshold" in job_input:
            training_params["early_stopping_threshold"] = float(job_input["early_stopping_threshold"])
        
        # ãƒ‡ãƒ¼ã‚¿é–¢é€£
        if "train_split" in job_input:
            training_params["train_split"] = float(job_input["train_split"])
        if "shuffle" in job_input:
            training_params["shuffle"] = bool(job_input["shuffle"])
        if "seed" in job_input:
            training_params["seed"] = int(job_input["seed"])
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        gen = load_model(mode, config_params if config_params else None)
        
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        prompt = job_input.get("prompt", "")
        max_tokens = int(job_input.get("max_tokens", 128))
        temperature = float(job_input.get("temperature", 0.7))
        top_k = int(job_input.get("top_k", 40))
        top_p = float(job_input.get("top_p", 0.9))
        repetition_penalty = float(job_input.get("repetition_penalty", 1.2))
        
        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not prompt:
            return {"error": "prompt is required"}
        
        if max_tokens < 1 or max_tokens > 1024:
            max_tokens = min(max(1, max_tokens), 1024)
        
        if temperature < 0.1 or temperature > 2.0:
            temperature = min(max(0.1, temperature), 2.0)
        
        print(f"ğŸ“ ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ:")
        print(f"   Mode: {mode}")
        print(f"   Prompt: {prompt[:50]}...")
        print(f"   Max tokens: {max_tokens}")
        print(f"   Temperature: {temperature}")
        if config_params:
            print(f"   Custom config: {json.dumps(config_params)}")
        if training_params:
            print(f"   Training params: {json.dumps(training_params)}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        output_text = gen.generate(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
        )
        
        print(f"âœ… ç”Ÿæˆå®Œäº†: {len(output_text)} æ–‡å­—")
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ãƒãƒ¼ã‚¸
        merged_training_params = DEFAULT_TRAINING_CONFIG.copy()
        merged_training_params.update(training_params)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        return {
            "prompt": prompt,
            "output": output_text,
            "model_info": gen.get_model_info(),
            "config": {
                "mode": mode,
                **config_params
            },
            "training_config": merged_training_params
        }
        
    except Exception as e:
        error_msg = f"Error: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}


# ========================================
# å­¦ç¿’ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def train_handler(job):
    """
    ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    å…¥åŠ›JSONå½¢å¼:
    {
        "input": {
            // === ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆã„ãšã‚Œã‹å¿…é ˆï¼‰ ===
            "training_data": ["ãƒ†ã‚­ã‚¹ãƒˆ1", "ãƒ†ã‚­ã‚¹ãƒˆ2", ...],  // ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®š
            // ã¾ãŸã¯
            "data_sources": ["common_crawl", "pubmed"],  // ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—
            
            // === ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ ===
            "common_crawl_config": {
                "domains": ["*.wikipedia.org", "*.news.yahoo.co.jp"],
                "max_records": 500,
                "crawl_id": "CC-MAIN-2024-10"
            },
            "pubmed_config": {
                "search_terms": ["quantum computing", "neural network"],
                "max_records": 500
            },
            
            "mode": "layered",        // ã‚ªãƒ—ã‚·ãƒ§ãƒ³: "brain" or "layered"
            
            // å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            "epochs": 10,
            "batch_size": 32,
            "learning_rate": 1e-4,
            "min_learning_rate": 1e-6,
            "warmup_steps": 100,
            "lr_scheduler": "cosine",
            "max_temperature": 1.5,
            "min_temperature": 0.1,
            "temperature_decay": "linear",
            "max_grad_norm": 1.0,
            "label_smoothing": 0.1,
            ...
        }
    }
    """
    try:
        job_input = job.get("input", {})
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆç›´æ¥æŒ‡å®šã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï¼‰
        training_data = job_input.get("training_data", [])
        data_sources = job_input.get("data_sources", [])
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
        loaded_data_info = {}
        if data_sources:
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            print(f"   ã‚½ãƒ¼ã‚¹: {data_sources}")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šã‚’æ§‹ç¯‰
            data_source_config = DEFAULT_DATA_SOURCE_CONFIG.copy()
            
            # Common Crawlè¨­å®šã‚’ãƒãƒ¼ã‚¸
            if "common_crawl_config" in job_input:
                cc_config = job_input["common_crawl_config"]
                data_source_config["common_crawl"].update(cc_config)
            
            # PubMedè¨­å®šã‚’ãƒãƒ¼ã‚¸
            if "pubmed_config" in job_input:
                pm_config = job_input["pubmed_config"]
                data_source_config["pubmed"].update(pm_config)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆ
            manager = DataSourceManager(data_source_config)
            
            # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
            source_kwargs = {}
            
            if "common_crawl" in data_sources:
                cc_kwargs = {}
                if "common_crawl_config" in job_input:
                    if "domains" in job_input["common_crawl_config"]:
                        cc_kwargs["domains"] = job_input["common_crawl_config"]["domains"]
                    if "max_records" in job_input["common_crawl_config"]:
                        cc_kwargs["max_records"] = job_input["common_crawl_config"]["max_records"]
                source_kwargs["common_crawl"] = cc_kwargs
            
            if "pubmed" in data_sources:
                pm_kwargs = {}
                if "pubmed_config" in job_input:
                    if "search_terms" in job_input["pubmed_config"]:
                        pm_kwargs["search_terms"] = job_input["pubmed_config"]["search_terms"]
                    if "max_records" in job_input["pubmed_config"]:
                        pm_kwargs["max_records"] = job_input["pubmed_config"]["max_records"]
                source_kwargs["pubmed"] = pm_kwargs
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
            loaded_data = manager.load_all(sources=data_sources, **source_kwargs)
            
            # ãƒ­ãƒ¼ãƒ‰çµæœã‚’è¨˜éŒ²
            for source, texts in loaded_data.items():
                loaded_data_info[source] = len(texts)
                training_data.extend(texts)
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†:")
            for source, count in loaded_data_info.items():
                print(f"   {source}: {count}ä»¶")
        
        if not training_data:
            return {"error": "training_data or data_sources is required"}
        
        mode = job_input.get("mode", DEFAULT_MODE)
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦ãƒãƒ¼ã‚¸
        training_config = DEFAULT_TRAINING_CONFIG.copy()
        
        training_param_keys = [
            "epochs", "batch_size", "gradient_accumulation_steps",
            "learning_rate", "min_learning_rate", "weight_decay", "warmup_steps", "lr_scheduler",
            "max_temperature", "min_temperature", "temperature_decay",
            "max_grad_norm", "label_smoothing",
            "eval_steps", "save_steps", "logging_steps",
            "early_stopping_patience", "early_stopping_threshold",
            "train_split", "shuffle", "seed"
        ]
        
        for key in training_param_keys:
            if key in job_input:
                value = job_input[key]
                # å‹å¤‰æ›
                if key in ["epochs", "batch_size", "gradient_accumulation_steps", "warmup_steps", 
                           "eval_steps", "save_steps", "logging_steps", "early_stopping_patience", "seed"]:
                    training_config[key] = int(value)
                elif key in ["learning_rate", "min_learning_rate", "weight_decay", 
                             "max_temperature", "min_temperature", "max_grad_norm", 
                             "label_smoothing", "early_stopping_threshold", "train_split"]:
                    training_config[key] = float(value)
                elif key == "shuffle":
                    training_config[key] = bool(value)
                else:
                    training_config[key] = str(value)
        
        print(f"ğŸ“ å­¦ç¿’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:")
        print(f"   Mode: {mode}")
        print(f"   Training data size: {len(training_data)} samples")
        print(f"   Epochs: {training_config['epochs']}")
        print(f"   Batch size: {training_config['batch_size']}")
        print(f"   Learning rate: {training_config['learning_rate']}")
        print(f"   Max/Min temperature: {training_config['max_temperature']}/{training_config['min_temperature']}")
        print(f"   Temperature decay: {training_config['temperature_decay']}")
        print(f"   LR scheduler: {training_config['lr_scheduler']}")
        print(f"   Warmup steps: {training_config['warmup_steps']}")
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        config_params = {}
        model_param_keys = [
            "embed_dim", "num_layers", "dropout", "max_seq_len",
            "num_neurons", "connection_density", "hidden_dim", "num_heads"
        ]
        for key in model_param_keys:
            if key in job_input:
                if key in ["dropout", "connection_density"]:
                    config_params[key] = float(job_input[key])
                else:
                    config_params[key] = int(job_input[key])
        
        if "lambda_entangle" in job_input:
            if mode == "brain":
                config_params["lambda_entangle_brain"] = float(job_input["lambda_entangle"])
            else:
                config_params["lambda_entangle_layered"] = float(job_input["lambda_entangle"])
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        gen = load_model(mode, config_params if config_params else None)
        
        # æ³¨: å®Ÿéš›ã®å­¦ç¿’å‡¦ç†ã¯NeuroQGeneratorã«å®Ÿè£…ãŒå¿…è¦
        # ã“ã“ã§ã¯å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å—ã‘æ¸¡ã—ã¨è¨­å®šã®ç¢ºèªã®ã¿
        
        return {
            "status": "training_config_ready",
            "message": "Training parameters received and validated",
            "mode": mode,
            "training_data_count": len(training_data),
            "data_sources_used": loaded_data_info if loaded_data_info else None,
            "model_config": config_params if config_params else DEFAULT_CONFIG,
            "training_config": training_config,
            "model_info": gen.get_model_info(),
        }
        
    except Exception as e:
        error_msg = f"Training Error: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}


# ========================================
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def fetch_data_handler(job):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    å…¥åŠ›JSONå½¢å¼:
    {
        "input": {
            "sources": ["common_crawl", "pubmed"],  // å–å¾—ã™ã‚‹ã‚½ãƒ¼ã‚¹
            
            // Common Crawlè¨­å®š
            "common_crawl_config": {
                "domains": ["*.wikipedia.org", "*.news.yahoo.co.jp"],
                "max_records": 100,
                "crawl_id": "CC-MAIN-2024-10"
            },
            
            // PubMedè¨­å®š
            "pubmed_config": {
                "search_terms": ["quantum computing", "neural network"],
                "max_records": 100
            }
        }
    }
    """
    try:
        job_input = job.get("input", {})
        sources = job_input.get("sources", ["common_crawl", "pubmed"])
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        print(f"   ã‚½ãƒ¼ã‚¹: {sources}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šã‚’æ§‹ç¯‰
        data_source_config = DEFAULT_DATA_SOURCE_CONFIG.copy()
        
        if "common_crawl_config" in job_input:
            data_source_config["common_crawl"].update(job_input["common_crawl_config"])
        
        if "pubmed_config" in job_input:
            data_source_config["pubmed"].update(job_input["pubmed_config"])
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆ
        manager = DataSourceManager(data_source_config)
        
        # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
        source_kwargs = {}
        
        if "common_crawl" in sources:
            cc_kwargs = {}
            if "common_crawl_config" in job_input:
                if "domains" in job_input["common_crawl_config"]:
                    cc_kwargs["domains"] = job_input["common_crawl_config"]["domains"]
                if "max_records" in job_input["common_crawl_config"]:
                    cc_kwargs["max_records"] = job_input["common_crawl_config"]["max_records"]
            source_kwargs["common_crawl"] = cc_kwargs
        
        if "pubmed" in sources:
            pm_kwargs = {}
            if "pubmed_config" in job_input:
                if "search_terms" in job_input["pubmed_config"]:
                    pm_kwargs["search_terms"] = job_input["pubmed_config"]["search_terms"]
                if "max_records" in job_input["pubmed_config"]:
                    pm_kwargs["max_records"] = job_input["pubmed_config"]["max_records"]
            source_kwargs["pubmed"] = pm_kwargs
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
        loaded_data = manager.load_all(sources=sources, **source_kwargs)
        
        # çµæœã‚’æ•´ç†
        results = {}
        total_count = 0
        for source, texts in loaded_data.items():
            results[source] = {
                "count": len(texts),
                "sample": texts[:3] if texts else [],  # æœ€åˆã®3ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦
            }
            total_count += len(texts)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: åˆè¨ˆ {total_count}ä»¶")
        
        return {
            "status": "success",
            "total_count": total_count,
            "sources": results,
            "available_sources": manager.get_available_sources(),
        }
        
    except Exception as e:
        error_msg = f"Data Fetch Error: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}


def data_source_config_handler(job):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šã‚’å–å¾—ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    """
    try:
        manager = DataSourceManager()
        
        return {
            "status": "success",
            "available_sources": manager.get_available_sources(),
            "default_config": DEFAULT_DATA_SOURCE_CONFIG,
            "usage_examples": {
                "fetch_from_common_crawl": {
                    "input": {
                        "sources": ["common_crawl"],
                        "common_crawl_config": {
                            "domains": ["*.wikipedia.org"],
                            "max_records": 100
                        }
                    }
                },
                "fetch_from_pubmed": {
                    "input": {
                        "sources": ["pubmed"],
                        "pubmed_config": {
                            "search_terms": ["quantum computing", "machine learning"],
                            "max_records": 100
                        }
                    }
                },
                "train_with_data_sources": {
                    "input": {
                        "data_sources": ["common_crawl", "pubmed"],
                        "common_crawl_config": {
                            "domains": ["*.wikipedia.org"],
                            "max_records": 500
                        },
                        "pubmed_config": {
                            "search_terms": ["neural network"],
                            "max_records": 500
                        },
                        "epochs": 10,
                        "batch_size": 32,
                        "learning_rate": 1e-4
                    }
                }
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
        }


# ========================================
# é‡å­æƒ…å ±ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def quantum_info(job):
    """é‡å­ã‚‚ã¤ã‚Œæƒ…å ±ã‚’å–å¾—"""
    try:
        job_input = job.get("input", {})
        mode = job_input.get("mode", DEFAULT_MODE)
        
        gen = load_model(mode)
        model_info = gen.get_model_info()
        quantum_info = gen.model.get_quantum_info()
        
        return {
            "status": "success",
            "mode": model_info.get('mode', 'unknown'),
            "model_info": model_info,
            "quantum_info": quantum_info,
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
        }


# ========================================
# ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def model_config(job):
    """
    ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã¨åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    """
    try:
        manager = DataSourceManager()
        
        return {
            "status": "success",
            "default_mode": DEFAULT_MODE,
            "default_config": DEFAULT_CONFIG,
            "default_training_config": DEFAULT_TRAINING_CONFIG,
            "default_data_source_config": DEFAULT_DATA_SOURCE_CONFIG,
            "available_data_sources": manager.get_available_sources(),
            "device": DEVICE,
            "cached_models": list(model_cache.keys()),
            "available_options": {
                "common": {
                    "embed_dim": "åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰",
                    "num_layers": "ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰",
                    "dropout": "ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰",
                    "max_seq_len": "æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 256ï¼‰",
                },
                "brain_mode": {
                    "num_neurons": "ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰",
                    "connection_density": "æ¥ç¶šå¯†åº¦ 0.0-1.0ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.25ï¼‰",
                    "lambda_entangle": "é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.35ï¼‰",
                },
                "layered_mode": {
                    "hidden_dim": "éš ã‚Œå±¤æ¬¡å…ƒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 256ï¼‰",
                    "num_heads": "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4ï¼‰",
                    "lambda_entangle": "é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5ï¼‰",
                },
                "training": {
                    "epochs": "ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰",
                    "batch_size": "ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 32ï¼‰",
                    "gradient_accumulation_steps": "å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰",
                    "learning_rate": "å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-4ï¼‰",
                    "min_learning_rate": "æœ€å°å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-6ï¼‰",
                    "weight_decay": "é‡ã¿æ¸›è¡°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.01ï¼‰",
                    "warmup_steps": "ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰",
                    "lr_scheduler": "å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼: cosine, linear, constantï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: cosineï¼‰",
                    "max_temperature": "æœ€å¤§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.5ï¼‰",
                    "min_temperature": "æœ€å°ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰",
                    "temperature_decay": "æ¸©åº¦æ¸›è¡°æ–¹å¼: linear, exponential, cosineï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: linearï¼‰",
                    "max_grad_norm": "å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰",
                    "label_smoothing": "ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰",
                    "eval_steps": "è©•ä¾¡é–“éš”ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 500ï¼‰",
                    "save_steps": "ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–“éš”ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰",
                    "logging_steps": "ãƒ­ã‚°å‡ºåŠ›é–“éš”ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰",
                    "early_stopping_patience": "æ—©æœŸåœæ­¢patienceï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰",
                    "early_stopping_threshold": "æ—©æœŸåœæ­¢é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.001ï¼‰",
                    "train_split": "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æ¯”ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.9ï¼‰",
                    "shuffle": "ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: trueï¼‰",
                    "seed": "ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 42ï¼‰",
                },
                "data_sources": {
                    "common_crawl": {
                        "description": "Common Crawl - å¤§è¦æ¨¡ã‚¦ã‚§ãƒ–ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (https://commoncrawl.org/)",
                        "domains": "æ¤œç´¢ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: *.wikipedia.orgï¼‰",
                        "max_records": "å–å¾—ã™ã‚‹æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°",
                        "crawl_id": "ã‚¯ãƒ­ãƒ¼ãƒ«IDï¼ˆä¾‹: CC-MAIN-2024-10ï¼‰",
                        "languages": "å¯¾è±¡è¨€èªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ja, enï¼‰",
                    },
                    "pubmed": {
                        "description": "PubMed - åŒ»å­¦ãƒ»ç”Ÿå‘½ç§‘å­¦è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (https://pubmed.ncbi.nlm.nih.gov/)",
                        "search_terms": "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ",
                        "max_records": "å–å¾—ã™ã‚‹æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°",
                        "api_key": "NCBI API Keyï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç·©å’Œç”¨ï¼‰",
                        "include_abstract": "ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’å«ã‚ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: trueï¼‰",
                        "include_mesh_terms": "MeSHã‚¿ãƒ¼ãƒ ã‚’å«ã‚ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: trueï¼‰",
                    },
                },
            },
            "example_request": {
                "input": {
                    "prompt": "ã“ã‚“ã«ã¡ã¯",
                    "mode": "brain",
                    "num_neurons": 200,
                    "connection_density": 0.3,
                    "max_tokens": 64,
                    "temperature": 0.7
                }
            },
            "example_training_request": {
                "input": {
                    "action": "train",
                    "training_data": ["ãƒ†ã‚­ã‚¹ãƒˆ1", "ãƒ†ã‚­ã‚¹ãƒˆ2", "..."],
                    "mode": "layered",
                    "epochs": 20,
                    "batch_size": 16,
                    "learning_rate": 5e-5,
                    "max_temperature": 1.2,
                    "min_temperature": 0.3,
                    "warmup_steps": 200,
                    "early_stopping_patience": 5
                }
            },
            "example_training_with_data_sources": {
                "input": {
                    "data_sources": ["common_crawl", "pubmed"],
                    "common_crawl_config": {
                        "domains": ["*.wikipedia.org", "*.news.yahoo.co.jp"],
                        "max_records": 500
                    },
                    "pubmed_config": {
                        "search_terms": ["quantum computing", "neural network", "machine learning"],
                        "max_records": 500
                    },
                    "mode": "layered",
                    "epochs": 20,
                    "batch_size": 16,
                    "learning_rate": 5e-5,
                    "max_temperature": 1.2,
                    "min_temperature": 0.3
                }
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
        }


# ========================================
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def health_check(job):
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        gen = load_model()
        return {
            "status": "healthy",
            "model_loaded": gen is not None,
            "device": DEVICE,
            "model_info": gen.get_model_info() if gen else None,
            "cached_models": len(model_cache),
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
        }


# ========================================
# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
# ========================================

def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(
        description="NeuroQ RunPod Serverless Worker - QBNN-LLM",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--max-tokens", "-m",
        type=int,
        default=128,
        help="ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°"
    )
    parser.add_argument(
        "--temperature", "-t",
        type=float,
        default=0.7,
        help="ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ (0.1-2.0)"
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=40,
        help="Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®Kå€¤"
    )
    parser.add_argument(
        "--top-p",
        type=float,
        default=0.9,
        help="Top-P (Nucleus) ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®På€¤"
    )
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    parser.add_argument(
        "--mode",
        type=str,
        choices=["brain", "layered"],
        default=DEFAULT_MODE,
        help="ãƒ¢ãƒ‡ãƒ«ãƒ¢ãƒ¼ãƒ‰: brain (è„³å‹æ•£åœ¨QBNN) or layered (å±¤çŠ¶QBNN-Transformer)"
    )
    parser.add_argument(
        "--embed-dim",
        type=int,
        default=DEFAULT_CONFIG["embed_dim"],
        help="åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ"
    )
    parser.add_argument(
        "--num-layers",
        type=int,
        default=DEFAULT_CONFIG["num_layers"],
        help="ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°"
    )
    parser.add_argument(
        "--num-neurons",
        type=int,
        default=DEFAULT_CONFIG["num_neurons"],
        help="ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (Brain Modeç”¨)"
    )
    parser.add_argument(
        "--hidden-dim",
        type=int,
        default=DEFAULT_CONFIG["hidden_dim"],
        help="éš ã‚Œå±¤æ¬¡å…ƒ (Layered Modeç”¨)"
    )
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
    parser.add_argument(
        "--test",
        action="store_true",
        help="ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆRunPodã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã›ãšã€ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã†ï¼‰"
    )
    parser.add_argument(
        "--prompt", "-p",
        type=str,
        default="ã“ã‚“ã«ã¡ã¯",
        help="ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰æ™‚ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
    )
    
    return parser.parse_args()


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹
    args = parse_args()
    
    print("=" * 60)
    print("ğŸ§ âš›ï¸ NeuroQ RunPod Serverless Worker")
    print("   Brain Mode: è„³å‹æ•£åœ¨QBNN")
    print("   Layered Mode: å±¤çŠ¶QBNN-Transformer")
    print("=" * 60)
    
    print("\nğŸ“‹ ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°:")
    print(f"   --max-tokens: {args.max_tokens}")
    print(f"   --temperature: {args.temperature}")
    print(f"   --top-k: {args.top_k}")
    print(f"   --top-p: {args.top_p}")
    print(f"   --mode: {args.mode}")
    
    print("\nğŸ“‹ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«è¨­å®š:")
    print(f"   Mode: {DEFAULT_MODE}")
    for key, value in DEFAULT_CONFIG.items():
        print(f"   {key}: {value}")
    
    print("\nğŸ“š ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå­¦ç¿’è¨­å®š:")
    for key, value in DEFAULT_TRAINING_CONFIG.items():
        print(f"   {key}: {value}")
    
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š:")
    print("   åˆ©ç”¨å¯èƒ½ãªã‚½ãƒ¼ã‚¹:")
    print("   - common_crawl: Common Crawl (https://commoncrawl.org/)")
    print("   - pubmed: PubMed (https://pubmed.ncbi.nlm.nih.gov/)")
    for source, config in DEFAULT_DATA_SOURCE_CONFIG.items():
        print(f"   [{source}]")
        if source == "common_crawl":
            print(f"     crawl_id: {config.get('crawl_id')}")
            print(f"     max_records: {config.get('max_records')}")
        elif source == "pubmed":
            print(f"     search_terms: {config.get('search_terms')}")
            print(f"     max_records: {config.get('max_records')}")
    print()
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å¼•æ•°ã‹ã‚‰æ§‹ç¯‰
    config_params = {
        "embed_dim": args.embed_dim,
        "num_layers": args.num_layers,
        "num_neurons": args.num_neurons,
        "hidden_dim": args.hidden_dim,
    }
    
    # èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
    gen = load_model(args.mode, config_params)
    
    if args.test:
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ç”Ÿæˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        print("\nğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ç”Ÿæˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ")
        print(f"   Prompt: {args.prompt}")
        print(f"   Max tokens: {args.max_tokens}")
        print(f"   Temperature: {args.temperature}")
        print()
        
        output = gen.generate(
            prompt=args.prompt,
            max_tokens=args.max_tokens,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
        )
        
        print("ğŸ“ ç”Ÿæˆçµæœ:")
        print("-" * 40)
        print(output)
        print("-" * 40)
    else:
        # RunPod Serverless ã‚’é–‹å§‹
        runpod.serverless.start({
            "handler": handler,
            "train": train_handler,
            "fetch_data": fetch_data_handler,
            "data_source_config": data_source_config_handler,
        })
