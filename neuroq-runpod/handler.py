#!/usr/bin/env python3
"""
NeuroQ RunPod Serverless Handler
================================
RunPod Serverless Endpointç”¨ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼

ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰:
- Brain Mode: è„³å‹æ•£åœ¨QBNN
- Layered Mode: å±¤çŠ¶QBNN-Transformer

ä½¿ç”¨æ–¹æ³•:
1. ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ neuroq_model.py ã‚’RunPodã«ãƒ‡ãƒ—ãƒ­ã‚¤
2. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (neuroq_model.pt, neuroq_tokenizer.json) ã‚’é…ç½®
3. RunPod Endpoint ã‹ã‚‰å‘¼ã³å‡ºã—

API ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
- ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: prompt, max_tokens, temperature, top_k, top_p
- ãƒ¢ãƒ‡ãƒ«è¨­å®š: mode, num_neurons, hidden_dim, connection_density, etc.
"""

import runpod
import torch
import os
import traceback
import json

from neuroq_model import (
    NeuroQGenerator, 
    NeuroQModel, 
    NeuroQTokenizer, 
    NeuroQConfig,
    create_neuroq_brain,
    create_neuroq_layered,
)

# ========================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
# ========================================

MODEL_PATH = os.environ.get("NEUROQ_MODEL_PATH", "neuroq_model.pt")
TOKENIZER_PATH = os.environ.get("NEUROQ_TOKENIZER_PATH", "neuroq_tokenizer.json")
DEFAULT_MODE = os.environ.get("NEUROQ_MODE", "layered")  # 'brain' or 'layered'

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«è¨­å®š
DEFAULT_CONFIG = {
    # å…±é€š
    "embed_dim": int(os.environ.get("NEUROQ_EMBED_DIM", "128")),
    "num_layers": int(os.environ.get("NEUROQ_NUM_LAYERS", "3")),
    "dropout": float(os.environ.get("NEUROQ_DROPOUT", "0.1")),
    "max_seq_len": int(os.environ.get("NEUROQ_MAX_SEQ_LEN", "256")),
    
    # Brain Mode
    "num_neurons": int(os.environ.get("NEUROQ_NUM_NEURONS", "100")),
    "connection_density": float(os.environ.get("NEUROQ_CONNECTION_DENSITY", "0.25")),
    "lambda_entangle_brain": float(os.environ.get("NEUROQ_LAMBDA_BRAIN", "0.35")),
    
    # Layered Mode
    "hidden_dim": int(os.environ.get("NEUROQ_HIDDEN_DIM", "256")),
    "num_heads": int(os.environ.get("NEUROQ_NUM_HEADS", "4")),
    "lambda_entangle_layered": float(os.environ.get("NEUROQ_LAMBDA_LAYERED", "0.5")),
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å­¦ç¿’è¨­å®š
DEFAULT_TRAINING_CONFIG = {
    # ã‚¨ãƒãƒƒã‚¯ãƒ»ã‚¹ãƒ†ãƒƒãƒ—é–¢é€£
    "epochs": int(os.environ.get("NEUROQ_EPOCHS", "10")),
    "batch_size": int(os.environ.get("NEUROQ_BATCH_SIZE", "32")),
    "gradient_accumulation_steps": int(os.environ.get("NEUROQ_GRAD_ACCUM_STEPS", "1")),
    
    # å­¦ç¿’ç‡é–¢é€£
    "learning_rate": float(os.environ.get("NEUROQ_LEARNING_RATE", "1e-4")),
    "min_learning_rate": float(os.environ.get("NEUROQ_MIN_LR", "1e-6")),
    "weight_decay": float(os.environ.get("NEUROQ_WEIGHT_DECAY", "0.01")),
    "warmup_steps": int(os.environ.get("NEUROQ_WARMUP_STEPS", "100")),
    "lr_scheduler": os.environ.get("NEUROQ_LR_SCHEDULER", "cosine"),  # cosine, linear, constant
    
    # Temperatureé–¢é€£ï¼ˆç”Ÿæˆæ™‚ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼‰
    "max_temperature": float(os.environ.get("NEUROQ_MAX_TEMPERATURE", "1.5")),
    "min_temperature": float(os.environ.get("NEUROQ_MIN_TEMPERATURE", "0.1")),
    "temperature_decay": os.environ.get("NEUROQ_TEMP_DECAY", "linear"),  # linear, exponential, cosine
    
    # æ­£å‰‡åŒ–ãƒ»æœ€é©åŒ–
    "max_grad_norm": float(os.environ.get("NEUROQ_MAX_GRAD_NORM", "1.0")),
    "label_smoothing": float(os.environ.get("NEUROQ_LABEL_SMOOTHING", "0.1")),
    
    # è©•ä¾¡ãƒ»ä¿å­˜
    "eval_steps": int(os.environ.get("NEUROQ_EVAL_STEPS", "500")),
    "save_steps": int(os.environ.get("NEUROQ_SAVE_STEPS", "1000")),
    "logging_steps": int(os.environ.get("NEUROQ_LOGGING_STEPS", "100")),
    
    # æ—©æœŸåœæ­¢
    "early_stopping_patience": int(os.environ.get("NEUROQ_EARLY_STOPPING", "3")),
    "early_stopping_threshold": float(os.environ.get("NEUROQ_EARLY_STOPPING_THRESHOLD", "0.001")),
    
    # ãƒ‡ãƒ¼ã‚¿é–¢é€£
    "train_split": float(os.environ.get("NEUROQ_TRAIN_SPLIT", "0.9")),
    "shuffle": True,
    "seed": int(os.environ.get("NEUROQ_SEED", "42")),
}

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
if torch.cuda.is_available():
    DEVICE = "cuda"
    print(f"ğŸ® CUDA GPU ã‚’ä½¿ç”¨: {torch.cuda.get_device_name(0)}")
elif torch.backends.mps.is_available():
    DEVICE = "mps"
    print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
else:
    DEVICE = "cpu"
    print("ğŸ’» CPU ã‚’ä½¿ç”¨")

# ========================================
# ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
# ========================================

# ãƒ¢ãƒ¼ãƒ‰ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
model_cache = {}

def get_config_key(config_params: dict) -> str:
    """è¨­å®šã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
    return json.dumps(config_params, sort_keys=True)


def load_model(mode: str = None, config_params: dict = None):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
    
    Args:
        mode: 'brain' or 'layered' (Noneã®å ´åˆã¯DEFAULT_MODEã‚’ä½¿ç”¨)
        config_params: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ï¼‰
    """
    global model_cache
    
    if mode is None:
        mode = DEFAULT_MODE
    
    # è¨­å®šã‚’ãƒãƒ¼ã‚¸
    params = DEFAULT_CONFIG.copy()
    if config_params:
        params.update(config_params)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    cache_key = f"{mode}_{get_config_key(params)}"
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ã‚Œã°ãã‚Œã‚’è¿”ã™
    if cache_key in model_cache:
        return model_cache[cache_key]
    
    print(f"ğŸ“¥ NeuroQ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    print(f"   Mode: {mode}")
    print(f"   Config: {json.dumps(params, indent=2)}")
    print(f"   Device: {DEVICE}")
    
    # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    mode_model_path = f"neuroq_{mode}_model.pt"
    actual_model_path = mode_model_path if os.path.exists(mode_model_path) else MODEL_PATH
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if not os.path.exists(actual_model_path):
        print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {actual_model_path}")
        print("   ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™")
        
        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        if mode == 'brain':
            config = NeuroQConfig(
                mode='brain',
                vocab_size=2000,
                embed_dim=params['embed_dim'],
                num_neurons=params['num_neurons'],
                hidden_dim=params['num_neurons'] * 2,
                num_heads=params.get('num_heads', 4),
                num_layers=params['num_layers'],
                max_seq_len=params['max_seq_len'],
                dropout=params['dropout'],
                connection_density=params['connection_density'],
                lambda_entangle=params['lambda_entangle_brain'],
            )
        else:  # layered
            config = NeuroQConfig(
                mode='layered',
                vocab_size=2000,
                embed_dim=params['embed_dim'],
                hidden_dim=params['hidden_dim'],
                num_heads=params['num_heads'],
                num_layers=params['num_layers'],
                max_seq_len=params['max_seq_len'],
                dropout=params['dropout'],
                lambda_entangle=params['lambda_entangle_layered'],
            )
        
        model = NeuroQModel(config)
        tokenizer = NeuroQTokenizer(vocab_size=2000)
        
        # åŸºæœ¬çš„ãªèªå½™ã‚’æ§‹ç¯‰
        basic_texts = [
            "ã“ã‚“ã«ã¡ã¯ã€ç§ã¯NeuroQã§ã™ã€‚é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIã§ã™ã€‚",
            "Hello, I am NeuroQ. A generative AI based on Quantum-Bit Neural Network.",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯æ¬¡ä¸–ä»£ã®è¨ˆç®—æŠ€è¡“ã§ã™ã€‚",
            "äººå·¥çŸ¥èƒ½ã¯ç§ãŸã¡ã®ç”Ÿæ´»ã‚’å¤‰é©ã—ã¦ã„ã¾ã™ã€‚",
            "QBNNã¯é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚",
        ]
        tokenizer.build_vocab(basic_texts)
        
        generator = NeuroQGenerator(model, tokenizer, DEVICE)
        print(f"âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº† (Mode: {mode})")
    else:
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        generator = NeuroQGenerator.load(actual_model_path, TOKENIZER_PATH, DEVICE)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†: {actual_model_path}")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    info = generator.get_model_info()
    print(f"   ãƒ¢ãƒ¼ãƒ‰: {info.get('mode', 'unknown')}")
    print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info['num_params']:,}")
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {info['embed_dim']}")
    print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {info['hidden_dim']}")
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {info.get('num_neurons', 'N/A')}")
    print(f"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {info['num_layers']}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    model_cache[cache_key] = generator
    
    return generator


# ========================================
# RunPod Handler
# ========================================

def handler(job):
    """
    RunPod Serverless ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    å…¥åŠ›JSONå½¢å¼:
    {
        "input": {
            // === ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
            "prompt": "ç”Ÿæˆã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",  // å¿…é ˆ
            "max_tokens": 128,        // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰
            "temperature": 0.7,       // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰
            "top_k": 40,              // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 40ï¼‰
            "top_p": 0.9,             // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.9ï¼‰
            "repetition_penalty": 1.2 // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.2ï¼‰
            
            // === ãƒ¢ãƒ‡ãƒ«è¨­å®š ===
            "mode": "brain",          // ã‚ªãƒ—ã‚·ãƒ§ãƒ³: "brain" or "layered"
            
            // Brain Mode å°‚ç”¨
            "num_neurons": 100,       // ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
            "connection_density": 0.25, // æ¥ç¶šå¯†åº¦ (0.0-1.0)
            "lambda_entangle": 0.35,  // é‡å­ã‚‚ã¤ã‚Œå¼·åº¦
            
            // Layered Mode å°‚ç”¨
            "hidden_dim": 256,        // éš ã‚Œå±¤æ¬¡å…ƒ
            "num_heads": 4,           // ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
            
            // å…±é€š
            "embed_dim": 128,         // åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
            "num_layers": 3,          // ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
            
            // === å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
            "epochs": 10,             // ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
            "batch_size": 32,         // ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 32ï¼‰
            "learning_rate": 1e-4,    // å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-4ï¼‰
            "min_learning_rate": 1e-6, // æœ€å°å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-6ï¼‰
            "weight_decay": 0.01,     // é‡ã¿æ¸›è¡°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.01ï¼‰
            "warmup_steps": 100,      // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰
            "lr_scheduler": "cosine", // LRã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼: cosine, linear, constant
            "max_temperature": 1.5,   // æœ€å¤§æ¸©åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.5ï¼‰
            "min_temperature": 0.1,   // æœ€å°æ¸©åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰
            "temperature_decay": "linear", // æ¸©åº¦æ¸›è¡°: linear, exponential, cosine
            "max_grad_norm": 1.0,     // å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰
            "label_smoothing": 0.1,   // ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰
            "gradient_accumulation_steps": 1, // å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—
            "eval_steps": 500,        // è©•ä¾¡é–“éš”ã‚¹ãƒ†ãƒƒãƒ—
            "save_steps": 1000,       // ä¿å­˜é–“éš”ã‚¹ãƒ†ãƒƒãƒ—
            "logging_steps": 100,     // ãƒ­ã‚°é–“éš”ã‚¹ãƒ†ãƒƒãƒ—
            "early_stopping_patience": 3,    // æ—©æœŸåœæ­¢patience
            "early_stopping_threshold": 0.001, // æ—©æœŸåœæ­¢é–¾å€¤
            "train_split": 0.9,       // å­¦ç¿’/æ¤œè¨¼åˆ†å‰²æ¯”ç‡
            "seed": 42,               // ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        }
    }
    
    å‡ºåŠ›JSONå½¢å¼:
    {
        "prompt": "å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        "output": "ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ",
        "model_info": {
            "mode": "brain" or "layered",
            "num_neurons": 100,
            "num_params": 123456,
            ...
        }
    }
    """
    try:
        # å…¥åŠ›ã‚’å–å¾—
        job_input = job.get("input", {})
        
        # ãƒ¢ãƒ¼ãƒ‰è¨­å®š
        mode = job_input.get("mode", DEFAULT_MODE)
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        config_params = {}
        
        # å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if "embed_dim" in job_input:
            config_params["embed_dim"] = int(job_input["embed_dim"])
        if "num_layers" in job_input:
            config_params["num_layers"] = int(job_input["num_layers"])
        if "dropout" in job_input:
            config_params["dropout"] = float(job_input["dropout"])
        if "max_seq_len" in job_input:
            config_params["max_seq_len"] = int(job_input["max_seq_len"])
        
        # Brain Mode å°‚ç”¨
        if "num_neurons" in job_input:
            config_params["num_neurons"] = int(job_input["num_neurons"])
        if "connection_density" in job_input:
            config_params["connection_density"] = float(job_input["connection_density"])
        if "lambda_entangle" in job_input and mode == "brain":
            config_params["lambda_entangle_brain"] = float(job_input["lambda_entangle"])
        
        # Layered Mode å°‚ç”¨
        if "hidden_dim" in job_input:
            config_params["hidden_dim"] = int(job_input["hidden_dim"])
        if "num_heads" in job_input:
            config_params["num_heads"] = int(job_input["num_heads"])
        if "lambda_entangle" in job_input and mode == "layered":
            config_params["lambda_entangle_layered"] = float(job_input["lambda_entangle"])
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        training_params = {}
        
        # ã‚¨ãƒãƒƒã‚¯ãƒ»ãƒãƒƒãƒé–¢é€£
        if "epochs" in job_input:
            training_params["epochs"] = int(job_input["epochs"])
        if "batch_size" in job_input:
            training_params["batch_size"] = int(job_input["batch_size"])
        if "gradient_accumulation_steps" in job_input:
            training_params["gradient_accumulation_steps"] = int(job_input["gradient_accumulation_steps"])
        
        # å­¦ç¿’ç‡é–¢é€£
        if "learning_rate" in job_input:
            training_params["learning_rate"] = float(job_input["learning_rate"])
        if "min_learning_rate" in job_input:
            training_params["min_learning_rate"] = float(job_input["min_learning_rate"])
        if "weight_decay" in job_input:
            training_params["weight_decay"] = float(job_input["weight_decay"])
        if "warmup_steps" in job_input:
            training_params["warmup_steps"] = int(job_input["warmup_steps"])
        if "lr_scheduler" in job_input:
            training_params["lr_scheduler"] = str(job_input["lr_scheduler"])
        
        # Temperatureé–¢é€£
        if "max_temperature" in job_input:
            training_params["max_temperature"] = float(job_input["max_temperature"])
        if "min_temperature" in job_input:
            training_params["min_temperature"] = float(job_input["min_temperature"])
        if "temperature_decay" in job_input:
            training_params["temperature_decay"] = str(job_input["temperature_decay"])
        
        # æ­£å‰‡åŒ–ãƒ»æœ€é©åŒ–
        if "max_grad_norm" in job_input:
            training_params["max_grad_norm"] = float(job_input["max_grad_norm"])
        if "label_smoothing" in job_input:
            training_params["label_smoothing"] = float(job_input["label_smoothing"])
        
        # è©•ä¾¡ãƒ»ä¿å­˜
        if "eval_steps" in job_input:
            training_params["eval_steps"] = int(job_input["eval_steps"])
        if "save_steps" in job_input:
            training_params["save_steps"] = int(job_input["save_steps"])
        if "logging_steps" in job_input:
            training_params["logging_steps"] = int(job_input["logging_steps"])
        
        # æ—©æœŸåœæ­¢
        if "early_stopping_patience" in job_input:
            training_params["early_stopping_patience"] = int(job_input["early_stopping_patience"])
        if "early_stopping_threshold" in job_input:
            training_params["early_stopping_threshold"] = float(job_input["early_stopping_threshold"])
        
        # ãƒ‡ãƒ¼ã‚¿é–¢é€£
        if "train_split" in job_input:
            training_params["train_split"] = float(job_input["train_split"])
        if "shuffle" in job_input:
            training_params["shuffle"] = bool(job_input["shuffle"])
        if "seed" in job_input:
            training_params["seed"] = int(job_input["seed"])
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        gen = load_model(mode, config_params if config_params else None)
        
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        prompt = job_input.get("prompt", "")
        max_tokens = int(job_input.get("max_tokens", 128))
        temperature = float(job_input.get("temperature", 0.7))
        top_k = int(job_input.get("top_k", 40))
        top_p = float(job_input.get("top_p", 0.9))
        repetition_penalty = float(job_input.get("repetition_penalty", 1.2))
        
        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not prompt:
            return {"error": "prompt is required"}
        
        if max_tokens < 1 or max_tokens > 1024:
            max_tokens = min(max(1, max_tokens), 1024)
        
        if temperature < 0.1 or temperature > 2.0:
            temperature = min(max(0.1, temperature), 2.0)
        
        print(f"ğŸ“ ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ:")
        print(f"   Mode: {mode}")
        print(f"   Prompt: {prompt[:50]}...")
        print(f"   Max tokens: {max_tokens}")
        print(f"   Temperature: {temperature}")
        if config_params:
            print(f"   Custom config: {json.dumps(config_params)}")
        if training_params:
            print(f"   Training params: {json.dumps(training_params)}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        output_text = gen.generate(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
        )
        
        print(f"âœ… ç”Ÿæˆå®Œäº†: {len(output_text)} æ–‡å­—")
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ãƒãƒ¼ã‚¸
        merged_training_params = DEFAULT_TRAINING_CONFIG.copy()
        merged_training_params.update(training_params)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        return {
            "prompt": prompt,
            "output": output_text,
            "model_info": gen.get_model_info(),
            "config": {
                "mode": mode,
                **config_params
            },
            "training_config": merged_training_params
        }
        
    except Exception as e:
        error_msg = f"Error: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}


# ========================================
# å­¦ç¿’ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def train_handler(job):
    """
    ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    å…¥åŠ›JSONå½¢å¼:
    {
        "input": {
            "training_data": ["ãƒ†ã‚­ã‚¹ãƒˆ1", "ãƒ†ã‚­ã‚¹ãƒˆ2", ...],  // å¿…é ˆ: å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            "mode": "layered",        // ã‚ªãƒ—ã‚·ãƒ§ãƒ³: "brain" or "layered"
            
            // å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            "epochs": 10,
            "batch_size": 32,
            "learning_rate": 1e-4,
            "min_learning_rate": 1e-6,
            "warmup_steps": 100,
            "lr_scheduler": "cosine",
            "max_temperature": 1.5,
            "min_temperature": 0.1,
            "temperature_decay": "linear",
            "max_grad_norm": 1.0,
            "label_smoothing": 0.1,
            ...
        }
    }
    """
    try:
        job_input = job.get("input", {})
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        training_data = job_input.get("training_data", [])
        if not training_data:
            return {"error": "training_data is required"}
        
        mode = job_input.get("mode", DEFAULT_MODE)
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦ãƒãƒ¼ã‚¸
        training_config = DEFAULT_TRAINING_CONFIG.copy()
        
        training_param_keys = [
            "epochs", "batch_size", "gradient_accumulation_steps",
            "learning_rate", "min_learning_rate", "weight_decay", "warmup_steps", "lr_scheduler",
            "max_temperature", "min_temperature", "temperature_decay",
            "max_grad_norm", "label_smoothing",
            "eval_steps", "save_steps", "logging_steps",
            "early_stopping_patience", "early_stopping_threshold",
            "train_split", "shuffle", "seed"
        ]
        
        for key in training_param_keys:
            if key in job_input:
                value = job_input[key]
                # å‹å¤‰æ›
                if key in ["epochs", "batch_size", "gradient_accumulation_steps", "warmup_steps", 
                           "eval_steps", "save_steps", "logging_steps", "early_stopping_patience", "seed"]:
                    training_config[key] = int(value)
                elif key in ["learning_rate", "min_learning_rate", "weight_decay", 
                             "max_temperature", "min_temperature", "max_grad_norm", 
                             "label_smoothing", "early_stopping_threshold", "train_split"]:
                    training_config[key] = float(value)
                elif key == "shuffle":
                    training_config[key] = bool(value)
                else:
                    training_config[key] = str(value)
        
        print(f"ğŸ“ å­¦ç¿’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:")
        print(f"   Mode: {mode}")
        print(f"   Training data size: {len(training_data)} samples")
        print(f"   Epochs: {training_config['epochs']}")
        print(f"   Batch size: {training_config['batch_size']}")
        print(f"   Learning rate: {training_config['learning_rate']}")
        print(f"   Max/Min temperature: {training_config['max_temperature']}/{training_config['min_temperature']}")
        print(f"   Temperature decay: {training_config['temperature_decay']}")
        print(f"   LR scheduler: {training_config['lr_scheduler']}")
        print(f"   Warmup steps: {training_config['warmup_steps']}")
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        config_params = {}
        model_param_keys = [
            "embed_dim", "num_layers", "dropout", "max_seq_len",
            "num_neurons", "connection_density", "hidden_dim", "num_heads"
        ]
        for key in model_param_keys:
            if key in job_input:
                if key in ["dropout", "connection_density"]:
                    config_params[key] = float(job_input[key])
                else:
                    config_params[key] = int(job_input[key])
        
        if "lambda_entangle" in job_input:
            if mode == "brain":
                config_params["lambda_entangle_brain"] = float(job_input["lambda_entangle"])
            else:
                config_params["lambda_entangle_layered"] = float(job_input["lambda_entangle"])
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        gen = load_model(mode, config_params if config_params else None)
        
        # æ³¨: å®Ÿéš›ã®å­¦ç¿’å‡¦ç†ã¯NeuroQGeneratorã«å®Ÿè£…ãŒå¿…è¦
        # ã“ã“ã§ã¯å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å—ã‘æ¸¡ã—ã¨è¨­å®šã®ç¢ºèªã®ã¿
        
        return {
            "status": "training_config_ready",
            "message": "Training parameters received and validated",
            "mode": mode,
            "training_data_count": len(training_data),
            "model_config": config_params if config_params else DEFAULT_CONFIG,
            "training_config": training_config,
            "model_info": gen.get_model_info(),
        }
        
    except Exception as e:
        error_msg = f"Training Error: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}


# ========================================
# é‡å­æƒ…å ±ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def quantum_info(job):
    """é‡å­ã‚‚ã¤ã‚Œæƒ…å ±ã‚’å–å¾—"""
    try:
        job_input = job.get("input", {})
        mode = job_input.get("mode", DEFAULT_MODE)
        
        gen = load_model(mode)
        model_info = gen.get_model_info()
        quantum_info = gen.model.get_quantum_info()
        
        return {
            "status": "success",
            "mode": model_info.get('mode', 'unknown'),
            "model_info": model_info,
            "quantum_info": quantum_info,
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
        }


# ========================================
# ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def model_config(job):
    """
    ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã¨åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    """
    try:
        return {
            "status": "success",
            "default_mode": DEFAULT_MODE,
            "default_config": DEFAULT_CONFIG,
            "default_training_config": DEFAULT_TRAINING_CONFIG,
            "device": DEVICE,
            "cached_models": list(model_cache.keys()),
            "available_options": {
                "common": {
                    "embed_dim": "åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰",
                    "num_layers": "ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰",
                    "dropout": "ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰",
                    "max_seq_len": "æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 256ï¼‰",
                },
                "brain_mode": {
                    "num_neurons": "ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰",
                    "connection_density": "æ¥ç¶šå¯†åº¦ 0.0-1.0ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.25ï¼‰",
                    "lambda_entangle": "é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.35ï¼‰",
                },
                "layered_mode": {
                    "hidden_dim": "éš ã‚Œå±¤æ¬¡å…ƒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 256ï¼‰",
                    "num_heads": "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4ï¼‰",
                    "lambda_entangle": "é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5ï¼‰",
                },
                "training": {
                    "epochs": "ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰",
                    "batch_size": "ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 32ï¼‰",
                    "gradient_accumulation_steps": "å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰",
                    "learning_rate": "å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-4ï¼‰",
                    "min_learning_rate": "æœ€å°å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-6ï¼‰",
                    "weight_decay": "é‡ã¿æ¸›è¡°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.01ï¼‰",
                    "warmup_steps": "ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰",
                    "lr_scheduler": "å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼: cosine, linear, constantï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: cosineï¼‰",
                    "max_temperature": "æœ€å¤§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.5ï¼‰",
                    "min_temperature": "æœ€å°ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰",
                    "temperature_decay": "æ¸©åº¦æ¸›è¡°æ–¹å¼: linear, exponential, cosineï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: linearï¼‰",
                    "max_grad_norm": "å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰",
                    "label_smoothing": "ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰",
                    "eval_steps": "è©•ä¾¡é–“éš”ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 500ï¼‰",
                    "save_steps": "ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–“éš”ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰",
                    "logging_steps": "ãƒ­ã‚°å‡ºåŠ›é–“éš”ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰",
                    "early_stopping_patience": "æ—©æœŸåœæ­¢patienceï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰",
                    "early_stopping_threshold": "æ—©æœŸåœæ­¢é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.001ï¼‰",
                    "train_split": "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æ¯”ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.9ï¼‰",
                    "shuffle": "ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: trueï¼‰",
                    "seed": "ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 42ï¼‰",
                },
            },
            "example_request": {
                "input": {
                    "prompt": "ã“ã‚“ã«ã¡ã¯",
                    "mode": "brain",
                    "num_neurons": 200,
                    "connection_density": 0.3,
                    "max_tokens": 64,
                    "temperature": 0.7
                }
            },
            "example_training_request": {
                "input": {
                    "action": "train",
                    "training_data": ["ãƒ†ã‚­ã‚¹ãƒˆ1", "ãƒ†ã‚­ã‚¹ãƒˆ2", "..."],
                    "mode": "layered",
                    "epochs": 20,
                    "batch_size": 16,
                    "learning_rate": 5e-5,
                    "max_temperature": 1.2,
                    "min_temperature": 0.3,
                    "warmup_steps": 200,
                    "early_stopping_patience": 5
                }
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
        }


# ========================================
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def health_check(job):
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        gen = load_model()
        return {
            "status": "healthy",
            "model_loaded": gen is not None,
            "device": DEVICE,
            "model_info": gen.get_model_info() if gen else None,
            "cached_models": len(model_cache),
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
        }


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ âš›ï¸ NeuroQ RunPod Serverless Worker")
    print("   Brain Mode: è„³å‹æ•£åœ¨QBNN")
    print("   Layered Mode: å±¤çŠ¶QBNN-Transformer")
    print("=" * 60)
    print("\nğŸ“‹ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«è¨­å®š:")
    print(f"   Mode: {DEFAULT_MODE}")
    for key, value in DEFAULT_CONFIG.items():
        print(f"   {key}: {value}")
    
    print("\nğŸ“š ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå­¦ç¿’è¨­å®š:")
    for key, value in DEFAULT_TRAINING_CONFIG.items():
        print(f"   {key}: {value}")
    print()
    
    # èµ·å‹•æ™‚ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
    load_model()
    
    # RunPod Serverless ã‚’é–‹å§‹
    runpod.serverless.start({
        "handler": handler,
        "train": train_handler,
    })
