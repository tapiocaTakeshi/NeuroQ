#!/usr/bin/env python3
"""
NeuroQ RunPod Serverless Handler
================================
RunPod Serverless Endpointç”¨ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼

ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰:
- Brain Mode: è„³å‹æ•£åœ¨QBNN
- Layered Mode: å±¤çŠ¶QBNN-Transformer

ä½¿ç”¨æ–¹æ³•:
1. ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ neuroq_model.py ã‚’RunPodã«ãƒ‡ãƒ—ãƒ­ã‚¤
2. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (neuroq_model.pt, neuroq_tokenizer.json) ã‚’é…ç½®
3. RunPod Endpoint ã‹ã‚‰å‘¼ã³å‡ºã—

API ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
- ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: prompt, max_tokens, temperature, top_k, top_p
- ãƒ¢ãƒ‡ãƒ«è¨­å®š: mode, num_neurons, hidden_dim, connection_density, etc.
"""

import runpod
import torch
import os
import traceback
import json

from neuroq_model import (
    NeuroQGenerator, 
    NeuroQModel, 
    NeuroQTokenizer, 
    NeuroQConfig,
    create_neuroq_brain,
    create_neuroq_layered,
)

# ========================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
# ========================================

MODEL_PATH = os.environ.get("NEUROQ_MODEL_PATH", "neuroq_model.pt")
TOKENIZER_PATH = os.environ.get("NEUROQ_TOKENIZER_PATH", "neuroq_tokenizer.json")
DEFAULT_MODE = os.environ.get("NEUROQ_MODE", "layered")  # 'brain' or 'layered'

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«è¨­å®š
DEFAULT_CONFIG = {
    # å…±é€š
    "embed_dim": int(os.environ.get("NEUROQ_EMBED_DIM", "128")),
    "num_layers": int(os.environ.get("NEUROQ_NUM_LAYERS", "3")),
    "dropout": float(os.environ.get("NEUROQ_DROPOUT", "0.1")),
    "max_seq_len": int(os.environ.get("NEUROQ_MAX_SEQ_LEN", "256")),
    
    # Brain Mode
    "num_neurons": int(os.environ.get("NEUROQ_NUM_NEURONS", "100")),
    "connection_density": float(os.environ.get("NEUROQ_CONNECTION_DENSITY", "0.25")),
    "lambda_entangle_brain": float(os.environ.get("NEUROQ_LAMBDA_BRAIN", "0.35")),
    
    # Layered Mode
    "hidden_dim": int(os.environ.get("NEUROQ_HIDDEN_DIM", "256")),
    "num_heads": int(os.environ.get("NEUROQ_NUM_HEADS", "4")),
    "lambda_entangle_layered": float(os.environ.get("NEUROQ_LAMBDA_LAYERED", "0.5")),
}

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
if torch.cuda.is_available():
    DEVICE = "cuda"
    print(f"ğŸ® CUDA GPU ã‚’ä½¿ç”¨: {torch.cuda.get_device_name(0)}")
elif torch.backends.mps.is_available():
    DEVICE = "mps"
    print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
else:
    DEVICE = "cpu"
    print("ğŸ’» CPU ã‚’ä½¿ç”¨")

# ========================================
# ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
# ========================================

# ãƒ¢ãƒ¼ãƒ‰ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
model_cache = {}

def get_config_key(config_params: dict) -> str:
    """è¨­å®šã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
    return json.dumps(config_params, sort_keys=True)


def load_model(mode: str = None, config_params: dict = None):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
    
    Args:
        mode: 'brain' or 'layered' (Noneã®å ´åˆã¯DEFAULT_MODEã‚’ä½¿ç”¨)
        config_params: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ï¼‰
    """
    global model_cache
    
    if mode is None:
        mode = DEFAULT_MODE
    
    # è¨­å®šã‚’ãƒãƒ¼ã‚¸
    params = DEFAULT_CONFIG.copy()
    if config_params:
        params.update(config_params)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    cache_key = f"{mode}_{get_config_key(params)}"
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ã‚Œã°ãã‚Œã‚’è¿”ã™
    if cache_key in model_cache:
        return model_cache[cache_key]
    
    print(f"ğŸ“¥ NeuroQ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    print(f"   Mode: {mode}")
    print(f"   Config: {json.dumps(params, indent=2)}")
    print(f"   Device: {DEVICE}")
    
    # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    mode_model_path = f"neuroq_{mode}_model.pt"
    actual_model_path = mode_model_path if os.path.exists(mode_model_path) else MODEL_PATH
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if not os.path.exists(actual_model_path):
        print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {actual_model_path}")
        print("   ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™")
        
        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        if mode == 'brain':
            config = NeuroQConfig(
                mode='brain',
                vocab_size=2000,
                embed_dim=params['embed_dim'],
                num_neurons=params['num_neurons'],
                hidden_dim=params['num_neurons'] * 2,
                num_heads=params.get('num_heads', 4),
                num_layers=params['num_layers'],
                max_seq_len=params['max_seq_len'],
                dropout=params['dropout'],
                connection_density=params['connection_density'],
                lambda_entangle=params['lambda_entangle_brain'],
            )
        else:  # layered
            config = NeuroQConfig(
                mode='layered',
                vocab_size=2000,
                embed_dim=params['embed_dim'],
                hidden_dim=params['hidden_dim'],
                num_heads=params['num_heads'],
                num_layers=params['num_layers'],
                max_seq_len=params['max_seq_len'],
                dropout=params['dropout'],
                lambda_entangle=params['lambda_entangle_layered'],
            )
        
        model = NeuroQModel(config)
        tokenizer = NeuroQTokenizer(vocab_size=2000)
        
        # åŸºæœ¬çš„ãªèªå½™ã‚’æ§‹ç¯‰
        basic_texts = [
            "ã“ã‚“ã«ã¡ã¯ã€ç§ã¯NeuroQã§ã™ã€‚é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIã§ã™ã€‚",
            "Hello, I am NeuroQ. A generative AI based on Quantum-Bit Neural Network.",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯æ¬¡ä¸–ä»£ã®è¨ˆç®—æŠ€è¡“ã§ã™ã€‚",
            "äººå·¥çŸ¥èƒ½ã¯ç§ãŸã¡ã®ç”Ÿæ´»ã‚’å¤‰é©ã—ã¦ã„ã¾ã™ã€‚",
            "QBNNã¯é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚",
        ]
        tokenizer.build_vocab(basic_texts)
        
        generator = NeuroQGenerator(model, tokenizer, DEVICE)
        print(f"âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº† (Mode: {mode})")
    else:
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        generator = NeuroQGenerator.load(actual_model_path, TOKENIZER_PATH, DEVICE)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†: {actual_model_path}")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    info = generator.get_model_info()
    print(f"   ãƒ¢ãƒ¼ãƒ‰: {info.get('mode', 'unknown')}")
    print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info['num_params']:,}")
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {info['embed_dim']}")
    print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {info['hidden_dim']}")
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {info.get('num_neurons', 'N/A')}")
    print(f"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {info['num_layers']}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    model_cache[cache_key] = generator
    
    return generator


# ========================================
# RunPod Handler
# ========================================

def handler(job):
    """
    RunPod Serverless ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    å…¥åŠ›JSONå½¢å¼:
    {
        "input": {
            // === ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
            "prompt": "ç”Ÿæˆã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",  // å¿…é ˆ
            "max_tokens": 128,        // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰
            "temperature": 0.7,       // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰
            "top_k": 40,              // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 40ï¼‰
            "top_p": 0.9,             // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.9ï¼‰
            "repetition_penalty": 1.2 // ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.2ï¼‰
            
            // === ãƒ¢ãƒ‡ãƒ«è¨­å®š ===
            "mode": "brain",          // ã‚ªãƒ—ã‚·ãƒ§ãƒ³: "brain" or "layered"
            
            // Brain Mode å°‚ç”¨
            "num_neurons": 100,       // ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
            "connection_density": 0.25, // æ¥ç¶šå¯†åº¦ (0.0-1.0)
            "lambda_entangle": 0.35,  // é‡å­ã‚‚ã¤ã‚Œå¼·åº¦
            
            // Layered Mode å°‚ç”¨
            "hidden_dim": 256,        // éš ã‚Œå±¤æ¬¡å…ƒ
            "num_heads": 4,           // ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
            
            // å…±é€š
            "embed_dim": 128,         // åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
            "num_layers": 3,          // ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        }
    }
    
    å‡ºåŠ›JSONå½¢å¼:
    {
        "prompt": "å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        "output": "ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ",
        "model_info": {
            "mode": "brain" or "layered",
            "num_neurons": 100,
            "num_params": 123456,
            ...
        }
    }
    """
    try:
        # å…¥åŠ›ã‚’å–å¾—
        job_input = job.get("input", {})
        
        # ãƒ¢ãƒ¼ãƒ‰è¨­å®š
        mode = job_input.get("mode", DEFAULT_MODE)
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        config_params = {}
        
        # å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if "embed_dim" in job_input:
            config_params["embed_dim"] = int(job_input["embed_dim"])
        if "num_layers" in job_input:
            config_params["num_layers"] = int(job_input["num_layers"])
        if "dropout" in job_input:
            config_params["dropout"] = float(job_input["dropout"])
        if "max_seq_len" in job_input:
            config_params["max_seq_len"] = int(job_input["max_seq_len"])
        
        # Brain Mode å°‚ç”¨
        if "num_neurons" in job_input:
            config_params["num_neurons"] = int(job_input["num_neurons"])
        if "connection_density" in job_input:
            config_params["connection_density"] = float(job_input["connection_density"])
        if "lambda_entangle" in job_input and mode == "brain":
            config_params["lambda_entangle_brain"] = float(job_input["lambda_entangle"])
        
        # Layered Mode å°‚ç”¨
        if "hidden_dim" in job_input:
            config_params["hidden_dim"] = int(job_input["hidden_dim"])
        if "num_heads" in job_input:
            config_params["num_heads"] = int(job_input["num_heads"])
        if "lambda_entangle" in job_input and mode == "layered":
            config_params["lambda_entangle_layered"] = float(job_input["lambda_entangle"])
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        gen = load_model(mode, config_params if config_params else None)
        
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        prompt = job_input.get("prompt", "")
        max_tokens = int(job_input.get("max_tokens", 128))
        temperature = float(job_input.get("temperature", 0.7))
        top_k = int(job_input.get("top_k", 40))
        top_p = float(job_input.get("top_p", 0.9))
        repetition_penalty = float(job_input.get("repetition_penalty", 1.2))
        
        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not prompt:
            return {"error": "prompt is required"}
        
        if max_tokens < 1 or max_tokens > 1024:
            max_tokens = min(max(1, max_tokens), 1024)
        
        if temperature < 0.1 or temperature > 2.0:
            temperature = min(max(0.1, temperature), 2.0)
        
        print(f"ğŸ“ ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ:")
        print(f"   Mode: {mode}")
        print(f"   Prompt: {prompt[:50]}...")
        print(f"   Max tokens: {max_tokens}")
        print(f"   Temperature: {temperature}")
        if config_params:
            print(f"   Custom config: {json.dumps(config_params)}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        output_text = gen.generate(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
        )
        
        print(f"âœ… ç”Ÿæˆå®Œäº†: {len(output_text)} æ–‡å­—")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        return {
            "prompt": prompt,
            "output": output_text,
            "model_info": gen.get_model_info(),
            "config": {
                "mode": mode,
                **config_params
            }
        }
        
    except Exception as e:
        error_msg = f"Error: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}


# ========================================
# é‡å­æƒ…å ±ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def quantum_info(job):
    """é‡å­ã‚‚ã¤ã‚Œæƒ…å ±ã‚’å–å¾—"""
    try:
        job_input = job.get("input", {})
        mode = job_input.get("mode", DEFAULT_MODE)
        
        gen = load_model(mode)
        model_info = gen.get_model_info()
        quantum_info = gen.model.get_quantum_info()
        
        return {
            "status": "success",
            "mode": model_info.get('mode', 'unknown'),
            "model_info": model_info,
            "quantum_info": quantum_info,
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
        }


# ========================================
# ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def model_config(job):
    """
    ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã¨åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    """
    try:
        return {
            "status": "success",
            "default_mode": DEFAULT_MODE,
            "default_config": DEFAULT_CONFIG,
            "device": DEVICE,
            "cached_models": list(model_cache.keys()),
            "available_options": {
                "common": {
                    "embed_dim": "åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰",
                    "num_layers": "ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰",
                    "dropout": "ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰",
                    "max_seq_len": "æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 256ï¼‰",
                },
                "brain_mode": {
                    "num_neurons": "ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰",
                    "connection_density": "æ¥ç¶šå¯†åº¦ 0.0-1.0ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.25ï¼‰",
                    "lambda_entangle": "é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.35ï¼‰",
                },
                "layered_mode": {
                    "hidden_dim": "éš ã‚Œå±¤æ¬¡å…ƒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 256ï¼‰",
                    "num_heads": "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4ï¼‰",
                    "lambda_entangle": "é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5ï¼‰",
                },
            },
            "example_request": {
                "input": {
                    "prompt": "ã“ã‚“ã«ã¡ã¯",
                    "mode": "brain",
                    "num_neurons": 200,
                    "connection_density": 0.3,
                    "max_tokens": 64,
                    "temperature": 0.7
                }
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
        }


# ========================================
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def health_check(job):
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        gen = load_model()
        return {
            "status": "healthy",
            "model_loaded": gen is not None,
            "device": DEVICE,
            "model_info": gen.get_model_info() if gen else None,
            "cached_models": len(model_cache),
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
        }


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ âš›ï¸ NeuroQ RunPod Serverless Worker")
    print("   Brain Mode: è„³å‹æ•£åœ¨QBNN")
    print("   Layered Mode: å±¤çŠ¶QBNN-Transformer")
    print("=" * 60)
    print("\nğŸ“‹ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š:")
    print(f"   Mode: {DEFAULT_MODE}")
    for key, value in DEFAULT_CONFIG.items():
        print(f"   {key}: {value}")
    print()
    
    # èµ·å‹•æ™‚ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
    load_model()
    
    # RunPod Serverless ã‚’é–‹å§‹
    runpod.serverless.start({
        "handler": handler,
    })
