#!/usr/bin/env python3
"""
NeuroQ RunPod Serverless Handler - Optimized Version
=====================================================
é«˜é€Ÿèµ·å‹• & å®‰å®šå‹•ä½œã®ãŸã‚ã®æœ€é©åŒ–æ¸ˆã¿ãƒãƒ³ãƒ‰ãƒ©ãƒ¼

ç‰¹å¾´:
- èµ·å‹•æ™‚ã«é‡ã„å‡¦ç†ã‚’ã—ãªã„ï¼ˆé«˜é€Ÿèµ·å‹•ï¼‰
- health checkã¯å³åº§ã«200ã‚’è¿”ã™
- ãƒ¢ãƒ‡ãƒ«ã¯åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã«lazy load
- vocab_sizeã®æ•´åˆæ€§ã‚’ä¿è¨¼
"""

import runpod
import torch
import os
import sys
import subprocess
import threading
import time
from pathlib import Path

# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆneuroq_pretrained.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ï¼‰
current_dir = str(Path(__file__).parent)
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

print("=" * 60)
print("âš›ï¸ NeuroQ RunPod Serverless - Starting...")
print("=" * 60)

# ========================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆèµ·å‹•æ™‚ã¯å…¨ã¦Noneï¼‰
# ========================================
model = None
is_initialized = False

# å­¦ç¿’çŠ¶æ…‹ç®¡ç†
pretrain_process = None
pretrain_status = "idle"  # idle, running, completed, error
pretrain_log_file = "training_openai.log"

# ä¼šè©±å±¥æ­´ç®¡ç†
conversation_sessions = {}  # session_id -> list of {role, content}

# è¨­å®š
VOCAB_SIZE = 8000
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä¼šè©±æŒ‡ç¤ºï¼‰
SYSTEM_PROMPT = """ã‚ãªãŸã¯è¦ªåˆ‡ã§æ­£ç¢ºãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«çŸ­ãæ­£ç¢ºã«ç­”ãˆã‚‹
2. ã‚ã‹ã‚‰ãªã„ã“ã¨ã¯è³ªå•ã™ã‚‹
3. èã‹ã‚ŒãŸã“ã¨ã ã‘ã«ç­”ãˆã‚‹ï¼ˆä½™è¨ˆãªæƒ…å ±ã‚’è¿½åŠ ã—ãªã„ï¼‰
4. å‰ã®æ–‡è„ˆã‚’è¸ã¾ãˆã¦è¿”ç­”ã™ã‚‹"""

print(f"ğŸ“Š Device: {DEVICE}")
print(f"ğŸ“Š CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}")


# ========================================
# äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
# ========================================
PRETRAINED_MODEL_PATH = "neuroq_pretrained.pt"


# ========================================
# PyTorch .ptãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
# ========================================
def is_lfs_pointer_file(file_path: str) -> bool:
    """
    Check if a file is a Git LFS pointer file.

    Args:
        file_path: Path to the file to check

    Returns:
        bool: True if the file is an LFS pointer, False otherwise
    """
    try:
        # LFS pointer files are typically very small (< 200 bytes)
        file_size = os.path.getsize(file_path)
        if file_size > 1024:
            return False

        # Read the first few bytes to check for LFS marker
        with open(file_path, 'rb') as f:
            header = f.read(50).decode('utf-8', errors='ignore')
            return header.startswith('version https://git-lfs.github.com/spec/')
    except:
        return False


def validate_pt_file(file_path: str) -> dict:
    """
    PyTorchã®.ptãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ã™ã‚‹

    Args:
        file_path: æ¤œè¨¼ã™ã‚‹ptãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        dict: æ¤œè¨¼çµæœ
            - valid: bool - ãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ‰åŠ¹ã‹ã©ã†ã‹
            - error: str - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆï¼‰
            - info: dict - ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
    """
    result = {
        "valid": False,
        "error": None,
        "info": {}
    }

    try:
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if not os.path.exists(file_path):
            result["error"] = f"File not found: {file_path}"
            return result

        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        file_size = os.path.getsize(file_path)
        result["info"]["file_size_bytes"] = file_size
        result["info"]["file_size_mb"] = round(file_size / (1024 * 1024), 2)

        if file_size < 1024:  # 1KBæœªæº€
            # Check if it's a Git LFS pointer file
            if is_lfs_pointer_file(file_path):
                result["error"] = (
                    f"Git LFS pointer file detected ({file_size} bytes). "
                    f"Run 'python download_model.py' or 'git lfs pull' to download the actual model file."
                )
                result["info"]["is_lfs_pointer"] = True
            else:
                result["error"] = f"File too small ({file_size} bytes). Possibly corrupted or empty."
            return result

        if file_size > 5 * 1024 * 1024 * 1024:  # 5GBä»¥ä¸Š
            result["error"] = f"File too large ({result['info']['file_size_mb']} MB). May cause memory issues."
            print(f"âš ï¸ Warning: {result['error']}")
            # è­¦å‘Šã®ã¿ã§ç¶šè¡Œ

        # 3. PyTorchã§èª­ã¿è¾¼ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        print(f"ğŸ” Loading checkpoint from {file_path}...")
        try:
            checkpoint = torch.load(file_path, map_location='cpu')
        except Exception as e:
            result["error"] = f"Failed to load PyTorch checkpoint: {str(e)}"
            return result

        # 4. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å‹ãƒã‚§ãƒƒã‚¯
        if not isinstance(checkpoint, dict):
            result["error"] = f"Invalid checkpoint format. Expected dict, got {type(checkpoint).__name__}"
            return result

        result["info"]["checkpoint_keys"] = list(checkpoint.keys())

        # 5. å¿…é ˆã‚­ãƒ¼ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        required_keys = ['config', 'model_state_dict']
        missing_keys = [key for key in required_keys if key not in checkpoint]

        if missing_keys:
            result["error"] = f"Missing required keys: {missing_keys}. Found keys: {list(checkpoint.keys())}"
            return result

        # 6. Configæƒ…å ±ã®æ¤œè¨¼
        config = checkpoint['config']
        if not isinstance(config, dict):
            result["error"] = f"Invalid config format. Expected dict, got {type(config).__name__}"
            return result

        # å¿…é ˆã®configé …ç›®
        required_config_keys = ['vocab_size', 'embed_dim', 'hidden_dim', 'num_heads', 'num_layers']
        missing_config_keys = [key for key in required_config_keys if key not in config]

        if missing_config_keys:
            result["error"] = f"Missing required config keys: {missing_config_keys}"
            return result

        # Configæƒ…å ±ã‚’çµæœã«è¿½åŠ 
        result["info"]["config"] = {
            "vocab_size": config.get('vocab_size'),
            "embed_dim": config.get('embed_dim'),
            "hidden_dim": config.get('hidden_dim'),
            "num_heads": config.get('num_heads'),
            "num_layers": config.get('num_layers'),
            "max_seq_len": config.get('max_seq_len'),
            "dropout": config.get('dropout'),
            "lambda_entangle": config.get('lambda_entangle'),
        }

        # 7. Configå€¤ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        vocab_size = config.get('vocab_size', 0)
        if vocab_size < 100 or vocab_size > 100000:
            result["error"] = f"Invalid vocab_size: {vocab_size}. Expected range: 100-100000"
            return result

        embed_dim = config.get('embed_dim', 0)
        if embed_dim < 32 or embed_dim > 4096:
            result["error"] = f"Invalid embed_dim: {embed_dim}. Expected range: 32-4096"
            return result

        # 8. model_state_dictã®æ¤œè¨¼
        model_state_dict = checkpoint['model_state_dict']
        if not isinstance(model_state_dict, dict):
            result["error"] = f"Invalid model_state_dict format. Expected dict, got {type(model_state_dict).__name__}"
            return result

        result["info"]["num_model_parameters"] = len(model_state_dict)

        if len(model_state_dict) == 0:
            result["error"] = "model_state_dict is empty"
            return result

        # 9. ãã®ä»–ã®æƒ…å ±ã‚’è¿½åŠ 
        if 'epoch' in checkpoint:
            result["info"]["epoch"] = checkpoint['epoch']
        if 'optimizer_state_dict' in checkpoint:
            result["info"]["has_optimizer_state"] = True
        if 'loss' in checkpoint:
            result["info"]["loss"] = checkpoint['loss']

        # ã™ã¹ã¦ã®æ¤œè¨¼ã‚’ãƒ‘ã‚¹
        result["valid"] = True
        print(f"âœ… Validation passed for {file_path}")
        print(f"   File size: {result['info']['file_size_mb']} MB")
        print(f"   Vocab size: {result['info']['config']['vocab_size']}")
        print(f"   Embed dim: {result['info']['config']['embed_dim']}")
        print(f"   Model parameters: {result['info']['num_model_parameters']}")

        return result

    except Exception as e:
        result["error"] = f"Unexpected error during validation: {str(e)}"
        import traceback
        traceback.print_exc()
        return result


# ========================================
# Lazy Model Loadingï¼ˆåˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®ã¿ï¼‰
# ========================================
def initialize_model():
    """ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆåˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®ã¿å‘¼ã°ã‚Œã‚‹ï¼‰"""
    global model, is_initialized

    if is_initialized:
        return True

    print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–é–‹å§‹...")

    try:
        from neuroquantum_layered import NeuroQuantumAI, NeuroQuantum, NeuroQuantumConfig, NeuroQuantumTokenizer

        # ========================================
        # æ–¹æ³•1: neuroq_pretrained.py ã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰
        # ========================================
        try:
            print("ğŸ“¦ Importing neuroq_pretrained module from parent directory...")
            import neuroq_pretrained

            # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            nn_model, config_dict, checkpoint = neuroq_pretrained.load_pretrained_model(
                model_path=PRETRAINED_MODEL_PATH,
                device=DEVICE,
                verbose=True
            )

            if nn_model is not None and config_dict is not None:
                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
                tokenizer = NeuroQuantumTokenizer(
                    vocab_size=config_dict['vocab_size'],
                    model_file="neuroq_tokenizer.model"
                )

                # vocab_size ã®æ•´åˆæ€§ç¢ºèª
                tokenizer_vocab_size = tokenizer.actual_vocab_size or tokenizer.vocab_size
                config_vocab_size = config_dict['vocab_size']
                print(f"ğŸ” Vocab size validation:")
                print(f"   Config vocab_size: {config_vocab_size}")
                print(f"   Tokenizer actual_vocab_size: {tokenizer_vocab_size}")

                if config_vocab_size != tokenizer_vocab_size:
                    print(f"âŒ CRITICAL: vocab_size mismatch detected!")
                    print(f"   Model was trained with vocab_size={config_vocab_size}")
                    print(f"   But tokenizer has vocab_size={tokenizer_vocab_size}")
                    print(f"   âš ï¸ WARNING: This may cause generation errors!")
                    print(f"   Recommendation: Retrain the model with correct vocab_size.")
                    # Note: We continue with a warning, as the model might still be usable

                # Configã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                config = NeuroQuantumConfig(
                    vocab_size=config_dict['vocab_size'],
                    embed_dim=config_dict['embed_dim'],
                    hidden_dim=config_dict['hidden_dim'],
                    num_heads=config_dict['num_heads'],
                    num_layers=config_dict['num_layers'],
                    max_seq_len=config_dict['max_seq_len'],
                    dropout=config_dict.get('dropout', 0.1),
                    lambda_entangle=config_dict.get('lambda_entangle', 0.5),
                )

                # NeuroQuantumAI ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½œæˆ
                model = NeuroQuantumAI(
                    embed_dim=config_dict['embed_dim'],
                    hidden_dim=config_dict['hidden_dim'],
                    num_heads=config_dict['num_heads'],
                    num_layers=config_dict['num_layers'],
                    max_seq_len=config_dict['max_seq_len'],
                    dropout=config_dict.get('dropout', 0.1),
                    lambda_entangle=config_dict.get('lambda_entangle', 0.5),
                )
                model.model = nn_model
                model.config = config
                model.tokenizer = tokenizer

                print(f"âœ… äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† (via neuroq_pretrained.py)!")
                print(f"   vocab_size: {config_dict['vocab_size']}")
                print(f"   embed_dim: {config_dict['embed_dim']}")
                print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {nn_model.num_params:,}")

                is_initialized = True
                return True
            else:
                print("âš ï¸ neuroq_pretrained.py ã§ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã€‚å¾“æ¥ã®æ–¹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")

        except ImportError as e:
            print(f"âš ï¸ neuroq_pretrained module not found: {e}")
            print("   Falling back to traditional loading method...")
        except Exception as e:
            print(f"âš ï¸ Error using neuroq_pretrained module: {e}")
            print("   Falling back to traditional loading method...")

        # ========================================
        # æ–¹æ³•2: å¾“æ¥ã®æ–¹æ³•ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        # ========================================
        if os.path.exists(PRETRAINED_MODEL_PATH):
            print(f"ğŸ“¦ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (traditional method): {PRETRAINED_MODEL_PATH}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
            validation_result = validate_pt_file(PRETRAINED_MODEL_PATH)

            if not validation_result["valid"]:
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼ã«å¤±æ•—: {validation_result['error']}")
                print(f"   ç°¡æ˜“å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                # æ¤œè¨¼å¤±æ•—æ™‚ã¯ç°¡æ˜“å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã¸ï¼ˆä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ã‚¹ãƒ«ãƒ¼ï¼‰
            else:
                # æ¤œè¨¼æˆåŠŸ - ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
                print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼æˆåŠŸ")

                # å†åº¦èª­ã¿è¾¼ã¿ï¼ˆæ¤œè¨¼æ™‚ã¯CPUã§èª­ã¿è¾¼ã‚“ã ãŸã‚ï¼‰
                checkpoint = torch.load(PRETRAINED_MODEL_PATH, map_location=DEVICE)
                config_dict = checkpoint['config']

                # Configã‚’å¾©å…ƒ
                config = NeuroQuantumConfig(
                    vocab_size=config_dict['vocab_size'],
                    embed_dim=config_dict['embed_dim'],
                    hidden_dim=config_dict['hidden_dim'],
                    num_heads=config_dict['num_heads'],
                    num_layers=config_dict['num_layers'],
                    max_seq_len=config_dict['max_seq_len'],
                    dropout=config_dict['dropout'],
                    lambda_entangle=config_dict['lambda_entangle'],
                )

                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
                tokenizer = NeuroQuantumTokenizer(
                    vocab_size=config_dict['vocab_size'],
                    model_file="neuroq_tokenizer.model"
                )

                # vocab_size ã®æ•´åˆæ€§ç¢ºèª
                tokenizer_vocab_size = tokenizer.actual_vocab_size or tokenizer.vocab_size
                config_vocab_size = config_dict['vocab_size']
                print(f"ğŸ” Vocab size validation:")
                print(f"   Config vocab_size: {config_vocab_size}")
                print(f"   Tokenizer actual_vocab_size: {tokenizer_vocab_size}")

                if config_vocab_size != tokenizer_vocab_size:
                    print(f"âŒ CRITICAL: vocab_size mismatch detected!")
                    print(f"   Model was trained with vocab_size={config_vocab_size}")
                    print(f"   But tokenizer has vocab_size={tokenizer_vocab_size}")
                    print(f"   âš ï¸ WARNING: This may cause generation errors!")
                    print(f"   Recommendation: Retrain the model with correct vocab_size.")
                    # Note: We continue with a warning, as the model might still be usable

                # ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ã‚¦ã‚§ã‚¤ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
                nn_model = NeuroQuantum(config).to(DEVICE)
                nn_model.load_state_dict(checkpoint['model_state_dict'])
                nn_model.eval()

                # NeuroQuantumAI ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½œæˆ
                model = NeuroQuantumAI(
                    embed_dim=config_dict['embed_dim'],
                    hidden_dim=config_dict['hidden_dim'],
                    num_heads=config_dict['num_heads'],
                    num_layers=config_dict['num_layers'],
                    max_seq_len=config_dict['max_seq_len'],
                    dropout=config_dict['dropout'],
                    lambda_entangle=config_dict['lambda_entangle'],
                )
                model.model = nn_model
                model.config = config
                model.tokenizer = tokenizer

                print(f"âœ… äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†!")
                print(f"   vocab_size: {config_dict['vocab_size']}")
                print(f"   embed_dim: {config_dict['embed_dim']}")
                print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {nn_model.num_params:,}")

                is_initialized = True
                return True

        # ========================================
        # æ–¹æ³•2: ç°¡æ˜“å­¦ç¿’ï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆï¼‰
        # ========================================
        print("âš ï¸ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç°¡æ˜“å­¦ç¿’ã‚’å®Ÿè¡Œ...")
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå€‹åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åˆæœŸåŒ–ï¼‰
        model = NeuroQuantumAI(
            embed_dim=128,
            hidden_dim=256,
            num_heads=4,
            num_layers=3,
            max_seq_len=256,
            dropout=0.1,
            lambda_entangle=0.5
        )

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ç¢ºèª
        if os.path.exists("neuroq_tokenizer.model"):
            print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: neuroq_tokenizer.model")
        else:
            print("âš ï¸ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ç°¡æ˜“å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        print("ğŸ”„ ç°¡æ˜“å­¦ç¿’é–‹å§‹...")
        training_data = [
            "ã“ã‚“ã«ã¡ã¯ã€‚ç§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã§ã™ã€‚é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ãŸäººå·¥çŸ¥èƒ½ã§ã™ã€‚",
            "ãƒ‹ãƒ¥ãƒ¼ãƒ­Qã¯é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®åŸç†ã‚’æ´»ç”¨ã—ãŸæ¬¡ä¸–ä»£ã®AIã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸæ¬¡ä¸–ä»£ã®è¨ˆç®—æ©Ÿã§ã™ã€‚",
            "äººå·¥çŸ¥èƒ½ã¯äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
            "æ©Ÿæ¢°å­¦ç¿’ã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹AIã®æ‰‹æ³•ã§ã™ã€‚",
            "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å¤šå±¤åŒ–ã—ãŸæŠ€è¡“ã§ã™ã€‚",
        ] * 10
        
        model.train(training_data, epochs=5, seq_len=64)
        
        is_initialized = True
        print("âœ… ç°¡æ˜“å­¦ç¿’å®Œäº†!")
        return True
    
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


# ========================================
# ä¼šè©±å±¥æ­´ç®¡ç†
# ========================================
def build_conversation_prompt(session_id: str, user_message: str, max_history: int = 4) -> str:
    """
    ä¼šè©±å±¥æ­´ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰

    Args:
        session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        max_history: æœ€å¤§å±¥æ­´ã‚¿ãƒ¼ãƒ³æ•°ï¼ˆç›´è¿‘ã®ã‚¿ãƒ¼ãƒ³ã®ã¿ä½¿ç”¨ï¼‰

    Returns:
        ä¼šè©±ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    global conversation_sessions

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    if session_id not in conversation_sessions:
        conversation_sessions[session_id] = []

    # å±¥æ­´ã‚’å–å¾—ï¼ˆæœ€æ–°ã®max_historyã‚¿ãƒ¼ãƒ³ã®ã¿ï¼‰
    history = conversation_sessions[session_id][-max_history:]

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯çœç•¥ã—ã€ç›´æ¥å¯¾è©±å½¢å¼ã§ï¼‰
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: <USER>...<ASSISTANT>...
    conversation_text = ""

    # å±¥æ­´ã‚’è¿½åŠ 
    for turn in history:
        if turn["role"] == "user":
            conversation_text += f"<USER>{turn['content']}"
        elif turn["role"] == "assistant":
            conversation_text += f"<ASSISTANT>{turn['content']}"

    # ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    conversation_text += f"<USER>{user_message}<ASSISTANT>"

    return conversation_text


def save_conversation_turn(session_id: str, user_message: str, assistant_response: str):
    """
    ä¼šè©±ã‚¿ãƒ¼ãƒ³ã‚’å±¥æ­´ã«ä¿å­˜

    Args:
        session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        assistant_response: ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”
    """
    global conversation_sessions

    if session_id not in conversation_sessions:
        conversation_sessions[session_id] = []

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’ä¿å­˜
    conversation_sessions[session_id].append({
        "role": "user",
        "content": user_message
    })
    conversation_sessions[session_id].append({
        "role": "assistant",
        "content": assistant_response
    })

    # å¤ã„å±¥æ­´ã‚’å‰Šé™¤ï¼ˆæœ€å¤§10ã‚¿ãƒ¼ãƒ³ = 20ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
    if len(conversation_sessions[session_id]) > 20:
        conversation_sessions[session_id] = conversation_sessions[session_id][-20:]


# ========================================
# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
# ========================================
def generate_text(prompt: str, max_length: int = 50,
                  temp_min: float = None, temp_max: float = None,
                  temperature: float = None, session_id: str = "default") -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¼šè©±å¯¾å¿œç‰ˆï¼‰

    Args:
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        max_length: æœ€å¤§ç”Ÿæˆé•·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ50 - ä¼šè©±å‘ã‘ã«çŸ­ãåˆ¶é™ï¼‰
        temp_min: æœ€ä½æ¸©åº¦ï¼ˆæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯temp_min/temp_maxã‚’ä½¿ç”¨ï¼‰
        temp_max: æœ€é«˜æ¸©åº¦
        temperature: äº’æ›æ€§ã®ãŸã‚ã®å˜ä¸€æ¸©åº¦ï¼ˆæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯è‡ªå‹•çš„ã«temp_min/temp_maxã«å¤‰æ›ï¼‰
        session_id: ä¼šè©±ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆä¼šè©±å±¥æ­´ç®¡ç†ç”¨ï¼‰

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    global model

    if model is None:
        return "Error: Model not initialized"

    try:
        # temperatureãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã€temp_min/temp_maxã«å¤‰æ›
        if temperature is not None and temp_min is None:
            temp_min = temperature * 0.8
            temp_max = temperature * 1.2

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆä¼šè©±ç”Ÿæˆã«æœ€é©åŒ– - ã‚ˆã‚Šä¿å®ˆçš„ï¼‰
        if temp_min is None:
            temp_min = 0.4  # ä¼šè©±å‘ã‘ã«ã‚ˆã‚Šä¿å®ˆçš„ãªæ¸©åº¦
        if temp_max is None:
            temp_max = 0.7  # 0.8 â†’ 0.7 ã«ä¸‹ã’ã¦æš´èµ°ã‚’é˜²ã

        # ä¼šè©±å±¥æ­´ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        conversation_prompt = build_conversation_prompt(session_id, prompt)

        # ç”Ÿæˆå®Ÿè¡Œï¼ˆä¼šè©±å‘ã‘ã«çŸ­ãåˆ¶é™ï¼‰
        result = model.generate(
            prompt=conversation_prompt,
            max_length=max_length,
            temp_min=temp_min,
            temp_max=temp_max,
            repetition_penalty=2.5,  # 2.0 â†’ 2.5 ã«å¼·åŒ–ï¼ˆç¹°ã‚Šè¿”ã—ã‚’ã‚ˆã‚Šå¼·ãæŠ‘åˆ¶ï¼‰
            no_repeat_ngram_size=3,   # 3-gramã®ç¹°ã‚Šè¿”ã—é˜²æ­¢
            top_k=40,                  # top_k ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            top_p=0.9,                 # top_p ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
        )

        # ä¼šè©±å±¥æ­´ã«ä¿å­˜
        save_conversation_turn(session_id, prompt, result)

        return result
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"Error: {str(e)}"


# ========================================
# ãƒ¡ã‚¤ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆRunPodç”¨ï¼‰
# ========================================
def handler(job):
    """
    RunPod Serverless Handler
    
    é‡è¦: health checkã¯å³åº§ã«è¿”ã™ï¼
    """
    global is_initialized
    
    job_input = job.get("input", {})
    action = job_input.get("action", "generate")
    
    # ========================================
    # HEALTH CHECKï¼ˆæœ€å„ªå…ˆãƒ»å³åº§ã«è¿”ã™ï¼‰
    # ========================================
    if action == "health":
        return {
            "status": "healthy",
            "device": DEVICE,
            "cuda_available": torch.cuda.is_available(),
            "model_initialized": is_initialized
        }
    
    # ========================================
    # STATUS CHECK
    # ========================================
    if action == "status":
        return {
            "status": "ok",
            "initialized": is_initialized,
            "device": DEVICE,
            "vocab_size": VOCAB_SIZE
        }
    
    # ========================================
    # GENERATEï¼ˆãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ãªå‡¦ç†ï¼‰
    # ========================================
    if action == "generate":
        # Lazy initialization
        if not is_initialized:
            print("ğŸ”„ åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆ - ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
            if not initialize_model():
                return {
                    "status": "error",
                    "error": "Failed to initialize model"
                }

        prompt = job_input.get("prompt", "ã“ã‚“ã«ã¡ã¯")
        max_length = job_input.get("max_length", 50)  # 100 â†’ 50 ã«å¤‰æ›´ï¼ˆä¼šè©±å‘ã‘ï¼‰
        session_id = job_input.get("session_id", "default")  # ä¼šè©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ID

        # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtemp_min/temp_maxå„ªå…ˆã€äº’æ›æ€§ã®ãŸã‚temperatureã‚‚ã‚µãƒãƒ¼ãƒˆï¼‰
        temp_min = job_input.get("temp_min")
        temp_max = job_input.get("temp_max")
        temperature = job_input.get("temperature", 0.5)  # 0.6 â†’ 0.5 ã«ä¸‹ã’ã¦å®‰å®šæ€§å‘ä¸Š

        print(f"ğŸ“ Generate: session_id='{session_id}', prompt='{prompt[:30]}...'")

        result = generate_text(
            prompt=prompt,
            max_length=max_length,
            temp_min=temp_min,
            temp_max=temp_max,
            temperature=temperature,
            session_id=session_id
        )

        return {
            "status": "success",
            "prompt": prompt,
            "generated": result,
            "session_id": session_id
        }
    
    # ========================================
    # PRETRAIN_OPENAIï¼ˆOpenAIãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆäº‹å‰å­¦ç¿’ï¼‰
    # ========================================
    if action == "pretrain_openai":
        global pretrain_process, pretrain_status

        # æ—¢ã«å®Ÿè¡Œä¸­ã®å ´åˆ
        if pretrain_status == "running":
            return {
                "status": "error",
                "error": "Pretraining is already running",
                "pretrain_status": pretrain_status
            }

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ç¢ºèª
        log_path = Path(pretrain_log_file)

        try:
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§pretrain_openai.pyã‚’å®Ÿè¡Œ
            print("ğŸš€ Starting OpenAI pretraining...")
            pretrain_status = "running"

            # python -u ã§ unbuffered output
            cmd = [
                sys.executable, "-u",
                "pretrain_openai.py"
            ]

            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦subprocessã‚’èµ·å‹•
            # IMPORTANT: Don't use 'with' statement as the file needs to stay open
            # for the entire duration of the subprocess
            log_file = open(log_path, 'w', buffering=1)  # Line buffered
            pretrain_process = subprocess.Popen(
                cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                cwd=os.path.dirname(os.path.abspath(__file__))
            )

            # éåŒæœŸã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç›£è¦–
            def monitor_pretrain():
                global pretrain_status, pretrain_process
                pretrain_process.wait()

                # Close the log file after the process finishes
                try:
                    log_file.close()
                except:
                    pass

                if pretrain_process.returncode == 0:
                    pretrain_status = "completed"
                    print("âœ… Pretraining completed successfully")
                else:
                    pretrain_status = "error"
                    print(f"âŒ Pretraining failed with code {pretrain_process.returncode}")

            monitor_thread = threading.Thread(target=monitor_pretrain, daemon=True)
            monitor_thread.start()

            return {
                "status": "success",
                "message": "Pretraining started",
                "pretrain_status": pretrain_status,
                "log_file": str(log_path),
                "pid": pretrain_process.pid
            }

        except Exception as e:
            pretrain_status = "error"
            return {
                "status": "error",
                "error": str(e),
                "pretrain_status": pretrain_status
            }

    # ========================================
    # PRETRAIN_STATUSï¼ˆäº‹å‰å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªï¼‰
    # ========================================
    if action == "pretrain_status":
        log_path = Path(pretrain_log_file)

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¾Œã®æ•°è¡Œã‚’èª­ã‚€
        log_tail = ""
        if log_path.exists():
            try:
                with open(log_path, 'r') as f:
                    lines = f.readlines()
                    log_tail = ''.join(lines[-20:])  # æœ€å¾Œã®20è¡Œ
            except Exception as e:
                log_tail = f"Error reading log: {e}"

        return {
            "status": "success",
            "pretrain_status": pretrain_status,
            "log_file": str(log_path),
            "log_exists": log_path.exists(),
            "log_tail": log_tail,
            "process_running": pretrain_process is not None and pretrain_process.poll() is None
        }

    # ========================================
    # TRAINï¼ˆå­¦ç¿’ï¼‰
    # ========================================
    if action == "train":
        if not is_initialized:
            if not initialize_model():
                return {
                    "status": "error",
                    "error": "Failed to initialize model"
                }
        
        texts = job_input.get("texts", [])
        epochs = job_input.get("epochs", 5)
        
        if not texts:
            return {
                "status": "error",
                "error": "No training texts provided"
            }
        
        try:
            model.train(texts, epochs=epochs, seq_len=32)
            return {
                "status": "success",
                "message": f"Training completed ({epochs} epochs)"
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    # ========================================
    # CLEAR_SESSIONï¼ˆä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢ï¼‰
    # ========================================
    if action == "clear_session":
        global conversation_sessions
        session_id = job_input.get("session_id", "default")

        if session_id in conversation_sessions:
            del conversation_sessions[session_id]
            return {
                "status": "success",
                "message": f"Session '{session_id}' cleared"
            }
        else:
            return {
                "status": "success",
                "message": f"Session '{session_id}' not found (already empty)"
            }

    # ========================================
    # UNKNOWN ACTION
    # ========================================
    return {
        "status": "error",
        "error": f"Unknown action: {action}",
        "available_actions": ["health", "status", "generate", "train", "pretrain_openai", "pretrain_status", "clear_session"]
    }


# ========================================
# èµ·å‹•ï¼ˆä½•ã‚‚ã—ãªã„ = é«˜é€Ÿèµ·å‹•ï¼‰
# ========================================
print("=" * 60)
print("âœ… NeuroQ Handler Ready")
print("   - Health check: instant response")
print("   - Model loading: lazy (on first request)")
print("=" * 60)

# RunPodèµ·å‹•
runpod.serverless.start({"handler": handler})
