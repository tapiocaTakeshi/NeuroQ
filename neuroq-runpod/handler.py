#!/usr/bin/env python3
"""
NeuroQ RunPod Serverless Handler
=================================
RunPod Serverless APIç”¨ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«

å‚ç…§å…ƒ:
- neuroquantum_layered.py: å±¤çŠ¶QBNN-Transformer
- neuroquantum_brain.py: è„³å‹æ•£åœ¨QBNN

ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:
- /generate: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
- /health: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
"""

import os
import sys
import json
import torch
from typing import Dict, Any, Optional

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆneuroquantum_*.py ã‚’å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
# Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã¯åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã•ã‚Œã‚‹ã®ã§ã€è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‚ç…§ã¯ä¸è¦
# ãŸã ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã§ã®äº’æ›æ€§ã®ãŸã‚æ®‹ã™
PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# ã¾ãšç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ ï¼ˆDockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã¯ã“ã‚Œã§ååˆ†ï¼‰
# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æœ€åˆã«è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå„ªå…ˆã•ã‚Œã‚‹
if CURRENT_DIR not in sys.path:
    sys.path.insert(0, CURRENT_DIR)
# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚è¿½åŠ ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
# ãŸã ã—ã€ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å¾Œã«è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå„ªå…ˆã•ã‚Œã‚‹
if PARENT_DIR not in sys.path:
    sys.path.insert(1, PARENT_DIR)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹1ã«æŒ¿å…¥ã—ã¦ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å„ªå…ˆ

# neuroquantum_layered.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜ç¤ºçš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    import importlib.util
    layered_path = os.path.join(CURRENT_DIR, "neuroquantum_layered.py")
    if os.path.exists(layered_path):
        spec = importlib.util.spec_from_file_location("neuroquantum_layered_local", layered_path)
        layered_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(layered_module)
        NeuroQuantumAI = layered_module.NeuroQuantumAI
        NeuroQuantumTokenizer = layered_module.NeuroQuantumTokenizer
        NeuroQuantumConfig = layered_module.NeuroQuantumConfig
        NeuroQuantum = layered_module.NeuroQuantum
        NEUROQUANTUM_LAYERED_AVAILABLE = True
        print(f"âœ… ãƒ­ãƒ¼ã‚«ãƒ«ã® neuroquantum_layered.py ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ ({layered_path})")
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        from neuroquantum_layered import (
            NeuroQuantumAI,
            NeuroQuantumTokenizer,
            NeuroQuantumConfig,
            NeuroQuantum,
        )
        NEUROQUANTUM_LAYERED_AVAILABLE = True
        print("âœ… neuroquantum_layered.py ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError as e:
    NEUROQUANTUM_LAYERED_AVAILABLE = False
    print(f"âš ï¸ neuroquantum_layered.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
except Exception as e:
    NEUROQUANTUM_LAYERED_AVAILABLE = False
    print(f"âš ï¸ neuroquantum_layered.py ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# neuroquantum_brain.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from neuroquantum_brain import (
        NeuroQuantumBrainAI,
        BrainTokenizer,
        NeuroQuantumBrain,
    )
    NEUROQUANTUM_BRAIN_AVAILABLE = True
    print("âœ… neuroquantum_brain.py ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError as e:
    NEUROQUANTUM_BRAIN_AVAILABLE = False
    print(f"âš ï¸ neuroquantum_brain.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")

# RunPod SDK
try:
    import runpod
    RUNPOD_AVAILABLE = True
except ImportError:
    RUNPOD_AVAILABLE = False
    print("âš ï¸ runpodãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install runpod ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# OpenAI APIï¼ˆChatGPTã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”¨ï¼‰
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install openai ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ========================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
# ========================================

# ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ä¿æŒï¼‰
model_layered: Optional[NeuroQuantumAI] = None
model_brain: Optional[NeuroQuantumBrainAI] = None

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
if torch.backends.mps.is_available():
    DEVICE = torch.device("mps")
    print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
elif torch.cuda.is_available():
    DEVICE = torch.device("cuda")
    print("ğŸ® NVIDIA GPU (CUDA) ã‚’ä½¿ç”¨")
else:
    DEVICE = torch.device("cpu")
    print("ğŸ’» CPU ã‚’ä½¿ç”¨")


# ========================================
# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
# ========================================

def init_model(mode: str = "layered", **kwargs) -> Dict[str, Any]:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    
    Args:
        mode: 'layered' ã¾ãŸã¯ 'brain'
        **kwargs: ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
        åˆæœŸåŒ–çµæœ
    """
    global model_layered, model_brain
    
    try:
        if mode == "layered":
            if not NEUROQUANTUM_LAYERED_AVAILABLE:
                return {"error": "neuroquantum_layered.py ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            embed_dim = kwargs.get("embed_dim", 64)
            # num_neuronsãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯hidden_dimã¨ã—ã¦ä½¿ç”¨
            hidden_dim = kwargs.get("hidden_dim", kwargs.get("num_neurons", 128))
            num_heads = kwargs.get("num_heads", 4)
            num_layers = kwargs.get("num_layers", 2)
            max_seq_len = kwargs.get("max_seq_len", 128)
            dropout = kwargs.get("dropout", 0.1)
            lambda_entangle = kwargs.get("lambda_entangle", 0.35)
            use_openai_embedding = kwargs.get("use_openai_embedding", False)
            openai_api_key = kwargs.get("openai_api_key")
            openai_model = kwargs.get("openai_model", "text-embedding-3-large")
            
            model_layered = NeuroQuantumAI(
                embed_dim=embed_dim,
                hidden_dim=hidden_dim,
                num_heads=num_heads,
                num_layers=num_layers,
                max_seq_len=max_seq_len,
                dropout=dropout,
                lambda_entangle=lambda_entangle,
                use_openai_embedding=use_openai_embedding,
                openai_api_key=openai_api_key,
                openai_model=openai_model,
            )
            model_layered.device = DEVICE
            
            return {
                "status": "success",
                "mode": "layered",
                "message": "Layered mode ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ"
            }
        
        elif mode == "brain":
            if not NEUROQUANTUM_BRAIN_AVAILABLE:
                return {"error": "neuroquantum_brain.py ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            embed_dim = kwargs.get("embed_dim", 128)
            num_heads = kwargs.get("num_heads", 4)
            num_layers = kwargs.get("num_layers", 3)
            num_neurons = kwargs.get("num_neurons", 75)
            max_vocab = kwargs.get("max_vocab", 50000)
            
            model_brain = NeuroQuantumBrainAI(
                embed_dim=embed_dim,
                num_heads=num_heads,
                num_layers=num_layers,
                num_neurons=num_neurons,
                max_vocab=max_vocab,
            )
            model_brain.device = DEVICE
            
            return {
                "status": "success",
                "mode": "brain",
                "message": "Brain mode ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ"
            }
        
        else:
            return {"error": f"ä¸æ˜ãªãƒ¢ãƒ¼ãƒ‰: {mode}"}
    
    except Exception as e:
        return {"error": f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}"}


# ========================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—
# ========================================

def fetch_training_data(
    data_sources: Optional[list] = None,
    common_crawl_config: Optional[Dict[str, Any]] = None,
    max_records: int = 100
) -> list:
    """
    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Args:
        data_sources: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆ (ä¾‹: ["common_crawl", "huggingface"])
        common_crawl_config: Common Crawlè¨­å®šï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
        max_records: æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
    
    Returns:
        ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    texts = []
    
    if data_sources is None:
        data_sources = ["huggingface"]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    # Hugging Faceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    if "huggingface" in data_sources or "hugging_face" in data_sources:
        try:
            from datasets import load_dataset
            print("   ğŸ“¡ Hugging Faceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
            
            # æ—¥æœ¬èªWikipedia
            try:
                ds = load_dataset("range3/wiki40b-ja", split="train", streaming=True)
                count = 0
                for item in ds:
                    if 'text' in item and len(item['text']) > 50:
                        texts.append(item['text'][:1000])  # é•·ã•åˆ¶é™
                        count += 1
                        if count >= max_records // 3:
                            break
            except Exception as e:
                print(f"   âš ï¸ æ—¥æœ¬èªWikipediaå–å¾—å¤±æ•—: {e}")
            
            # è‹±èªãƒ‡ãƒ¼ã‚¿
            try:
                ds = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
                for item in ds[:max_records // 3]:
                    if 'text' in item and len(item['text']) > 30:
                        texts.append(item['text'])
            except Exception as e:
                print(f"   âš ï¸ WikiTextå–å¾—å¤±æ•—: {e}")
            
            # æ—¥æœ¬èªå¯¾è©±
            try:
                ds = load_dataset("kunishou/databricks-dolly-15k-ja", split="train")
                for item in ds[:max_records // 3]:
                    if 'output' in item:
                        texts.append(item['output'])
            except Exception as e:
                print(f"   âš ï¸ æ—¥æœ¬èªå¯¾è©±ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {e}")
            
            print(f"   âœ… Hugging Face: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«å–å¾—")
            
        except ImportError:
            print("   âš ï¸ datasetsæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        except Exception as e:
            print(f"   âš ï¸ Hugging Faceå–å¾—å¤±æ•—: {e}")
    
    # Common Crawlï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
    if "common_crawl" in data_sources:
        print("   âš ï¸ Common Crawlã¯ç¾åœ¨å®Ÿè£…ä¸­ã§ã™ã€‚Hugging Faceãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        # TODO: Common Crawl APIå®Ÿè£…
    
    # çµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if len(texts) == 0:
        print("   ğŸ“ çµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨...")
        texts = [
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦æƒ…å ±ã‚’å‡¦ç†ã™ã‚‹é©æ–°çš„ãªè¨ˆç®—æ©Ÿã§ã™ã€‚",
            "äººå·¥çŸ¥èƒ½ã¯ã€äººé–“ã®çŸ¥çš„æ´»å‹•ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å®Ÿç¾ã—ã‚ˆã†ã¨ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
            "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã¦æ”¹å–„ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã—ã¾ã™ã€‚",
            "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã§ã™ã€‚",
            "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€äººé–“ã®è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
        ] * (max_records // 5)
    
    return texts[:max_records]


# ========================================
# å­¦ç¿’å‡¦ç†
# ========================================

def train_model(
    mode: str = "layered",
    texts: Optional[list] = None,
    epochs: int = 20,
    batch_size: int = 16,
    learning_rate: float = 0.001,
    seq_length: int = 64,
    **kwargs
) -> Dict[str, Any]:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    
    Args:
        mode: 'layered' ã¾ãŸã¯ 'brain'
        texts: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼‰
        epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        learning_rate: å­¦ç¿’ç‡
        seq_length: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
        å­¦ç¿’çµæœ
    """
    global model_layered, model_brain
    
    try:
        if texts is None or len(texts) == 0:
            return {"error": "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™"}
        
        if mode == "layered":
            if model_layered is None:
                init_result = init_model(mode="layered", **kwargs)
                if "error" in init_result:
                    return init_result
            
            print(f"   ğŸ“ Layered mode å­¦ç¿’é–‹å§‹: {len(texts)}ã‚µãƒ³ãƒ—ãƒ«, {epochs}ã‚¨ãƒãƒƒã‚¯")
            model_layered.train(
                texts=texts,
                epochs=epochs,
                batch_size=batch_size,
                lr=learning_rate,
                seq_len=seq_length
            )
            
            return {
                "status": "success",
                "mode": "layered",
                "message": f"å­¦ç¿’å®Œäº†: {epochs}ã‚¨ãƒãƒƒã‚¯"
            }
        
        elif mode == "brain":
            if model_brain is None:
                init_result = init_model(mode="brain", **kwargs)
                if "error" in init_result:
                    return init_result
            
            print(f"   ğŸ“ Brain mode å­¦ç¿’é–‹å§‹: {len(texts)}ã‚µãƒ³ãƒ—ãƒ«, {epochs}ã‚¨ãƒãƒƒã‚¯")
            model_brain.train(
                texts=texts,
                epochs=epochs,
                batch_size=batch_size,
                lr=learning_rate,
                seq_length=seq_length
            )
            
            return {
                "status": "success",
                "mode": "brain",
                "message": f"å­¦ç¿’å®Œäº†: {epochs}ã‚¨ãƒãƒƒã‚¯"
            }
        
        else:
            return {"error": f"ä¸æ˜ãªãƒ¢ãƒ¼ãƒ‰: {mode}"}
    
    except Exception as e:
        return {"error": f"å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {str(e)}"}


# ========================================
# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
# ========================================

def generate_text(
    prompt: str,
    mode: str = "layered",
    max_length: int = 100,
    temperature: float = 0.7,
    top_k: int = 40,
    top_p: float = 0.9,
    **kwargs
) -> Dict[str, Any]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    
    Args:
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        mode: 'layered' ã¾ãŸã¯ 'brain'
        max_length: æœ€å¤§ç”Ÿæˆé•·
        temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        top_k: Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        top_p: Top-P ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
        ç”Ÿæˆçµæœ
    """
    global model_layered, model_brain
    
    try:
        if mode == "layered":
            if model_layered is None:
                # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
                init_result = init_model(mode="layered", **kwargs)
                if "error" in init_result:
                    return init_result
            
            if model_layered.model is None:
                return {"error": "ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            generated = model_layered.generate(
                prompt=prompt,
                max_length=max_length,
                temp_min=temperature * 0.8,
                temp_max=temperature * 1.2,
                top_k=top_k,
                top_p=top_p,
            )
            
            return {
                "status": "success",
                "mode": "layered",
                "prompt": prompt,
                "generated": generated,
            }
        
        elif mode == "brain":
            if model_brain is None:
                # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
                init_result = init_model(mode="brain", **kwargs)
                if "error" in init_result:
                    return init_result
            
            if model_brain.model is None:
                return {"error": "ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            generated = model_brain.generate(
                prompt=prompt,
                max_length=max_length,
                temperature_min=temperature * 0.8,
                temperature_max=temperature * 1.2,
                top_k=top_k,
                top_p=top_p,
            )
            
            return {
                "status": "success",
                "mode": "brain",
                "prompt": prompt,
                "generated": generated,
            }
        
        else:
            return {"error": f"ä¸æ˜ãªãƒ¢ãƒ¼ãƒ‰: {mode}"}
    
    except Exception as e:
        return {"error": f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"}


# ========================================
# RunPod Handler
# ========================================

def handler(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    RunPod Serverless Handler
    
    ãƒªã‚¯ã‚¨ã‚¹ãƒˆå½¢å¼:
    {
        "input": {
            "action": "generate" | "init" | "health",
            "mode": "layered" | "brain",
            "prompt": "ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "max_length": 100,
            "temperature": 0.7,
            ...
        }
    }
    """
    try:
        input_data = event.get("input", {})
        action = input_data.get("action", "generate")
        
        if action == "health":
            return {
                "status": "healthy",
                "layered_available": NEUROQUANTUM_LAYERED_AVAILABLE,
                "brain_available": NEUROQUANTUM_BRAIN_AVAILABLE,
                "openai_available": OPENAI_AVAILABLE,
                "device": str(DEVICE),
            }
        
        elif action == "init":
            mode = input_data.get("mode", "layered")
            kwargs = {k: v for k, v in input_data.items() if k != "action" and k != "mode"}
            return init_model(mode=mode, **kwargs)
        
        elif action == "generate":
            prompt = input_data.get("prompt", "")
            if not prompt:
                return {"error": "promptãŒå¿…è¦ã§ã™"}
            
            mode = input_data.get("mode", "layered")
            max_length = input_data.get("max_length", input_data.get("max_tokens", 100))
            temperature = input_data.get("temperature", 0.7)
            top_k = input_data.get("top_k", 40)
            top_p = input_data.get("top_p", 0.9)
            
            # train_before_generate ãƒ•ãƒ©ã‚°ã®å‡¦ç†
            train_before_generate = input_data.get("train_before_generate", False)
            
            if train_before_generate:
                # ãƒ‡ãƒ¼ã‚¿å–å¾—
                data_sources = input_data.get("data_sources", ["huggingface"])
                common_crawl_config = input_data.get("common_crawl_config", {})
                max_records = common_crawl_config.get("max_records", 100)
                
                print(f"ğŸ“¥ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­... (ã‚½ãƒ¼ã‚¹: {data_sources}, æœ€å¤§{max_records}ãƒ¬ã‚³ãƒ¼ãƒ‰)")
                texts = fetch_training_data(
                    data_sources=data_sources,
                    common_crawl_config=common_crawl_config,
                    max_records=max_records
                )
                
                if len(texts) == 0:
                    return {"error": "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"}
                
                # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                epochs = input_data.get("epochs", 20)
                batch_size = input_data.get("batch_size", 16)
                learning_rate = input_data.get("learning_rate", 0.001)
                seq_length = input_data.get("seq_length", 64)
                
                # ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                # num_neuronsã¯brainãƒ¢ãƒ¼ãƒ‰ã§ã¯num_neuronsã€layeredãƒ¢ãƒ¼ãƒ‰ã§ã¯hidden_dimã¨ã—ã¦ä½¿ç”¨
                model_kwargs = {
                    k: v for k, v in input_data.items()
                    if k in [
                        "embed_dim", "hidden_dim", "num_heads", "num_layers",
                        "num_neurons", "max_vocab", "max_seq_len", "dropout",
                        "lambda_entangle", "use_openai_embedding", "openai_api_key",
                        "openai_model"
                    ]
                }
                
                # num_neuronsãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦å¤‰æ›
                if "num_neurons" in input_data and "num_neurons" not in model_kwargs:
                    num_neurons = input_data["num_neurons"]
                    if mode == "layered":
                        # layeredãƒ¢ãƒ¼ãƒ‰ã§ã¯hidden_dimã¨ã—ã¦ä½¿ç”¨
                        model_kwargs["hidden_dim"] = num_neurons
                    elif mode == "brain":
                        # brainãƒ¢ãƒ¼ãƒ‰ã§ã¯num_neuronsã¨ã—ã¦ä½¿ç”¨
                        model_kwargs["num_neurons"] = num_neurons
                
                # å­¦ç¿’å®Ÿè¡Œ
                train_result = train_model(
                    mode=mode,
                    texts=texts,
                    epochs=epochs,
                    batch_size=batch_size,
                    learning_rate=learning_rate,
                    seq_length=seq_length,
                    **model_kwargs
                )
                
                if "error" in train_result:
                    return train_result
                
                print(f"âœ… å­¦ç¿’å®Œäº†: {train_result.get('message', '')}")
            
            # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            kwargs = {
                k: v for k, v in input_data.items()
                if k not in [
                    "action", "prompt", "mode", "max_length", "max_tokens",
                    "temperature", "top_k", "top_p", "train_before_generate",
                    "data_sources", "common_crawl_config", "epochs", "batch_size",
                    "learning_rate", "seq_length"
                ]
            }
            
            return generate_text(
                prompt=prompt,
                mode=mode,
                max_length=max_length,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                **kwargs
            )
        
        else:
            return {"error": f"ä¸æ˜ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {action}"}
    
    except Exception as e:
        return {"error": f"ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}"}


# ========================================
# RunPod Serverless èµ·å‹•
# ========================================

if __name__ == "__main__":
    if RUNPOD_AVAILABLE:
        print("ğŸš€ RunPod Serverless Handler ã‚’èµ·å‹•ã—ã¾ã™...")
        runpod.serverless.start({"handler": handler})
    else:
        print("âš ï¸ RunPod SDKãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
        print("\nãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹:")
        print(json.dumps({
            "input": {
                "action": "health"
            }
        }, indent=2))

