#!/usr/bin/env python3
"""
NeuroQ RunPod Serverless Handler
================================
RunPod Serverless Endpointç”¨ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼

ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰:
- Brain Mode: è„³å‹æ•£åœ¨QBNN
- Layered Mode: å±¤çŠ¶QBNN-Transformer

ä½¿ç”¨æ–¹æ³•:
1. ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ neuroq_model.py ã‚’RunPodã«ãƒ‡ãƒ—ãƒ­ã‚¤
2. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (neuroq_model.pt, neuroq_tokenizer.json) ã‚’é…ç½®
3. RunPod Endpoint ã‹ã‚‰å‘¼ã³å‡ºã—
"""

import runpod
import torch
import os
import traceback

from neuroq_model import (
    NeuroQGenerator, 
    NeuroQModel, 
    NeuroQTokenizer, 
    NeuroQConfig,
    create_neuroq_brain,
    create_neuroq_layered,
)

# ========================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
# ========================================

MODEL_PATH = os.environ.get("NEUROQ_MODEL_PATH", "neuroq_model.pt")
TOKENIZER_PATH = os.environ.get("NEUROQ_TOKENIZER_PATH", "neuroq_tokenizer.json")
DEFAULT_MODE = os.environ.get("NEUROQ_MODE", "layered")  # 'brain' or 'layered'

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
if torch.cuda.is_available():
    DEVICE = "cuda"
    print(f"ğŸ® CUDA GPU ã‚’ä½¿ç”¨: {torch.cuda.get_device_name(0)}")
elif torch.backends.mps.is_available():
    DEVICE = "mps"
    print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
else:
    DEVICE = "cpu"
    print("ğŸ’» CPU ã‚’ä½¿ç”¨")

# ========================================
# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
# ========================================

generator = None

def load_model(mode: str = None):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆé…å»¶ãƒ­ãƒ¼ãƒ‰ï¼‰
    
    Args:
        mode: 'brain' or 'layered' (Noneã®å ´åˆã¯DEFAULT_MODEã‚’ä½¿ç”¨)
    """
    global generator
    
    if generator is not None:
        return generator
    
    if mode is None:
        mode = DEFAULT_MODE
    
    print(f"ğŸ“¥ NeuroQ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    print(f"   Mode: {mode}")
    print(f"   Model: {MODEL_PATH}")
    print(f"   Tokenizer: {TOKENIZER_PATH}")
    print(f"   Device: {DEVICE}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if not os.path.exists(MODEL_PATH):
        print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MODEL_PATH}")
        print("   ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ã—ã¾ã™ï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãªã—ï¼‰")
        
        # ãƒ‡ãƒ¢ç”¨ã®å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        if mode == 'brain':
            config = NeuroQConfig(
                mode='brain',
                vocab_size=2000,
                embed_dim=64,
                num_neurons=32,
                num_heads=4,
                num_layers=2,
                max_seq_len=128,
                connection_density=0.25,
            )
        else:  # layered
            config = NeuroQConfig(
                mode='layered',
                vocab_size=2000,
                embed_dim=64,
                hidden_dim=128,
                num_heads=4,
                num_layers=2,
                max_seq_len=128,
            )
        
        model = NeuroQModel(config)
        tokenizer = NeuroQTokenizer(vocab_size=2000)
        
        # åŸºæœ¬çš„ãªèªå½™ã‚’æ§‹ç¯‰
        basic_texts = [
            "ã“ã‚“ã«ã¡ã¯ã€ç§ã¯NeuroQã§ã™ã€‚é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIã§ã™ã€‚",
            "Hello, I am NeuroQ. A generative AI based on Quantum-Bit Neural Network.",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯æ¬¡ä¸–ä»£ã®è¨ˆç®—æŠ€è¡“ã§ã™ã€‚",
            "äººå·¥çŸ¥èƒ½ã¯ç§ãŸã¡ã®ç”Ÿæ´»ã‚’å¤‰é©ã—ã¦ã„ã¾ã™ã€‚",
            "QBNNã¯é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚",
        ]
        tokenizer.build_vocab(basic_texts)
        
        generator = NeuroQGenerator(model, tokenizer, DEVICE)
        print(f"âœ… ãƒ‡ãƒ¢ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº† (Mode: {mode})")
    else:
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        generator = NeuroQGenerator.load(MODEL_PATH, TOKENIZER_PATH, DEVICE)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    info = generator.get_model_info()
    print(f"   ãƒ¢ãƒ¼ãƒ‰: {info.get('mode', 'unknown')}")
    print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info['num_params']:,}")
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {info['embed_dim']}")
    print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {info['hidden_dim']}")
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {info.get('num_neurons', 'N/A')}")
    print(f"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {info['num_layers']}")
    
    return generator


# ========================================
# RunPod Handler
# ========================================

def handler(job):
    """
    RunPod Serverless ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    å…¥åŠ›JSONå½¢å¼:
    {
        "input": {
            "prompt": "ç”Ÿæˆã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "max_tokens": 128,        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰
            "temperature": 0.7,       # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰
            "top_k": 40,              # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 40ï¼‰
            "top_p": 0.9,             # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.9ï¼‰
            "repetition_penalty": 1.2 # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.2ï¼‰
        }
    }
    
    å‡ºåŠ›JSONå½¢å¼:
    {
        "prompt": "å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        "output": "ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ",
        "model_info": {
            "mode": "brain" or "layered",
            ...
        }
    }
    """
    try:
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿ï¼‰
        gen = load_model()
        
        # å…¥åŠ›ã‚’å–å¾—
        job_input = job.get("input", {})
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        prompt = job_input.get("prompt", "")
        max_tokens = int(job_input.get("max_tokens", 128))
        temperature = float(job_input.get("temperature", 0.7))
        top_k = int(job_input.get("top_k", 40))
        top_p = float(job_input.get("top_p", 0.9))
        repetition_penalty = float(job_input.get("repetition_penalty", 1.2))
        
        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not prompt:
            return {"error": "prompt is required"}
        
        if max_tokens < 1 or max_tokens > 1024:
            max_tokens = min(max(1, max_tokens), 1024)
        
        if temperature < 0.1 or temperature > 2.0:
            temperature = min(max(0.1, temperature), 2.0)
        
        print(f"ğŸ“ ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ:")
        print(f"   Prompt: {prompt[:50]}...")
        print(f"   Max tokens: {max_tokens}")
        print(f"   Temperature: {temperature}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        output_text = gen.generate(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
        )
        
        print(f"âœ… ç”Ÿæˆå®Œäº†: {len(output_text)} æ–‡å­—")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        return {
            "prompt": prompt,
            "output": output_text,
            "model_info": gen.get_model_info(),
        }
        
    except Exception as e:
        error_msg = f"Error: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}


# ========================================
# é‡å­æƒ…å ±ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def quantum_info(job):
    """é‡å­ã‚‚ã¤ã‚Œæƒ…å ±ã‚’å–å¾—"""
    try:
        gen = load_model()
        model_info = gen.get_model_info()
        quantum_info = gen.model.get_quantum_info()
        
        return {
            "status": "success",
            "mode": model_info.get('mode', 'unknown'),
            "quantum_info": quantum_info,
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
        }


# ========================================
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

def health_check(job):
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        gen = load_model()
        return {
            "status": "healthy",
            "model_loaded": gen is not None,
            "device": DEVICE,
            "model_info": gen.get_model_info() if gen else None,
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
        }


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ âš›ï¸ NeuroQ RunPod Serverless Worker")
    print("   Brain Mode: è„³å‹æ•£åœ¨QBNN")
    print("   Layered Mode: å±¤çŠ¶QBNN-Transformer")
    print("=" * 60)
    
    # èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
    load_model()
    
    # RunPod Serverless ã‚’é–‹å§‹
    runpod.serverless.start({
        "handler": handler,
    })
