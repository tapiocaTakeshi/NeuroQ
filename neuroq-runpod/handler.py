#!/usr/bin/env python3
"""
NeuroQ RunPod Serverless Handler - Optimized Version
=====================================================
é«˜é€Ÿèµ·å‹• & å®‰å®šå‹•ä½œã®ãŸã‚ã®æœ€é©åŒ–æ¸ˆã¿ãƒãƒ³ãƒ‰ãƒ©ãƒ¼

ç‰¹å¾´:
- èµ·å‹•æ™‚ã«é‡ã„å‡¦ç†ã‚’ã—ãªã„ï¼ˆé«˜é€Ÿèµ·å‹•ï¼‰
- health checkã¯å³åº§ã«200ã‚’è¿”ã™
- ãƒ¢ãƒ‡ãƒ«ã¯åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã«lazy load
- vocab_sizeã®æ•´åˆæ€§ã‚’ä¿è¨¼
"""

import runpod
import torch
import os
import sys
import subprocess
import threading
import time
from pathlib import Path

# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆneuroq_pretrained.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ï¼‰
current_dir = str(Path(__file__).parent)
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

print("=" * 60)
print("âš›ï¸ NeuroQ RunPod Serverless - Starting...")
print("=" * 60)

# NeuroQuantumBrainAI ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from neuroquantum_brain import NeuroQuantumBrainAI, get_training_data
    from neuroquantum_layered import NeuroQuantumTokenizer
    print("âœ… neuroquantum_brain.py ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
    print("âœ… neuroquantum_layered.py ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError as e:
    print(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    NeuroQuantumBrainAI = None
    NeuroQuantumTokenizer = None

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
TOKENIZER_MODEL_PATH = "neuroq_tokenizer.model"

# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹
MODEL_CHECKPOINT_PATH = "checkpoints/neuroq_checkpoint.pt"

# ========================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆèµ·å‹•æ™‚ã¯å…¨ã¦Noneï¼‰
# ========================================
model = None  # NeuroQuantumBrainAI ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
is_initialized = False

# å­¦ç¿’çŠ¶æ…‹ç®¡ç†
pretrain_process = None
pretrain_status = "idle"  # idle, running, completed, error
pretrain_log_file = "training_openai.log"

# ä¼šè©±å±¥æ­´ç®¡ç†
conversation_sessions = {}  # session_id -> list of {role, content}

# è¨­å®š
VOCAB_SIZE = 8000
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä¼šè©±æŒ‡ç¤ºï¼‰
SYSTEM_PROMPT = """ã‚ãªãŸã¯è¦ªåˆ‡ã§æ­£ç¢ºãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«çŸ­ãæ­£ç¢ºã«ç­”ãˆã‚‹
2. ã‚ã‹ã‚‰ãªã„ã“ã¨ã¯è³ªå•ã™ã‚‹
3. èã‹ã‚ŒãŸã“ã¨ã ã‘ã«ç­”ãˆã‚‹ï¼ˆä½™è¨ˆãªæƒ…å ±ã‚’è¿½åŠ ã—ãªã„ï¼‰
4. å‰ã®æ–‡è„ˆã‚’è¸ã¾ãˆã¦è¿”ç­”ã™ã‚‹"""

print(f"ğŸ“Š Device: {DEVICE}")
print(f"ğŸ“Š CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}")




# ========================================
# Checkpointç®¡ç†ï¼ˆä¿å­˜ãƒ»ãƒ­ãƒ¼ãƒ‰ï¼‰
# ========================================
def save_checkpoint(model_instance, checkpoint_path: str = MODEL_CHECKPOINT_PATH):
    """
    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦ä¿å­˜

    Args:
        model_instance: NeuroQuantumBrainAI ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        checkpoint_path: ä¿å­˜å…ˆãƒ‘ã‚¹
    """
    try:
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        Path(checkpoint_path).parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
        checkpoint = {
            'model_state_dict': model_instance.qbnn.state_dict(),
            'config': {
                'embed_dim': model_instance.embed_dim,
                'num_heads': model_instance.num_heads,
                'num_layers': model_instance.num_layers,
                'num_neurons': model_instance.num_neurons,
                'max_vocab': model_instance.max_vocab,
            },
            'tokenizer_path': TOKENIZER_MODEL_PATH,
        }

        torch.save(checkpoint, checkpoint_path)
        print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: {checkpoint_path}")
        return True

    except Exception as e:
        print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def load_checkpoint(checkpoint_path: str = MODEL_CHECKPOINT_PATH):
    """
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰

    Args:
        checkpoint_path: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹

    Returns:
        NeuroQuantumBrainAI ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€ã¾ãŸã¯None
    """
    try:
        if not os.path.exists(checkpoint_path):
            print(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint_path}")
            return None

        print(f"ğŸ“¦ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)

        # è¨­å®šã‚’å¾©å…ƒ
        config = checkpoint['config']

        # ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
        model_instance = NeuroQuantumBrainAI(
            embed_dim=config['embed_dim'],
            num_heads=config['num_heads'],
            num_layers=config['num_layers'],
            num_neurons=config['num_neurons'],
            max_vocab=config['max_vocab'],
            use_sentencepiece=True
        )

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
        if os.path.exists(TOKENIZER_MODEL_PATH):
            print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰: {TOKENIZER_MODEL_PATH}")
            model_instance.tokenizer = NeuroQuantumTokenizer(
                vocab_size=config['max_vocab'],
                model_file=TOKENIZER_MODEL_PATH
            )

        # é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
        model_instance.qbnn.load_state_dict(checkpoint['model_state_dict'])

        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
        model_instance.qbnn.eval()

        print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
        return model_instance

    except Exception as e:
        print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


# ========================================
# Lazy Model Loadingï¼ˆåˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®ã¿ï¼‰
# ========================================
def initialize_model():
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆåˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®ã¿å‘¼ã°ã‚Œã‚‹ï¼‰

    æ¨è«–å°‚ç”¨ï¼šãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ã€å­¦ç¿’ã¯ä¸€åˆ‡ã—ãªã„
    """
    global model, is_initialized

    if is_initialized:
        return True

    print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–é–‹å§‹ï¼ˆæ¨è«–å°‚ç”¨ï¼‰...")

    try:
        if NeuroQuantumBrainAI is None:
            raise ImportError("NeuroQuantumBrainAI ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¦ã„ã¾ã›ã‚“")

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
        model = load_checkpoint(MODEL_CHECKPOINT_PATH)

        if model is None:
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒãªã„å ´åˆã¯ã€æœªå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆè­¦å‘Šã‚’å‡ºã™ï¼‰
            print("âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœªå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
            print("âš ï¸ æ¨è«–å‰ã« action='train' ã§å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

            model = NeuroQuantumBrainAI(
                embed_dim=128,
                num_heads=4,
                num_layers=3,
                num_neurons=100,
                max_vocab=8000,
                use_sentencepiece=True
            )

            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ˜ç¤ºçš„ã«ãƒ­ãƒ¼ãƒ‰
            if os.path.exists(TOKENIZER_MODEL_PATH):
                print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰: {TOKENIZER_MODEL_PATH}")
                model.tokenizer = NeuroQuantumTokenizer(
                    vocab_size=8000,
                    model_file=TOKENIZER_MODEL_PATH
                )

            # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            model.qbnn.eval()

        is_initialized = True
        print("âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†ï¼ˆæ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼‰!")
        return True

    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


# ========================================
# ä¼šè©±å±¥æ­´ç®¡ç†
# ========================================
def save_conversation_turn(session_id: str, user_message: str, assistant_response: str):
    """
    ä¼šè©±ã‚¿ãƒ¼ãƒ³ã‚’å±¥æ­´ã«ä¿å­˜

    Args:
        session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        assistant_response: ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”
    """
    global conversation_sessions

    if session_id not in conversation_sessions:
        conversation_sessions[session_id] = []

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’ä¿å­˜
    conversation_sessions[session_id].append({
        "role": "user",
        "content": user_message
    })
    conversation_sessions[session_id].append({
        "role": "assistant",
        "content": assistant_response
    })

    # å¤ã„å±¥æ­´ã‚’å‰Šé™¤ï¼ˆæœ€å¤§10ã‚¿ãƒ¼ãƒ³ = 20ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
    if len(conversation_sessions[session_id]) > 20:
        conversation_sessions[session_id] = conversation_sessions[session_id][-20:]


# ========================================
# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
# ========================================
def generate_text(prompt: str, max_length: int = 50,
                  temp_min: float = None, temp_max: float = None,
                  temperature: float = None, session_id: str = "default") -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¼šè©±å¯¾å¿œç‰ˆ - NeuroQuantumBrainAIä½¿ç”¨ï¼‰

    æ¨è«–å°‚ç”¨ï¼šmodel.eval() + torch.no_grad() ã§å­¦ç¿’ã‚’ä¸€åˆ‡ã—ãªã„

    Args:
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        max_length: æœ€å¤§ç”Ÿæˆé•·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ50 - ä¼šè©±å‘ã‘ã«çŸ­ãåˆ¶é™ï¼‰
        temp_min: æœ€ä½æ¸©åº¦ï¼ˆæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯temp_min/temp_maxã‚’ä½¿ç”¨ï¼‰
        temp_max: æœ€é«˜æ¸©åº¦
        temperature: äº’æ›æ€§ã®ãŸã‚ã®å˜ä¸€æ¸©åº¦ï¼ˆæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯è‡ªå‹•çš„ã«temp_min/temp_maxã«å¤‰æ›ï¼‰
        session_id: ä¼šè©±ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆä¼šè©±å±¥æ­´ç®¡ç†ç”¨ï¼‰

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    global model

    if model is None:
        return "Error: Model not initialized"

    try:
        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šï¼ˆé‡è¦ï¼šå­¦ç¿’ã‚’é˜²ãï¼‰
        model.qbnn.eval()

        # temperatureãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã€temp_min/temp_maxã«å¤‰æ›
        if temperature is not None and temp_min is None:
            temp_min = temperature * 0.8
            temp_max = temperature * 1.2

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆä¼šè©±ç”Ÿæˆã«æœ€é©åŒ– - ã‚ˆã‚Šä¿å®ˆçš„ï¼‰
        if temp_min is None:
            temp_min = 0.4  # ä¼šè©±å‘ã‘ã«ã‚ˆã‚Šä¿å®ˆçš„ãªæ¸©åº¦
        if temp_max is None:
            temp_max = 0.7  # 0.8 â†’ 0.7 ã«ä¸‹ã’ã¦æš´èµ°ã‚’é˜²ã

        # ä¼šè©±å±¥æ­´ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        # NeuroQuantumBrainAI.generate() ã¯å†…éƒ¨ã§ <USER>...<ASSISTANT> ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å‡¦ç†ã™ã‚‹
        # ãŸã ã—ã€å±¥æ­´ã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã«ã“ã“ã§ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
        history = conversation_sessions.get(session_id, [])[-4:]  # æœ€æ–°4ã‚¿ãƒ¼ãƒ³

        history_context = ""
        for turn in history:
            if turn["role"] == "user":
                history_context += f"<USER>{turn['content']}"
            elif turn["role"] == "assistant":
                history_context += f"<ASSISTANT>{turn['content']}"

        # å±¥æ­´ã‚’å«ã‚€å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        full_prompt = history_context + prompt if history_context else prompt

        # æ¨è«–å®Ÿè¡Œï¼ˆtorch.no_grad()ã§å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–ï¼‰
        with torch.no_grad():
            result = model.generate(
                prompt=full_prompt,
                max_length=max_length,
                temperature_min=temp_min,
                temperature_max=temp_max,
                top_k=40,
                top_p=0.9,
            )

        # ä¼šè©±å±¥æ­´ã«ä¿å­˜
        save_conversation_turn(session_id, prompt, result)

        return result
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"Error: {str(e)}"


# ========================================
# ãƒ¡ã‚¤ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆRunPodç”¨ï¼‰
# ========================================
def handler(job):
    """
    RunPod Serverless Handler
    
    é‡è¦: health checkã¯å³åº§ã«è¿”ã™ï¼
    """
    global is_initialized
    
    job_input = job.get("input", {})
    action = job_input.get("action", "generate")
    
    # ========================================
    # HEALTH CHECKï¼ˆæœ€å„ªå…ˆãƒ»å³åº§ã«è¿”ã™ï¼‰
    # ========================================
    if action == "health":
        return {
            "status": "healthy",
            "device": DEVICE,
            "cuda_available": torch.cuda.is_available(),
            "model_initialized": is_initialized
        }
    
    # ========================================
    # STATUS CHECK
    # ========================================
    if action == "status":
        return {
            "status": "ok",
            "initialized": is_initialized,
            "device": DEVICE,
            "vocab_size": VOCAB_SIZE
        }
    
    # ========================================
    # GENERATEï¼ˆãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ãªå‡¦ç†ï¼‰
    # ========================================
    if action == "generate":
        # Lazy initialization
        if not is_initialized:
            print("ğŸ”„ åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆ - ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
            if not initialize_model():
                return {
                    "status": "error",
                    "error": "Failed to initialize model"
                }

        prompt = job_input.get("prompt", "ã“ã‚“ã«ã¡ã¯")
        max_length = job_input.get("max_length", 50)  # 100 â†’ 50 ã«å¤‰æ›´ï¼ˆä¼šè©±å‘ã‘ï¼‰
        session_id = job_input.get("session_id", "default")  # ä¼šè©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ID

        # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtemp_min/temp_maxå„ªå…ˆã€äº’æ›æ€§ã®ãŸã‚temperatureã‚‚ã‚µãƒãƒ¼ãƒˆï¼‰
        temp_min = job_input.get("temp_min")
        temp_max = job_input.get("temp_max")
        temperature = job_input.get("temperature", 0.5)  # 0.6 â†’ 0.5 ã«ä¸‹ã’ã¦å®‰å®šæ€§å‘ä¸Š

        print(f"ğŸ“ Generate: session_id='{session_id}', prompt='{prompt[:30]}...'")

        result = generate_text(
            prompt=prompt,
            max_length=max_length,
            temp_min=temp_min,
            temp_max=temp_max,
            temperature=temperature,
            session_id=session_id
        )

        return {
            "status": "success",
            "prompt": prompt,
            "generated": result,
            "session_id": session_id
        }
    
    # ========================================
    # PRETRAIN_OPENAIï¼ˆOpenAIãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆäº‹å‰å­¦ç¿’ï¼‰
    # ========================================
    if action == "pretrain_openai":
        global pretrain_process, pretrain_status

        # æ—¢ã«å®Ÿè¡Œä¸­ã®å ´åˆ
        if pretrain_status == "running":
            return {
                "status": "error",
                "error": "Pretraining is already running",
                "pretrain_status": pretrain_status
            }

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ç¢ºèª
        log_path = Path(pretrain_log_file)

        try:
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§pretrain_openai.pyã‚’å®Ÿè¡Œ
            print("ğŸš€ Starting OpenAI pretraining...")
            pretrain_status = "running"

            # python -u ã§ unbuffered output
            cmd = [
                sys.executable, "-u",
                "pretrain_openai.py"
            ]

            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦subprocessã‚’èµ·å‹•
            # IMPORTANT: Don't use 'with' statement as the file needs to stay open
            # for the entire duration of the subprocess
            log_file = open(log_path, 'w', buffering=1)  # Line buffered
            pretrain_process = subprocess.Popen(
                cmd,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                cwd=os.path.dirname(os.path.abspath(__file__))
            )

            # éåŒæœŸã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç›£è¦–
            def monitor_pretrain():
                global pretrain_status, pretrain_process
                pretrain_process.wait()

                # Close the log file after the process finishes
                try:
                    log_file.close()
                except:
                    pass

                if pretrain_process.returncode == 0:
                    pretrain_status = "completed"
                    print("âœ… Pretraining completed successfully")
                else:
                    pretrain_status = "error"
                    print(f"âŒ Pretraining failed with code {pretrain_process.returncode}")

            monitor_thread = threading.Thread(target=monitor_pretrain, daemon=True)
            monitor_thread.start()

            return {
                "status": "success",
                "message": "Pretraining started",
                "pretrain_status": pretrain_status,
                "log_file": str(log_path),
                "pid": pretrain_process.pid
            }

        except Exception as e:
            pretrain_status = "error"
            return {
                "status": "error",
                "error": str(e),
                "pretrain_status": pretrain_status
            }

    # ========================================
    # PRETRAIN_STATUSï¼ˆäº‹å‰å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªï¼‰
    # ========================================
    if action == "pretrain_status":
        log_path = Path(pretrain_log_file)

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¾Œã®æ•°è¡Œã‚’èª­ã‚€
        log_tail = ""
        if log_path.exists():
            try:
                with open(log_path, 'r') as f:
                    lines = f.readlines()
                    log_tail = ''.join(lines[-20:])  # æœ€å¾Œã®20è¡Œ
            except Exception as e:
                log_tail = f"Error reading log: {e}"

        return {
            "status": "success",
            "pretrain_status": pretrain_status,
            "log_file": str(log_path),
            "log_exists": log_path.exists(),
            "log_tail": log_tail,
            "process_running": pretrain_process is not None and pretrain_process.poll() is None
        }

    # ========================================
    # TRAINï¼ˆå­¦ç¿’ - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä»˜ãï¼‰
    # ========================================
    if action == "train":
        """
        å­¦ç¿’å°‚ç”¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

        - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã‚’å®Ÿè¡Œ
        - å­¦ç¿’å¾Œã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
        - æ¨è«–ã¯ä¸€åˆ‡è¡Œã‚ãªã„
        """
        # ãƒ¢ãƒ‡ãƒ«ãŒæœªåˆæœŸåŒ–ã®å ´åˆã€æ–°è¦ä½œæˆ
        if not is_initialized:
            print("ğŸ”„ å­¦ç¿’ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’æ–°è¦ä½œæˆ...")
            try:
                model = NeuroQuantumBrainAI(
                    embed_dim=128,
                    num_heads=4,
                    num_layers=3,
                    num_neurons=100,
                    max_vocab=8000,
                    use_sentencepiece=True
                )

                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
                if os.path.exists(TOKENIZER_MODEL_PATH):
                    print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰: {TOKENIZER_MODEL_PATH}")
                    model.tokenizer = NeuroQuantumTokenizer(
                        vocab_size=8000,
                        model_file=TOKENIZER_MODEL_PATH
                    )

                is_initialized = True

            except Exception as e:
                return {
                    "status": "error",
                    "error": f"Failed to create model: {e}"
                }

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        texts = job_input.get("texts", None)
        epochs = job_input.get("epochs", 25)
        batch_size = job_input.get("batch_size", 16)
        lr = job_input.get("lr", 0.002)
        seq_length = job_input.get("seq_length", 48)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        if texts is None:
            print("ğŸ“š ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
            texts = get_training_data()

        if not texts:
            return {
                "status": "error",
                "error": "No training texts provided"
            }

        print(f"ğŸ”„ å­¦ç¿’é–‹å§‹: {len(texts)}ã‚µãƒ³ãƒ—ãƒ«, {epochs}ã‚¨ãƒãƒƒã‚¯")

        try:
            # å­¦ç¿’å®Ÿè¡Œ
            model.train(
                texts,
                epochs=epochs,
                batch_size=batch_size,
                lr=lr,
                seq_length=seq_length
            )

            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            checkpoint_path = job_input.get("checkpoint_path", MODEL_CHECKPOINT_PATH)
            if save_checkpoint(model, checkpoint_path):
                return {
                    "status": "success",
                    "message": f"Training completed ({epochs} epochs)",
                    "checkpoint_path": checkpoint_path,
                    "num_samples": len(texts)
                }
            else:
                return {
                    "status": "warning",
                    "message": "Training completed but checkpoint save failed",
                    "num_samples": len(texts)
                }

        except Exception as e:
            import traceback
            traceback.print_exc()
            return {
                "status": "error",
                "error": str(e)
            }
    
    # ========================================
    # CLEAR_SESSIONï¼ˆä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢ï¼‰
    # ========================================
    if action == "clear_session":
        global conversation_sessions
        session_id = job_input.get("session_id", "default")

        if session_id in conversation_sessions:
            del conversation_sessions[session_id]
            return {
                "status": "success",
                "message": f"Session '{session_id}' cleared"
            }
        else:
            return {
                "status": "success",
                "message": f"Session '{session_id}' not found (already empty)"
            }

    # ========================================
    # UNKNOWN ACTION
    # ========================================
    return {
        "status": "error",
        "error": f"Unknown action: {action}",
        "available_actions": [
            "health",           # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
            "status",           # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
            "train",            # å­¦ç¿’ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ï¼‰
            "generate",         # æ¨è«–ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰ï¼‰
            "pretrain_openai",  # OpenAIãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆäº‹å‰å­¦ç¿’
            "pretrain_status",  # äº‹å‰å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            "clear_session"     # ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢
        ]
    }


# ========================================
# èµ·å‹•ï¼ˆä½•ã‚‚ã—ãªã„ = é«˜é€Ÿèµ·å‹•ï¼‰
# ========================================
print("=" * 60)
print("âœ… NeuroQ Handler Ready")
print("   - Health check: instant response")
print("   - Model loading: lazy (on first request)")
print("=" * 60)

# RunPodèµ·å‹•
runpod.serverless.start({"handler": handler})
