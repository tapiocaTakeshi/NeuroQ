#!/usr/bin/env python3
"""
NeuroQ vocab_size ç°¡æ˜“ãƒã‚§ãƒƒã‚«ãƒ¼
================================
ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®vocab_sizeã®ã¿ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå­¦ç¿’ãªã—ï¼‰
"""

import os
import sys

try:
    import sentencepiece as spm

    print("=" * 70)
    print("ğŸ” NeuroQ vocab_size ç°¡æ˜“ãƒã‚§ãƒƒã‚¯")
    print("=" * 70)

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    tokenizer_paths = [
        "neuroq_tokenizer.model",
        "../neuroq_tokenizer.model",
        "neuroq_tokenizer_8k.model",
        "../neuroq_tokenizer_8k.model",
    ]

    print("\nğŸ“Š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«:")
    print("-" * 70)

    found_tokenizers = []
    for path in tokenizer_paths:
        if os.path.exists(path):
            try:
                sp = spm.SentencePieceProcessor()
                sp.load(path)
                vocab_size = sp.get_piece_size()
                found_tokenizers.append((path, vocab_size))
                print(f"âœ… {path}")
                print(f"   èªå½™ã‚µã‚¤ã‚º: {vocab_size:,}")
                print(f"   ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³:")
                print(f"   - <pad>: {sp.pad_id()}")
                print(f"   - <unk>: {sp.unk_id()}")
                print(f"   - <s>: {sp.bos_id()}")
                print(f"   - </s>: {sp.eos_id()}")
                print()
            except Exception as e:
                print(f"âŒ {path}: {e}")

    if not found_tokenizers:
        print("âŒ æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    # ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    print("\nğŸ§ª ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºãƒ†ã‚¹ãƒˆ:")
    print("-" * 70)

    test_path, test_vocab_size = found_tokenizers[0]
    sp = spm.SentencePieceProcessor()
    sp.load(test_path)

    test_texts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¤ã„ã¦æ•™ãˆã¦",
        "äººå·¥çŸ¥èƒ½ãŒæœªæ¥ã‚’å¤‰ãˆã‚‹",
        "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã­",
    ]

    for text in test_texts:
        tokens = sp.encode(text, out_type=str)
        ids = sp.encode(text, out_type=int)
        decoded = sp.decode(ids)

        print(f"\nå…¥åŠ›: {text}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
        print(f"IDæ•°: {len(ids)}")
        print(f"ãƒ‡ã‚³ãƒ¼ãƒ‰: {decoded}")

    print("\n" + "=" * 70)
    print("âœ… ãƒã‚§ãƒƒã‚¯å®Œäº†")
    print("=" * 70)
    print(f"\næ¨å¥¨vocab_size: {test_vocab_size:,}")
    print("ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã§ä»¥ä¸‹ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:")
    print(f"  - NeuroQuantumConfig(vocab_size={test_vocab_size})")
    print(f"  - NeuroQuantumTokenizer(vocab_size={test_vocab_size}, model_file='{test_path}')")

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
