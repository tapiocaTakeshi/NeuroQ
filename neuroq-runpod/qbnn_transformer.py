#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—                                       â•‘
â•‘  â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘                                       â•‘
â•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘                                       â•‘
â•‘  â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘                                       â•‘
â•‘  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                                       â•‘
â•‘   â•šâ•â•â–€â–€â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•  â•šâ•â•â•â•                                       â•‘
â•‘                                                                               â•‘
â•‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â•‘
â•‘  â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â•‘
â•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â•‘
â•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â•‘
â•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â•‘
â•‘     â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•      â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•     â•šâ•â•â•‘
â•‘                                                                               â•‘
â•‘   QBNN Transformer: é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹Transformer          â•‘
â•‘                                                                               â•‘
â•‘   ç‰¹å¾´:                                                                       â•‘
â•‘   - é‡å­æ³¨æ„æ©Ÿæ§‹ (Quantum Attention) - APQBãƒ™ãƒ¼ã‚¹ã®è‡ªå·±æ³¨æ„                   â•‘
â•‘   - é‡å­ã‚‚ã¤ã‚Œãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ (Entangled Multi-Head) - é‡å­ç›¸é–¢ã‚’æ´»ç”¨            â•‘
â•‘   - é‡å­ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (Quantum Positional Encoding)                    â•‘
â•‘   - å¹¾ä½•å­¦çš„åˆ¶ç´„ã«ã‚ˆã‚‹æ­£å‰‡åŒ– (rÂ² + TÂ² = 1)                                    â•‘
â•‘                                                                               â•‘
â•‘   transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¸è¦ - ç´”ç²‹ãªPyTorch + QBNNå®Ÿè£…                       â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Optional, Tuple, List
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("ğŸ§ âš›ï¸ QBNN Transformer")
print("   é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹Transformer")
print("   (transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¸è¦)")
print("=" * 70)


# ========================================================================
# 1. APQB (Adjustable Pseudo Quantum Bit) - é‡å­ãƒ“ãƒƒãƒˆã®ã‚³ã‚¢
# ========================================================================

class APQB:
    """
    èª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆ (APQB)
    
    é‡å­çŠ¶æ…‹ã‚’å¤å…¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ä¸Šã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    - Î¸: å†…éƒ¨è§’åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (0 â‰¤ Î¸ â‰¤ Ï€/2)
    - r = cos(2Î¸): ç›¸é–¢ä¿‚æ•° (-1 â‰¤ r â‰¤ 1)
    - T = |sin(2Î¸)|: æ¸©åº¦/æºã‚‰ã (0 â‰¤ T â‰¤ 1)
    - åˆ¶ç´„: rÂ² + TÂ² = 1 (å˜ä½å††ä¸Š)
    """
    
    @staticmethod
    def theta_to_state(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ é‡å­çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« [cos(Î¸), sin(Î¸)]"""
        return torch.stack([torch.cos(theta), torch.sin(theta)], dim=-1)
    
    @staticmethod
    def theta_to_r(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ ç›¸é–¢ä¿‚æ•° r = cos(2Î¸)"""
        return torch.cos(2 * theta)
    
    @staticmethod
    def theta_to_T(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ æ¸©åº¦ T = |sin(2Î¸)|"""
        return torch.abs(torch.sin(2 * theta))
    
    @staticmethod
    def theta_to_complex(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ è¤‡ç´ æ•°è¡¨ç¾ z = e^{i2Î¸} = (cos(2Î¸), sin(2Î¸))"""
        return torch.stack([torch.cos(2 * theta), torch.sin(2 * theta)], dim=-1)
    
    @staticmethod
    def constraint_loss(theta: torch.Tensor) -> torch.Tensor:
        """å¹¾ä½•å­¦çš„åˆ¶ç´„ rÂ² + TÂ² = 1 ã®æå¤±"""
        r = APQB.theta_to_r(theta)
        T = APQB.theta_to_T(theta)
        return ((r**2 + T**2 - 1) ** 2).mean()
    
    @staticmethod
    def measure(theta: torch.Tensor) -> torch.Tensor:
        """é‡å­æ¸¬å®šï¼ˆç¢ºç‡çš„ã«0 or 1ã‚’è¿”ã™ï¼‰"""
        prob_1 = torch.sin(theta) ** 2
        return (torch.rand_like(prob_1) < prob_1).float()


# ========================================================================
# 2. é‡å­å›è»¢ã‚²ãƒ¼ãƒˆ (Quantum Rotation Gates)
# ========================================================================

class QuantumRotationGate(nn.Module):
    """
    é‡å­å›è»¢ã‚²ãƒ¼ãƒˆ
    
    Rx(Î¸), Ry(Î¸), Rz(Î¸) ã®å®Ÿè£…
    ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚„çŠ¶æ…‹å¤‰æ›ã«ä½¿ç”¨
    """
    
    def __init__(self, dim: int, gate_type: str = 'Ry'):
        super().__init__()
        self.dim = dim
        self.gate_type = gate_type
        self.theta = nn.Parameter(torch.rand(dim) * np.pi / 4)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å›è»¢ã‚²ãƒ¼ãƒˆã‚’é©ç”¨
        
        Args:
            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« [..., dim]
        
        Returns:
            å›è»¢å¾Œã®ãƒ†ãƒ³ã‚½ãƒ« [..., dim]
        """
        if self.gate_type == 'Rx':
            # Rx(Î¸) = cos(Î¸/2)I - i*sin(Î¸/2)X
            cos_t = torch.cos(self.theta / 2)
            sin_t = torch.sin(self.theta / 2)
            return x * cos_t + x.roll(1, dims=-1) * sin_t
        
        elif self.gate_type == 'Ry':
            # Ry(Î¸) = cos(Î¸/2)I - i*sin(Î¸/2)Y  
            cos_t = torch.cos(self.theta / 2)
            sin_t = torch.sin(self.theta / 2)
            return x * cos_t + torch.flip(x, dims=[-1]) * sin_t
        
        elif self.gate_type == 'Rz':
            # Rz(Î¸) = e^{-iÎ¸/2}|0âŸ©âŸ¨0| + e^{iÎ¸/2}|1âŸ©âŸ¨1|
            phase = torch.exp(1j * self.theta / 2)
            return x * phase.real
        
        return x


# ========================================================================
# 3. é‡å­ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (Quantum Positional Encoding)
# ========================================================================

class QuantumPositionalEncoding(nn.Module):
    """
    é‡å­ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    
    å¾“æ¥ã®sin/cosã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«åŠ ãˆã€
    é‡å­å›è»¢ã‚²ãƒ¼ãƒˆã«ã‚ˆã‚‹ä½ç›¸æƒ…å ±ã‚’ä»˜åŠ 
    """
    
    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.dropout = nn.Dropout(p=dropout)
        
        # å¾“æ¥ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
        # é‡å­å›è»¢ã‚²ãƒ¼ãƒˆã«ã‚ˆã‚‹è¿½åŠ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.quantum_gate = QuantumRotationGate(d_model, 'Ry')
        
        # é‡å­ä½ç›¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.quantum_phase = nn.Parameter(torch.rand(max_len, d_model) * np.pi / 4)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨
        
        Args:
            x: å…¥åŠ› [batch, seq_len, d_model]
        
        Returns:
            ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä»˜ã [batch, seq_len, d_model]
        """
        seq_len = x.size(1)
        
        # å¾“æ¥ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        classical_pe = self.pe[:, :seq_len, :]
        
        # é‡å­ä½ç›¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        quantum_pe = torch.cos(self.quantum_phase[:seq_len, :]) * 0.1
        
        # çµåˆ
        x = x + classical_pe + quantum_pe.unsqueeze(0)
        
        # é‡å­å›è»¢ã‚²ãƒ¼ãƒˆã‚’é©ç”¨
        x = self.quantum_gate(x)
        
        return self.dropout(x)


# ========================================================================
# 4. é‡å­æ³¨æ„æ©Ÿæ§‹ (Quantum Attention)
# ========================================================================

class QuantumAttention(nn.Module):
    """
    é‡å­æ³¨æ„æ©Ÿæ§‹ (Quantum Attention)
    
    APQBã‚’ä½¿ç”¨ã—ãŸè‡ªå·±æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
    - é‡å­çŠ¶æ…‹ãƒ™ãƒ¼ã‚¹ã®Query/Key/Valueå¤‰æ›
    - é‡å­ã‚‚ã¤ã‚Œã«ã‚ˆã‚‹ç›¸é–¢è¨ˆç®—
    - ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    """
    
    def __init__(self, d_model: int, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.scale = math.sqrt(d_model)
        
        # Q, K, V ã®ç·šå½¢å¤‰æ›
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
        # é‡å­çŠ¶æ…‹å¤‰æ›ï¼ˆÎ¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆï¼‰
        self.theta_proj = nn.Linear(d_model, d_model)
        
        # å‡ºåŠ›æŠ•å½±
        self.out_proj = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
        # é‡å­ã‚‚ã¤ã‚Œå¼·åº¦
        self.entangle_strength = nn.Parameter(torch.tensor(0.5))
    
    def forward(
        self, 
        x: torch.Tensor, 
        mask: Optional[torch.Tensor] = None,
        use_quantum_sampling: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        é‡å­æ³¨æ„ã®è¨ˆç®—
        
        Args:
            x: å…¥åŠ› [batch, seq_len, d_model]
            mask: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ [batch, seq_len, seq_len]
            use_quantum_sampling: é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        
        Returns:
            output: æ³¨æ„å‡ºåŠ› [batch, seq_len, d_model]
            attn_weights: æ³¨æ„é‡ã¿ [batch, seq_len, seq_len]
        """
        batch_size, seq_len, _ = x.shape
        
        # Q, K, V ã‚’è¨ˆç®—
        Q = self.W_q(x)  # [batch, seq_len, d_model]
        K = self.W_k(x)
        V = self.W_v(x)
        
        # é‡å­çŠ¶æ…‹Î¸ã‚’è¨ˆç®—
        theta_q = torch.sigmoid(self.theta_proj(Q)) * np.pi / 2
        theta_k = torch.sigmoid(self.theta_proj(K)) * np.pi / 2
        
        # é‡å­ç›¸é–¢ä¿‚æ•° r ã‚’è¨ˆç®—
        r_q = APQB.theta_to_r(theta_q)  # [batch, seq_len, d_model]
        r_k = APQB.theta_to_r(theta_k)
        
        # é‡å­æ¸©åº¦ T ã‚’è¨ˆç®—ï¼ˆæºã‚‰ãï¼‰
        T_q = APQB.theta_to_T(theta_q)
        T_k = APQB.theta_to_T(theta_k)
        
        # === é‡å­ã‚‚ã¤ã‚Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ ===
        
        # 1. å¤å…¸çš„ãªã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ
        classical_attn = torch.bmm(Q, K.transpose(-2, -1)) / self.scale
        
        # 2. é‡å­ç›¸é–¢é …ï¼ˆr_q ã¨ r_k ã®ç›¸äº’ä½œç”¨ï¼‰
        quantum_corr = torch.bmm(r_q, r_k.transpose(-2, -1)) * self.entangle_strength
        
        # 3. æ¸©åº¦ã«ã‚ˆã‚‹æºã‚‰ã
        if use_quantum_sampling:
            noise = torch.randn_like(classical_attn) * T_q.mean(dim=-1, keepdim=True)
            classical_attn = classical_attn + noise * 0.1
        
        # 4. æœ€çµ‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢
        attn_scores = classical_attn + quantum_corr
        
        # ãƒã‚¹ã‚¯é©ç”¨
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))
        
        # Softmax
        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # Value ã¨ã®ç©
        output = torch.bmm(attn_weights, V)
        output = self.out_proj(output)
        
        return output, attn_weights
    
    def get_quantum_stats(self, x: torch.Tensor) -> dict:
        """é‡å­çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        theta = torch.sigmoid(self.theta_proj(x)) * np.pi / 2
        r = APQB.theta_to_r(theta)
        T = APQB.theta_to_T(theta)
        
        return {
            'r_mean': r.mean().item(),
            'T_mean': T.mean().item(),
            'entangle_strength': self.entangle_strength.item(),
            'constraint': (r**2 + T**2).mean().item()
        }


# ========================================================================
# 5. é‡å­ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ (Quantum Multi-Head Attention)
# ========================================================================

class QuantumMultiHeadAttention(nn.Module):
    """
    é‡å­ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
    
    è¤‡æ•°ã®QuantumAttentionãƒ˜ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã€
    ãƒ˜ãƒƒãƒ‰é–“ã§é‡å­ã‚‚ã¤ã‚Œã‚’å…±æœ‰
    """
    
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Q, K, V ã®ç·šå½¢å¤‰æ›
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
        # å‡ºåŠ›æŠ•å½±
        self.out_proj = nn.Linear(d_model, d_model)
        
        # é‡å­çŠ¶æ…‹Î¸æŠ•å½±ï¼ˆå„ãƒ˜ãƒƒãƒ‰ç”¨ï¼‰
        self.theta_proj = nn.Linear(self.d_k, self.d_k)
        
        # ãƒ˜ãƒƒãƒ‰é–“ã‚‚ã¤ã‚Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.inter_head_entangle = nn.Parameter(torch.rand(num_heads, num_heads) * 0.1)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_k)
        
        # å„ãƒ˜ãƒƒãƒ‰ã®ã‚‚ã¤ã‚Œå¼·åº¦
        self.entangle_strengths = nn.Parameter(torch.ones(num_heads) * 0.5)
    
    def forward(
        self, 
        x: torch.Tensor, 
        mask: Optional[torch.Tensor] = None,
        use_quantum_sampling: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰é‡å­æ³¨æ„ã®è¨ˆç®—
        
        Args:
            x: å…¥åŠ› [batch, seq_len, d_model]
            mask: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯
            use_quantum_sampling: é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä½¿ç”¨
        
        Returns:
            output: å‡ºåŠ› [batch, seq_len, d_model]
            attn_weights: æ³¨æ„é‡ã¿ [batch, num_heads, seq_len, seq_len]
        """
        batch_size, seq_len, _ = x.shape
        
        # Q, K, V ã‚’è¨ˆç®—ã—ã¦ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰²
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        # [batch, num_heads, seq_len, d_k]
        
        # å„ãƒ˜ãƒƒãƒ‰ã§é‡å­æ³¨æ„ã‚’è¨ˆç®—
        all_attn_scores = []
        all_quantum_corrs = []
        
        for h in range(self.num_heads):
            q_h = Q[:, h]  # [batch, seq_len, d_k]
            k_h = K[:, h]
            
            # é‡å­çŠ¶æ…‹Î¸ã‚’è¨ˆç®—
            theta_q = torch.sigmoid(self.theta_proj(q_h)) * np.pi / 2
            theta_k = torch.sigmoid(self.theta_proj(k_h)) * np.pi / 2
            
            # é‡å­ç›¸é–¢
            r_q = APQB.theta_to_r(theta_q)
            r_k = APQB.theta_to_r(theta_k)
            
            # å¤å…¸çš„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
            classical_attn = torch.bmm(q_h, k_h.transpose(-2, -1)) / self.scale
            
            # é‡å­ç›¸é–¢é …
            quantum_corr = torch.bmm(r_q, r_k.transpose(-2, -1)) * self.entangle_strengths[h]
            
            all_attn_scores.append(classical_attn)
            all_quantum_corrs.append(quantum_corr)
        
        # ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ãƒ˜ãƒƒãƒ‰é–“ã‚‚ã¤ã‚Œã‚’é©ç”¨
        attn_scores = torch.stack(all_attn_scores, dim=1)  # [batch, heads, seq, seq]
        quantum_corrs = torch.stack(all_quantum_corrs, dim=1)
        
        # ãƒ˜ãƒƒãƒ‰é–“ã‚‚ã¤ã‚Œï¼ˆä»–ã®ãƒ˜ãƒƒãƒ‰ã®é‡å­ç›¸é–¢ã‚’æ··åˆï¼‰
        inter_head_effect = torch.einsum('bhij,hg->bgij', quantum_corrs, self.inter_head_entangle)
        
        # æœ€çµ‚ã‚¹ã‚³ã‚¢
        final_scores = attn_scores + quantum_corrs + inter_head_effect * 0.1
        
        # é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if use_quantum_sampling:
            noise = torch.randn_like(final_scores) * 0.05
            final_scores = final_scores + noise
        
        # ãƒã‚¹ã‚¯é©ç”¨
        if mask is not None:
            if mask.dim() == 3:
                mask = mask.unsqueeze(1)  # [batch, 1, seq, seq]
            final_scores = final_scores.masked_fill(mask == 0, float('-inf'))
        
        # Softmax
        attn_weights = F.softmax(final_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # Valueã¨ã®ç©
        output = torch.matmul(attn_weights, V)  # [batch, heads, seq, d_k]
        
        # ãƒ˜ãƒƒãƒ‰ã‚’çµåˆ
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        output = self.out_proj(output)
        
        return output, attn_weights


# ========================================================================
# 6. é‡å­ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (Quantum Feed-Forward)
# ========================================================================

class QuantumFeedForward(nn.Module):
    """
    é‡å­ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    
    APQBãƒ™ãƒ¼ã‚¹ã®éç·šå½¢å¤‰æ›ã‚’ä½¿ç”¨
    """
    
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.d_ff = d_ff
        
        # 2å±¤ã®FFN
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        
        # é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.theta_ff = nn.Parameter(torch.rand(d_ff) * np.pi / 4)
        
        self.dropout = nn.Dropout(dropout)
    
    def quantum_activation(self, x: torch.Tensor) -> torch.Tensor:
        """
        é‡å­æ´»æ€§åŒ–é–¢æ•°
        
        GELU + é‡å­æºã‚‰ãã®çµ„ã¿åˆã‚ã›
        """
        # åŸºæœ¬ã®GELU
        gelu = F.gelu(x)
        
        # é‡å­æ¸©åº¦ã«ã‚ˆã‚‹æºã‚‰ã
        T = APQB.theta_to_T(self.theta_ff)
        quantum_noise = T * 0.1
        
        return gelu * (1 + quantum_noise.unsqueeze(0).unsqueeze(0))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰è¨ˆç®—
        
        Args:
            x: å…¥åŠ› [batch, seq_len, d_model]
        
        Returns:
            å‡ºåŠ› [batch, seq_len, d_model]
        """
        x = self.linear1(x)
        x = self.quantum_activation(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x


# ========================================================================
# 7. QBNN Transformer ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼
# ========================================================================

class QBNNTransformerEncoderLayer(nn.Module):
    """
    QBNN Transformer ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼
    
    æ§‹æˆ:
    1. é‡å­ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
    2. æ®‹å·®æ¥ç¶š + ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–
    3. é‡å­ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
    4. æ®‹å·®æ¥ç¶š + ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–
    """
    
    def __init__(
        self, 
        d_model: int, 
        num_heads: int, 
        d_ff: int, 
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.self_attn = QuantumMultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = QuantumFeedForward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(
        self, 
        x: torch.Tensor, 
        mask: Optional[torch.Tensor] = None,
        use_quantum_sampling: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é †ä¼æ’­
        
        Args:
            x: å…¥åŠ› [batch, seq_len, d_model]
            mask: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯
            use_quantum_sampling: é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        
        Returns:
            output: å‡ºåŠ› [batch, seq_len, d_model]
            attn_weights: æ³¨æ„é‡ã¿
        """
        # é‡å­è‡ªå·±æ³¨æ„
        attn_output, attn_weights = self.self_attn(x, mask, use_quantum_sampling)
        x = self.norm1(x + self.dropout(attn_output))
        
        # é‡å­ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x, attn_weights


# ========================================================================
# 8. QBNN Transformer ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼
# ========================================================================

class QBNNTransformerDecoderLayer(nn.Module):
    """
    QBNN Transformer ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼
    
    æ§‹æˆ:
    1. ãƒã‚¹ã‚¯ä»˜ãé‡å­è‡ªå·±æ³¨æ„
    2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼-ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼æ³¨æ„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    3. é‡å­ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
    """
    
    def __init__(
        self, 
        d_model: int, 
        num_heads: int, 
        d_ff: int, 
        dropout: float = 0.1
    ):
        super().__init__()
        
        # ãƒã‚¹ã‚¯ä»˜ãè‡ªå·±æ³¨æ„
        self.self_attn = QuantumMultiHeadAttention(d_model, num_heads, dropout)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼-ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼æ³¨æ„
        self.cross_attn = QuantumMultiHeadAttention(d_model, num_heads, dropout)
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        self.feed_forward = QuantumFeedForward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(
        self, 
        x: torch.Tensor,
        encoder_output: Optional[torch.Tensor] = None,
        self_attn_mask: Optional[torch.Tensor] = None,
        cross_attn_mask: Optional[torch.Tensor] = None,
        use_quantum_sampling: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        """
        ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é †ä¼æ’­
        """
        # ãƒã‚¹ã‚¯ä»˜ãè‡ªå·±æ³¨æ„
        attn_output, self_attn_weights = self.self_attn(x, self_attn_mask, use_quantum_sampling)
        x = self.norm1(x + self.dropout(attn_output))
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼-ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼æ³¨æ„
        cross_attn_weights = None
        if encoder_output is not None:
            cross_output, cross_attn_weights = self.cross_attn(
                x, cross_attn_mask, use_quantum_sampling
            )
            x = self.norm2(x + self.dropout(cross_output))
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x, self_attn_weights, cross_attn_weights


# ========================================================================
# 9. QBNN Transformer (å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«)
# ========================================================================

class QBNNTransformer(nn.Module):
    """
    QBNN Transformer
    
    é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«
    transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¸è¦
    
    ç‰¹å¾´:
    - é‡å­æ³¨æ„æ©Ÿæ§‹
    - é‡å­ã‚‚ã¤ã‚Œãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰
    - é‡å­ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    - å¹¾ä½•å­¦çš„åˆ¶ç´„ã«ã‚ˆã‚‹æ­£å‰‡åŒ–
    """
    
    def __init__(
        self,
        vocab_size: int,
        d_model: int = 256,
        num_heads: int = 8,
        num_layers: int = 6,
        d_ff: int = 1024,
        max_seq_len: int = 512,
        dropout: float = 0.1,
        pad_token_id: int = 0
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.pad_token_id = pad_token_id
        
        # åŸ‹ã‚è¾¼ã¿å±¤
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)
        
        # é‡å­ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.pos_encoding = QuantumPositionalEncoding(d_model, max_seq_len, dropout)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆè¨€èªãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰
        self.layers = nn.ModuleList([
            QBNNTransformerDecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
        # å‡ºåŠ›å±¤
        self.output_norm = nn.LayerNorm(d_model)
        self.output_proj = nn.Linear(d_model, vocab_size)
        
        # é‡ã¿å…±æœ‰ï¼ˆembedding ã¨ outputï¼‰
        self.output_proj.weight = self.embedding.weight
        
        # åˆæœŸåŒ–
        self._init_weights()
        
        print(f"âœ… QBNN Transformer åˆæœŸåŒ–å®Œäº†")
        print(f"   vocab_size: {vocab_size}")
        print(f"   d_model: {d_model}")
        print(f"   num_heads: {num_heads}")
        print(f"   num_layers: {num_layers}")
        print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.count_parameters():,}")
    
    def _init_weights(self):
        """é‡ã¿ã®åˆæœŸåŒ–"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def count_parameters(self) -> int:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def generate_causal_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:
        """å› æœãƒã‚¹ã‚¯ã‚’ç”Ÿæˆï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãˆãªãã™ã‚‹ï¼‰"""
        mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask
    
    def forward(
        self, 
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        use_quantum_sampling: bool = False
    ) -> torch.Tensor:
        """
        é †ä¼æ’­
        
        Args:
            input_ids: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ID [batch, seq_len]
            attention_mask: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ [batch, seq_len]
            use_quantum_sampling: é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨
        
        Returns:
            logits: å‡ºåŠ›ãƒ­ã‚¸ãƒƒãƒˆ [batch, seq_len, vocab_size]
        """
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        # åŸ‹ã‚è¾¼ã¿ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        x = self.embedding(input_ids) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        
        # å› æœãƒã‚¹ã‚¯ï¼ˆè‡ªå·±å›å¸°ç”¨ï¼‰
        causal_mask = self.generate_causal_mask(seq_len, device)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯ã¨çµ„ã¿åˆã‚ã›
        if attention_mask is not None:
            padding_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            combined_mask = causal_mask.unsqueeze(0) + (1 - padding_mask) * float('-inf')
        else:
            combined_mask = causal_mask.unsqueeze(0)
        
        # Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é€šé
        for layer in self.layers:
            x, _, _ = layer(x, None, combined_mask, None, use_quantum_sampling)
        
        # å‡ºåŠ›
        x = self.output_norm(x)
        logits = self.output_proj(x)
        
        return logits
    
    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.Tensor,
        max_length: int = 100,
        temperature: float = 0.8,
        top_k: int = 50,
        top_p: float = 0.9,
        repetition_penalty: float = 1.2,
        use_quantum_sampling: bool = True,
        eos_token_id: Optional[int] = None
    ) -> torch.Tensor:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            input_ids: é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ [batch, seq_len] or [seq_len]
            max_length: æœ€å¤§ç”Ÿæˆé•·
            temperature: æ¸©åº¦ï¼ˆé«˜ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
            top_k: Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            top_p: Nucleus ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            repetition_penalty: ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
            use_quantum_sampling: é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            eos_token_id: çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ID
        
        Returns:
            generated: ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ [batch, new_seq_len]
        """
        self.eval()
        
        # å…¥åŠ›ã®å½¢çŠ¶ã‚’èª¿æ•´
        if input_ids.dim() == 1:
            input_ids = input_ids.unsqueeze(0)
        
        device = input_ids.device
        batch_size = input_ids.size(0)
        generated = input_ids.clone()
        
        for _ in range(max_length):
            # ç¾åœ¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§äºˆæ¸¬
            logits = self(generated, use_quantum_sampling=use_quantum_sampling)
            
            # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ã‚’å–å¾—
            next_token_logits = logits[:, -1, :] / temperature
            
            # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
            if repetition_penalty != 1.0:
                for i in range(batch_size):
                    for token_id in generated[i].unique():
                        next_token_logits[i, token_id] /= repetition_penalty
            
            # Top-K ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_k > 0:
                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # Top-P (Nucleus) ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                for i in range(batch_size):
                    indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]
                    next_token_logits[i, indices_to_remove] = float('-inf')
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            probs = F.softmax(next_token_logits, dim=-1)
            
            if use_quantum_sampling:
                # é‡å­ãƒã‚¤ã‚ºã‚’è¿½åŠ 
                quantum_noise = torch.randn_like(probs) * 0.01
                probs = probs + quantum_noise.abs()
                probs = probs / probs.sum(dim=-1, keepdim=True)
            
            next_token = torch.multinomial(probs, num_samples=1)
            
            # ç”Ÿæˆçµæœã«è¿½åŠ 
            generated = torch.cat([generated, next_token], dim=1)
            
            # EOSãƒã‚§ãƒƒã‚¯
            if eos_token_id is not None and (next_token == eos_token_id).all():
                break
        
        return generated
    
    def get_quantum_stats(self) -> List[dict]:
        """å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡å­çµ±è¨ˆã‚’å–å¾—"""
        stats = []
        for i, layer in enumerate(self.layers):
            layer_stats = {
                'layer': i,
                'entangle_strengths': layer.self_attn.entangle_strengths.tolist(),
                'inter_head_entangle_mean': layer.self_attn.inter_head_entangle.mean().item()
            }
            stats.append(layer_stats)
        return stats
    
    def get_constraint_loss(self) -> torch.Tensor:
        """å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å¹¾ä½•å­¦çš„åˆ¶ç´„æå¤±"""
        total_loss = torch.tensor(0.0, device=next(self.parameters()).device)
        
        for layer in self.layers:
            ff = layer.feed_forward
            theta = ff.theta_ff
            total_loss += APQB.constraint_loss(theta)
        
        return total_loss / len(self.layers)


# ========================================================================
# 10. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================================================

def create_qbnn_transformer(
    vocab_size: int,
    model_size: str = 'small'
) -> QBNNTransformer:
    """
    QBNN Transformer ã‚’ä½œæˆ
    
    Args:
        vocab_size: èªå½™ã‚µã‚¤ã‚º
        model_size: 'tiny', 'small', 'medium', 'large'
    
    Returns:
        QBNNTransformer ãƒ¢ãƒ‡ãƒ«
    """
    configs = {
        'tiny': {'d_model': 128, 'num_heads': 4, 'num_layers': 2, 'd_ff': 512},
        'small': {'d_model': 256, 'num_heads': 8, 'num_layers': 4, 'd_ff': 1024},
        'medium': {'d_model': 512, 'num_heads': 8, 'num_layers': 6, 'd_ff': 2048},
        'large': {'d_model': 768, 'num_heads': 12, 'num_layers': 12, 'd_ff': 3072}
    }
    
    config = configs.get(model_size, configs['small'])
    
    return QBNNTransformer(
        vocab_size=vocab_size,
        **config
    )


# ========================================================================
# ãƒ†ã‚¹ãƒˆ
# ========================================================================

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ğŸ§ª QBNN Transformer ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # ãƒ‡ãƒã‚¤ã‚¹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    vocab_size = 8000
    model = create_qbnn_transformer(vocab_size, 'small').to(device)
    
    # ãƒ†ã‚¹ãƒˆå…¥åŠ›
    batch_size = 2
    seq_len = 32
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
    
    # é †ä¼æ’­ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“Š é †ä¼æ’­ãƒ†ã‚¹ãƒˆ...")
    logits = model(input_ids)
    print(f"   å…¥åŠ›å½¢çŠ¶: {input_ids.shape}")
    print(f"   å‡ºåŠ›å½¢çŠ¶: {logits.shape}")
    
    # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    print("\nğŸ² ç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
    start_tokens = torch.randint(0, vocab_size, (1, 5)).to(device)
    generated = model.generate(start_tokens, max_length=20, use_quantum_sampling=True)
    print(f"   é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³: {start_tokens.shape}")
    print(f"   ç”Ÿæˆçµæœ: {generated.shape}")
    
    # é‡å­çµ±è¨ˆ
    print("\nâš›ï¸ é‡å­çµ±è¨ˆ:")
    stats = model.get_quantum_stats()
    for s in stats[:2]:
        print(f"   Layer {s['layer']}: entangle_mean={s['inter_head_entangle_mean']:.4f}")
    
    # åˆ¶ç´„æå¤±
    constraint_loss = model.get_constraint_loss()
    print(f"\nğŸ“ å¹¾ä½•å­¦çš„åˆ¶ç´„æå¤±: {constraint_loss.item():.6f}")
    
    print("\nâœ… QBNN Transformer ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print("=" * 70)
