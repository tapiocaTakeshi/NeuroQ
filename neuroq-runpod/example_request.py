#!/usr/bin/env python3
"""
RunPod Serverless Handler ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹

ä½¿ç”¨æ–¹æ³•:
    python example_request.py
"""

import requests
import json
import os
from typing import Dict, Any, Optional

# ========================================
# è¨­å®š
# ========================================

# RunPod APIè¨­å®š
RUNPOD_API_KEY = os.getenv("RUNPOD_API_KEY", "YOUR_API_KEY_HERE")
ENDPOINT_ID = os.getenv("RUNPOD_ENDPOINT_ID", "YOUR_ENDPOINT_ID_HERE")
RUNPOD_URL = f"https://api.runpod.ai/v2/{ENDPOINT_ID}/run"

# ========================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ========================================

def send_request(payload: Dict[str, Any], timeout: int = 600) -> Dict[str, Any]:
    """
    RunPod Serverless Handlerã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
    
    Args:
        payload: ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    
    Returns:
        ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {RUNPOD_API_KEY}"
    }
    
    try:
        print(f"ğŸ“¤ ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡: {payload.get('input', {}).get('action', 'unknown')}")
        response = requests.post(
            RUNPOD_URL,
            headers=headers,
            json=payload,
            timeout=timeout
        )
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        return {"error": f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}"}


def wait_for_completion(job_id: str, timeout: int = 600) -> Dict[str, Any]:
    """
    ã‚¸ãƒ§ãƒ–ã®å®Œäº†ã‚’å¾…ã¤
    
    Args:
        job_id: ã‚¸ãƒ§ãƒ–ID
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    
    Returns:
        çµæœ
    """
    import time
    
    status_url = f"https://api.runpod.ai/v2/{ENDPOINT_ID}/status/{job_id}"
    headers = {
        "Authorization": f"Bearer {RUNPOD_API_KEY}"
    }
    
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            response = requests.get(status_url, headers=headers, timeout=30)
            response.raise_for_status()
            result = response.json()
            
            status = result.get("status", "UNKNOWN")
            print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")
            
            if status == "COMPLETED":
                return result.get("output", {})
            elif status == "FAILED":
                return {"error": result.get("error", "ã‚¸ãƒ§ãƒ–ãŒå¤±æ•—ã—ã¾ã—ãŸ")}
            
            time.sleep(2)  # 2ç§’å¾…æ©Ÿ
        except requests.exceptions.RequestException as e:
            return {"error": f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}"}
    
    return {"error": "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ"}


# ========================================
# ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹
# ========================================

def health_check() -> Dict[str, Any]:
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    payload = {
        "input": {
            "action": "health"
        }
    }
    return send_request(payload, timeout=30)


def init_model_layered(
    embed_dim: int = 64,
    hidden_dim: int = 128,
    num_heads: int = 4,
    num_layers: int = 2,
    max_seq_len: int = 128,
    dropout: float = 0.1,
    lambda_entangle: float = 0.35,
    use_openai_embedding: bool = False,
    openai_api_key: Optional[str] = None,
    openai_model: str = "text-embedding-3-large"
) -> Dict[str, Any]:
    """Layeredãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
    payload = {
        "input": {
            "action": "init",
            "mode": "layered",
            "embed_dim": embed_dim,
            "hidden_dim": hidden_dim,
            "num_heads": num_heads,
            "num_layers": num_layers,
            "max_seq_len": max_seq_len,
            "dropout": dropout,
            "lambda_entangle": lambda_entangle,
            "use_openai_embedding": use_openai_embedding,
            "openai_api_key": openai_api_key,
            "openai_model": openai_model,
        }
    }
    return send_request(payload, timeout=60)


def init_model_brain(
    embed_dim: int = 128,
    num_heads: int = 4,
    num_layers: int = 3,
    num_neurons: int = 75,
    max_vocab: int = 16000,
    use_openai_embedding: bool = False,
    openai_api_key: Optional[str] = None,
    openai_model: str = "text-embedding-3-large"
) -> Dict[str, Any]:
    """Brainãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
    payload = {
        "input": {
            "action": "init",
            "mode": "brain",
            "embed_dim": embed_dim,
            "num_heads": num_heads,
            "num_layers": num_layers,
            "num_neurons": num_neurons,
            "max_vocab": max_vocab,
            "use_openai_embedding": use_openai_embedding,
            "openai_api_key": openai_api_key,
            "openai_model": openai_model,
        }
    }
    return send_request(payload, timeout=60)


def train_model_layered(
    texts: Optional[list] = None,
    epochs: int = 20,
    batch_size: int = 16,
    learning_rate: float = 0.001,
    seq_length: int = 64,
    vocab_size: int = 16000,
    data_sources: list = ["huggingface"],
    common_crawl_config: Optional[Dict] = None,
    max_records: int = 100,
    **model_kwargs
) -> Dict[str, Any]:
    """Layeredãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    payload = {
        "input": {
            "action": "train",
            "mode": "layered",
            "epochs": epochs,
            "batch_size": batch_size,
            "learning_rate": learning_rate,
            "seq_length": seq_length,
            "vocab_size": vocab_size,
            "data_sources": data_sources,
            "common_crawl_config": common_crawl_config or {},
            "max_records": max_records,
            **model_kwargs
        }
    }
    
    if texts:
        payload["input"]["texts"] = texts
    
    return send_request(payload, timeout=3600)  # 1æ™‚é–“


def train_model_brain(
    texts: Optional[list] = None,
    epochs: int = 20,
    batch_size: int = 16,
    learning_rate: float = 0.001,
    seq_length: int = 64,
    max_records: int = 100,
    data_sources: list = ["huggingface"],
    common_crawl_config: Optional[Dict] = None,
    **model_kwargs
) -> Dict[str, Any]:
    """Brainãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    payload = {
        "input": {
            "action": "train",
            "mode": "brain",
            "epochs": epochs,
            "batch_size": batch_size,
            "learning_rate": learning_rate,
            "seq_length": seq_length,
            "data_sources": data_sources,
            "common_crawl_config": common_crawl_config or {},
            "max_records": max_records,
            **model_kwargs
        }
    }
    
    if texts:
        payload["input"]["texts"] = texts
    
    return send_request(payload, timeout=3600)  # 1æ™‚é–“


def generate_text_layered(
    prompt: str,
    max_length: int = 100,
    temperature: float = 0.7,
    top_k: int = 40,
    top_p: float = 0.9,
    repetition_penalty: float = 2.0,
    train_before_generate: bool = False,
    data_sources: list = ["huggingface"],
    common_crawl_config: Optional[Dict] = None,
    max_records: int = 100,
    epochs: int = 20,
    **model_kwargs
) -> Dict[str, Any]:
    """Layeredãƒ¢ãƒ¼ãƒ‰ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    payload = {
        "input": {
            "action": "generate",
            "mode": "layered",
            "prompt": prompt,
            "max_length": max_length,
            "temperature": temperature,
            "top_k": top_k,
            "top_p": top_p,
            "repetition_penalty": repetition_penalty,
            "train_before_generate": train_before_generate,
            "data_sources": data_sources,
            "common_crawl_config": common_crawl_config or {},
            "max_records": max_records,
            "epochs": epochs,
            **model_kwargs
        }
    }
    
    timeout = 3600 if train_before_generate else 300
    return send_request(payload, timeout=timeout)


def generate_text_brain(
    prompt: str,
    max_length: int = 100,
    temperature: float = 0.7,
    top_k: int = 40,
    top_p: float = 0.9,
    train_before_generate: bool = False,
    data_sources: list = ["huggingface"],
    common_crawl_config: Optional[Dict] = None,
    max_records: int = 100,
    epochs: int = 20,
    **model_kwargs
) -> Dict[str, Any]:
    """Brainãƒ¢ãƒ¼ãƒ‰ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    payload = {
        "input": {
            "action": "generate",
            "mode": "brain",
            "prompt": prompt,
            "max_length": max_length,
            "temperature": temperature,
            "top_k": top_k,
            "top_p": top_p,
            "train_before_generate": train_before_generate,
            "data_sources": data_sources,
            "common_crawl_config": common_crawl_config or {},
            "max_records": max_records,
            "epochs": epochs,
            **model_kwargs
        }
    }
    
    timeout = 3600 if train_before_generate else 300
    return send_request(payload, timeout=timeout)


# ========================================
# ä½¿ç”¨ä¾‹
# ========================================

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - ä½¿ç”¨ä¾‹"""
    
    print("=" * 70)
    print("ğŸ§ âš›ï¸ ãƒ‹ãƒ¥ãƒ¼ãƒ­Q RunPod ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹")
    print("=" * 70)
    
    # 1. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    print("\n1. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯")
    print("-" * 70)
    health_result = health_check()
    print(json.dumps(health_result, indent=2, ensure_ascii=False))
    
    # 2. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆLayeredï¼‰
    print("\n2. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆLayeredï¼‰")
    print("-" * 70)
    init_result = init_model_layered(
        embed_dim=64,
        hidden_dim=128,
        num_heads=4,
        num_layers=2
    )
    print(json.dumps(init_result, indent=2, ensure_ascii=False))
    
    # 3. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆLayeredï¼‰- å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆ
    print("\n3. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆLayeredï¼‰")
    print("-" * 70)
    generate_result = generate_text_layered(
        prompt="ChatGPTã«ã¤ã„ã¦æ•™ãˆã¦",
        max_length=100,
        temperature=0.7,
        top_k=40,
        top_p=0.9,
        repetition_penalty=2.0
    )
    print(json.dumps(generate_result, indent=2, ensure_ascii=False))
    
    # 4. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆå­¦ç¿’ã‚‚åŒæ™‚ã«å®Ÿè¡Œï¼‰
    print("\n4. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆå­¦ç¿’ã‚‚åŒæ™‚ã«å®Ÿè¡Œï¼‰")
    print("-" * 70)
    generate_with_train_result = generate_text_layered(
        prompt="é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        max_length=150,
        temperature=0.7,
        repetition_penalty=2.0,
        train_before_generate=True,
        data_sources=["huggingface"],
        max_records=100,
        epochs=20
    )
    print(json.dumps(generate_with_train_result, indent=2, ensure_ascii=False))
    
    # 5. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆBrainï¼‰
    print("\n5. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆBrainï¼‰")
    print("-" * 70)
    init_brain_result = init_model_brain(
        embed_dim=128,
        num_neurons=100,
        max_vocab=16000
    )
    print(json.dumps(init_brain_result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    # ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
    if RUNPOD_API_KEY == "YOUR_API_KEY_HERE":
        print("âš ï¸ è­¦å‘Š: RUNPOD_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        print("   export RUNPOD_API_KEY='your_api_key'")
    
    if ENDPOINT_ID == "YOUR_ENDPOINT_ID_HERE":
        print("âš ï¸ è­¦å‘Š: RUNPOD_ENDPOINT_IDç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        print("   export RUNPOD_ENDPOINT_ID='your_endpoint_id'")
        print("\nä½¿ç”¨ä¾‹ã®ã¿ã‚’è¡¨ç¤ºã—ã¾ã™...")
        print("\n" + "=" * 70)
        print("ä½¿ç”¨ä¾‹:")
        print("=" * 70)
        print("\n1. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯:")
        print("""
result = health_check()
print(json.dumps(result, indent=2, ensure_ascii=False))
        """)
        print("\n2. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ:")
        print("""
result = generate_text_layered(
    prompt="ChatGPTã«ã¤ã„ã¦æ•™ãˆã¦",
    max_length=100,
    temperature=0.7,
    repetition_penalty=2.0
)
print(result.get("generated", ""))
        """)
        print("\n3. å­¦ç¿’ã—ã¦ã‹ã‚‰ç”Ÿæˆ:")
        print("""
result = generate_text_layered(
    prompt="é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    max_length=150,
    train_before_generate=True,
    data_sources=["huggingface"],
    max_records=100,
    epochs=20
)
print(result.get("generated", ""))
        """)
    else:
        main()

