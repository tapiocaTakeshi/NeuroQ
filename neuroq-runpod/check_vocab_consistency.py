#!/usr/bin/env python3
"""
NeuroQ vocab_size æ•´åˆæ€§ãƒã‚§ãƒƒã‚«ãƒ¼
================================
ãƒ¢ãƒ‡ãƒ«ã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®vocab_sizeãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
"""

import torch
import os
import sys

# neuroquantum_layered ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.insert(0, os.path.dirname(__file__))

try:
    from neuroquantum_layered import NeuroQuantumAI, NeuroQuantumTokenizer
    import sentencepiece as spm

    print("=" * 70)
    print("ğŸ” NeuroQ vocab_size æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")
    print("=" * 70)

    # 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®vocab_sizeã‚’ç¢ºèª
    print("\n1ï¸âƒ£ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®vocab_size:")
    print("-" * 70)

    tokenizer_paths = [
        "neuroq_tokenizer.model",
        "../neuroq_tokenizer.model",
        "neuroq_tokenizer_8k.model",
        "../neuroq_tokenizer_8k.model",
    ]

    tokenizer_vocab_size = None
    tokenizer_path = None

    for path in tokenizer_paths:
        if os.path.exists(path):
            try:
                sp = spm.SentencePieceProcessor()
                sp.load(path)
                tokenizer_vocab_size = sp.get_piece_size()
                tokenizer_path = path
                print(f"   âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«: {path}")
                print(f"   ğŸ“Š èªå½™ã‚µã‚¤ã‚º: {tokenizer_vocab_size:,}")
                break
            except Exception as e:
                print(f"   âŒ {path}: {e}")

    if tokenizer_vocab_size is None:
        print("   âŒ æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    # 2. NeuroQuantumAIã‚’åˆæœŸåŒ–ã—ã¦ç¢ºèª
    print("\n2ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«ã®vocab_size:")
    print("-" * 70)

    ai = NeuroQuantumAI(embed_dim=64, num_heads=2, num_layers=2)

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼ˆè»½é‡ï¼‰
    sample_texts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é©æ–°çš„ã§ã™ã€‚",
        "äººå·¥çŸ¥èƒ½ãŒæœªæ¥ã‚’å¤‰ãˆã¾ã™ã€‚",
    ] * 10

    ai.train(sample_texts, epochs=1, seq_len=16)

    # vocab_sizeã‚’ç¢ºèª
    print(f"   ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿéš›ã®vocab_size: {ai.tokenizer.actual_vocab_size:,}")
    print(f"   ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®vocab_size: {ai.tokenizer.vocab_size:,}")
    print(f"   ğŸ“Š ãƒ¢ãƒ‡ãƒ«config.vocab_size: {ai.config.vocab_size:,}")
    print(f"   ğŸ“Š Embeddingå±¤ã®num_embeddings: {ai.model.text_embedding.num_embeddings:,}")
    print(f"   ğŸ“Š LM Headå‡ºåŠ›æ¬¡å…ƒ: {ai.model.output_head.out_features:,}")

    # 3. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    print("\n3ï¸âƒ£ æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯:")
    print("-" * 70)

    vocab_sizes = {
        "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼(actual)": ai.tokenizer.actual_vocab_size,
        "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼(è¨­å®š)": ai.tokenizer.vocab_size,
        "ãƒ¢ãƒ‡ãƒ«Config": ai.config.vocab_size,
        "Embeddingå±¤": ai.model.text_embedding.num_embeddings,
        "LM Head": ai.model.output_head.out_features,
    }

    all_match = len(set(vocab_sizes.values())) == 1

    if all_match:
        print(f"   âœ… ã™ã¹ã¦ã®vocab_sizeãŒä¸€è‡´ã—ã¦ã„ã¾ã™: {list(vocab_sizes.values())[0]:,}")
    else:
        print("   âŒ vocab_sizeã«ä¸ä¸€è‡´ãŒã‚ã‚Šã¾ã™:")
        for name, size in vocab_sizes.items():
            print(f"      {name}: {size:,}")

    print("\n" + "=" * 70)
    if all_match:
        print("âœ… æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: åˆæ ¼")
    else:
        print("âŒ æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: ä¸åˆæ ¼")
    print("=" * 70)

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
