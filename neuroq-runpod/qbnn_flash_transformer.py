#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—                                        â•‘
â•‘  â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘                                        â•‘
â•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘                                        â•‘
â•‘  â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘                                        â•‘
â•‘  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                                        â•‘
â•‘   â•šâ•â•â–€â–€â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•  â•šâ•â•â•â•                                        â•‘
â•‘                                                                               â•‘
â•‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—                                     â•‘
â•‘  â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                                     â•‘
â•‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                                     â•‘
â•‘  â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘                                     â•‘
â•‘  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                                     â•‘
â•‘  â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•                                     â•‘
â•‘                                                                               â•‘
â•‘   QBNN Flash Transformer                                                      â•‘
â•‘   é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ + FlashAttention                              â•‘
â•‘                                                                               â•‘
â•‘   ç‰¹å¾´:                                                                       â•‘
â•‘   - FlashAttention: O(N)ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã€é«˜é€Ÿã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³                             â•‘
â•‘   - QBNN: é‡å­ã‚‚ã¤ã‚Œã«ã‚ˆã‚‹å±¤é–“ç›¸é–¢                                               â•‘
â•‘   - APQB: èª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆç†è«–                                             â•‘
â•‘   - é‡å­ä½ç›¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°                                                     â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Optional, Tuple, List
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("âš›ï¸âš¡ QBNN Flash Transformer")
print("   é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ + FlashAttention")
print("=" * 70)


# ========================================================================
# è¨­å®š
# ========================================================================

@dataclass
class QBNNFlashConfig:
    """QBNN Flash Transformer è¨­å®š"""
    vocab_size: int = 8000
    embed_dim: int = 256
    hidden_dim: int = 512
    num_heads: int = 8
    num_layers: int = 6
    max_seq_len: int = 512
    dropout: float = 0.1
    
    # QBNNå›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    lambda_min: float = 0.2       # é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ã®æœ€å°å€¤
    lambda_max: float = 0.5       # é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ã®æœ€å¤§å€¤
    use_quantum_phase: bool = True  # é‡å­ä½ç›¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    entangle_heads: bool = True     # ãƒ˜ãƒƒãƒ‰é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
    
    # FlashAttentionè¨­å®š
    use_flash_attention: bool = True
    attention_dropout: float = 0.0


# ========================================================================
# APQBï¼ˆèª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆï¼‰
# ========================================================================

class APQB(nn.Module):
    """
    Adjustable Pseudo Quantum Bit
    
    é‡å­ãƒ“ãƒƒãƒˆã®æ•°å­¦çš„æ€§è³ªã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡¨ç¾:
    - Î¸: å†…éƒ¨è§’åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - r = cos(2Î¸): ç›¸é–¢ä¿‚æ•° âˆˆ [-1, 1]
    - T = |sin(2Î¸)|: æ¸©åº¦ï¼ˆé‡å­ã‚†ã‚‰ãï¼‰âˆˆ [0, 1]
    - åˆ¶ç´„: rÂ² + TÂ² = 1
    """
    
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        # å­¦ç¿’å¯èƒ½ãªÎ¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.theta = nn.Parameter(torch.rand(dim) * math.pi / 2)
    
    def get_r(self, theta: Optional[torch.Tensor] = None) -> torch.Tensor:
        """ç›¸é–¢ä¿‚æ•° r = cos(2Î¸)"""
        t = theta if theta is not None else self.theta
        return torch.cos(2 * t)
    
    def get_T(self, theta: Optional[torch.Tensor] = None) -> torch.Tensor:
        """æ¸©åº¦ T = |sin(2Î¸)|"""
        t = theta if theta is not None else self.theta
        return torch.abs(torch.sin(2 * t))
    
    def get_quantum_state(self, theta: Optional[torch.Tensor] = None) -> torch.Tensor:
        """é‡å­çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« [cos(Î¸), sin(Î¸)]"""
        t = theta if theta is not None else self.theta
        return torch.stack([torch.cos(t), torch.sin(t)], dim=-1)
    
    def measure(self, theta: Optional[torch.Tensor] = None) -> torch.Tensor:
        """é‡å­æ¸¬å®šï¼ˆç¢ºç‡çš„ï¼‰"""
        t = theta if theta is not None else self.theta
        prob_1 = torch.sin(t) ** 2
        return (torch.rand_like(prob_1) < prob_1).float()
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å…¥åŠ›ã‚’APQBç©ºé–“ã«å¤‰æ›
        
        Returns:
            r: ç›¸é–¢ä¿‚æ•°
            T: æ¸©åº¦
        """
        # å…¥åŠ›ã‚’Î¸ã«å¤‰æ›
        theta = torch.sigmoid(x) * math.pi / 2
        return self.get_r(theta), self.get_T(theta)


# ========================================================================
# é‡å­ä½ç›¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
# ========================================================================

class QuantumPhaseEncoding(nn.Module):
    """
    é‡å­ä½ç›¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    
    ä½ç½®æƒ…å ±ã‚’é‡å­ä½ç›¸ã¨ã—ã¦è¡¨ç¾:
    - Ï†(pos) = 2Ï€ * pos / max_len
    - å›è»¢ã‚²ãƒ¼ãƒˆ R_z(Ï†) ã‚’æ¨¡å€£
    """
    
    def __init__(self, dim: int, max_seq_len: int = 512):
        super().__init__()
        self.dim = dim
        self.max_seq_len = max_seq_len
        
        # ä½ç›¸ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆäº‹å‰è¨ˆç®—ï¼‰
        position = torch.arange(max_seq_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))
        
        # é‡å­ä½ç›¸: cos(Ï†) ã¨ sin(Ï†)
        pe = torch.zeros(max_seq_len, dim)
        pe[:, 0::2] = torch.cos(position * div_term)  # å®Ÿéƒ¨
        pe[:, 1::2] = torch.sin(position * div_term)  # è™šéƒ¨
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
        # å­¦ç¿’å¯èƒ½ãªä½ç›¸ã‚·ãƒ•ãƒˆ
        self.phase_shift = nn.Parameter(torch.zeros(1, 1, dim))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, seq_len, dim)
        Returns:
            x + quantum_positional_encoding
        """
        seq_len = x.size(1)
        # é‡å­ä½ç›¸ã‚’åŠ ç®—
        phase = self.pe[:, :seq_len, :] + self.phase_shift
        return x + phase


# ========================================================================
# QBNNå±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
# ========================================================================

class QBNNEntanglement(nn.Module):
    """
    QBNNå±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
    
    éš£æ¥å±¤é–“ã®é‡å­ã‚‚ã¤ã‚Œã‚’è¡¨ç¾:
    - e^(l) = f(q^(l), q^(l-1))
    - Î»_eff âˆˆ [Î»_min, Î»_max] ã§å‹•çš„ã«å¤‰åŒ–
    """
    
    def __init__(self, dim: int, lambda_min: float = 0.2, lambda_max: float = 0.5):
        super().__init__()
        self.dim = dim
        self.lambda_min = lambda_min
        self.lambda_max = lambda_max
        
        # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«é‡ã¿
        self.W_entangle = nn.Linear(dim, dim, bias=False)
        
        # ä½ç›¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.phase = nn.Parameter(torch.rand(dim) * math.pi / 2)
        
        # Î»ãƒ™ãƒ¼ã‚¹ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.lambda_base = nn.Parameter(torch.tensor(0.5))
        
        # å‘¼ã³å‡ºã—ã‚«ã‚¦ãƒ³ã‚¿
        self.register_buffer('call_count', torch.tensor(0))
    
    def get_lambda_eff(self) -> float:
        """å‹•çš„Î»ã‚’è¨ˆç®—"""
        # Î»ã‚’[0,1]ã«æ­£è¦åŒ–
        lambda_norm = torch.sigmoid(self.lambda_base)
        # [lambda_min, lambda_max]ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        lambda_eff = self.lambda_min + (self.lambda_max - self.lambda_min) * lambda_norm
        
        # é‡å­ã‚†ã‚‰ãã‚’è¿½åŠ 
        if self.training:
            noise = torch.randn(1, device=lambda_norm.device) * 0.01
            lambda_eff = lambda_eff + noise
            lambda_eff = torch.clamp(lambda_eff, self.lambda_min, self.lambda_max)
        
        return lambda_eff
    
    def forward(self, h_current: torch.Tensor, h_prev: torch.Tensor) -> torch.Tensor:
        """
        å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«è¨ˆç®—
        
        Args:
            h_current: ç¾åœ¨ã®å±¤ã®çŠ¶æ…‹ (batch, seq, dim)
            h_prev: å‰ã®å±¤ã®çŠ¶æ…‹ (batch, seq, dim)
        
        Returns:
            ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆè£œæ­£å¾Œã®çŠ¶æ…‹
        """
        self.call_count += 1
        
        # 1. æ­£è¦åŒ–ï¼ˆBlochçƒåº§æ¨™ï¼‰
        s_current = torch.tanh(h_current)
        s_prev = torch.tanh(h_prev)
        
        # 2. ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ç›¸äº’ä½œç”¨
        entangle_signal = self.W_entangle(s_prev)
        
        # 3. ä½ç›¸å›è»¢
        phase_factor = torch.cos(self.phase)
        
        # 4. ç›¸é–¢è¨ˆç®—
        correlation = s_current * entangle_signal * phase_factor
        
        # 5. å‹•çš„Î»ã§é‡ã¿ä»˜ã‘
        lambda_eff = self.get_lambda_eff()
        delta = lambda_eff * correlation
        
        return h_current + delta


# ========================================================================
# QBNN Flash Attention
# ========================================================================

class QBNNFlashAttention(nn.Module):
    """
    QBNN Flash Attention
    
    FlashAttention + é‡å­ã‚‚ã¤ã‚Œãƒ˜ãƒƒãƒ‰é–“ç›¸é–¢:
    - FlashAttention: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡O(N)ã€é«˜é€Ÿè¨ˆç®—
    - QBNN: ãƒ˜ãƒƒãƒ‰é–“ã®é‡å­ã‚‚ã¤ã‚Œç›¸é–¢
    - é‡å­ä½ç›¸: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã¸ã®ä½ç›¸å¤‰èª¿
    """
    
    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.0,
        lambda_min: float = 0.2,
        lambda_max: float = 0.5,
        entangle_heads: bool = True
    ):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.dropout = dropout
        self.entangle_heads = entangle_heads
        
        # Q, K, V æŠ•å½±
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        # é‡å­ä½ç›¸å¤‰èª¿
        self.quantum_phase = nn.Parameter(torch.rand(num_heads) * math.pi)
        
        # ãƒ˜ãƒƒãƒ‰é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
        if entangle_heads and num_heads > 1:
            self.head_entangle = nn.Parameter(
                torch.randn(num_heads, num_heads) * 0.02
            )
            self.lambda_heads = nn.Parameter(torch.tensor(0.3))
        
        # APQB for attention modulation
        self.apqb = APQB(num_heads)
    
    def _reshape_for_attention(self, x: torch.Tensor) -> torch.Tensor:
        """(batch, seq, embed) -> (batch, heads, seq, head_dim)"""
        batch, seq, _ = x.shape
        x = x.view(batch, seq, self.num_heads, self.head_dim)
        return x.transpose(1, 2)
    
    def _apply_quantum_phase(self, attn_weights: torch.Tensor) -> torch.Tensor:
        """ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã«é‡å­ä½ç›¸å¤‰èª¿ã‚’é©ç”¨"""
        # attn_weights: (batch, heads, seq, seq)
        phase_modulation = torch.cos(self.quantum_phase).view(1, -1, 1, 1)
        return attn_weights * phase_modulation
    
    def _apply_head_entanglement(self, x: torch.Tensor) -> torch.Tensor:
        """ãƒ˜ãƒƒãƒ‰é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã‚’é©ç”¨"""
        if not self.entangle_heads or self.num_heads == 1:
            return x
        
        # x: (batch, heads, seq, head_dim)
        batch, heads, seq, head_dim = x.shape
        
        # ãƒ˜ãƒƒãƒ‰é–“ç›¸é–¢
        # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆè¡Œåˆ—ã‚’æ­£è¦åŒ–
        entangle_matrix = torch.softmax(self.head_entangle, dim=-1)
        
        # ãƒ˜ãƒƒãƒ‰æ··åˆ: å„ãƒ˜ãƒƒãƒ‰ã«ä»–ã®ãƒ˜ãƒƒãƒ‰ã®æƒ…å ±ã‚’æ³¨å…¥
        x_flat = x.permute(0, 2, 3, 1)  # (batch, seq, head_dim, heads)
        x_entangled = torch.matmul(x_flat, entangle_matrix.t())  # (batch, seq, head_dim, heads)
        x_entangled = x_entangled.permute(0, 3, 1, 2)  # (batch, heads, seq, head_dim)
        
        # Î»ã§é‡ã¿ä»˜ã‘
        lambda_h = torch.sigmoid(self.lambda_heads)
        return x + lambda_h * (x_entangled - x)
    
    def forward(
        self,
        x: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None,
        is_causal: bool = True
    ) -> torch.Tensor:
        """
        Forward pass with FlashAttention + QBNN
        
        Args:
            x: (batch, seq_len, embed_dim)
            attn_mask: optional attention mask
            is_causal: use causal masking
        
        Returns:
            output: (batch, seq_len, embed_dim)
        """
        batch_size, seq_len, _ = x.shape
        
        # Q, K, V æŠ•å½±
        q = self._reshape_for_attention(self.q_proj(x))
        k = self._reshape_for_attention(self.k_proj(x))
        v = self._reshape_for_attention(self.v_proj(x))
        
        # ãƒ˜ãƒƒãƒ‰é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆï¼ˆQ, K, Vã«é©ç”¨ï¼‰
        q = self._apply_head_entanglement(q)
        k = self._apply_head_entanglement(k)
        
        # FlashAttention (PyTorch 2.0+)
        # scaled_dot_product_attention ã¯è‡ªå‹•çš„ã«FlashAttentionã‚’ä½¿ç”¨
        try:
            # is_causal=True ã§å› æœãƒã‚¹ã‚­ãƒ³ã‚°
            attn_output = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=attn_mask,
                dropout_p=self.dropout if self.training else 0.0,
                is_causal=is_causal
            )
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‰‹å‹•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—
            attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale
            
            # é‡å­ä½ç›¸å¤‰èª¿
            attn_weights = self._apply_quantum_phase(attn_weights)
            
            if is_causal:
                causal_mask = torch.triu(
                    torch.ones(seq_len, seq_len, device=x.device, dtype=torch.bool),
                    diagonal=1
                )
                attn_weights = attn_weights.masked_fill(causal_mask, float('-inf'))
            
            if attn_mask is not None:
                attn_weights = attn_weights + attn_mask
            
            attn_weights = F.softmax(attn_weights, dim=-1)
            attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)
            attn_output = torch.matmul(attn_weights, v)
        
        # (batch, heads, seq, head_dim) -> (batch, seq, embed)
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.embed_dim)
        
        return self.out_proj(attn_output)


# ========================================================================
# QBNN Feed Forward
# ========================================================================

class QBNNFeedForward(nn.Module):
    """
    QBNN Feed Forward Network
    
    é‡å­ã‚‚ã¤ã‚Œã‚’çµ„ã¿è¾¼ã‚“ã FFN:
    - SwiGLUæ´»æ€§åŒ–ï¼ˆLLaMA styleï¼‰
    - QBNNè£œæ­£é …
    """
    
    def __init__(
        self,
        embed_dim: int,
        hidden_dim: int,
        dropout: float = 0.1,
        lambda_min: float = 0.2,
        lambda_max: float = 0.5
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        
        # SwiGLU: gate * silu(x)
        self.w1 = nn.Linear(embed_dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, embed_dim, bias=False)
        self.w3 = nn.Linear(embed_dim, hidden_dim, bias=False)
        
        self.dropout = nn.Dropout(dropout)
        
        # QBNNè£œæ­£
        self.qbnn_entangle = QBNNEntanglement(hidden_dim, lambda_min, lambda_max)
        
        # éš ã‚ŒçŠ¶æ…‹ã‚’ä¿æŒ
        self.register_buffer('prev_hidden', None)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        SwiGLU + QBNN
        """
        # SwiGLU: silu(w1(x)) * w3(x)
        hidden = F.silu(self.w1(x)) * self.w3(x)
        
        # QBNNè£œæ­£ï¼ˆå‰ã®éš ã‚ŒçŠ¶æ…‹ãŒã‚ã‚Œã°ï¼‰
        if self.prev_hidden is not None and self.prev_hidden.shape == hidden.shape:
            hidden = self.qbnn_entangle(hidden, self.prev_hidden)
        
        # ç¾åœ¨ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä¿å­˜
        self.prev_hidden = hidden.detach()
        
        output = self.w2(hidden)
        return self.dropout(output)


# ========================================================================
# QBNN Transformer Block
# ========================================================================

class QBNNTransformerBlock(nn.Module):
    """
    QBNN Transformer Block
    
    æ§‹æˆ:
    1. QBNN Flash Attention (Pre-Norm)
    2. QBNN Feed Forward (Pre-Norm)
    3. å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
    """
    
    def __init__(self, config: QBNNFlashConfig, layer_idx: int = 0):
        super().__init__()
        self.layer_idx = layer_idx
        
        # Layer Normalization (Pre-Norm style)
        self.attn_norm = nn.LayerNorm(config.embed_dim)
        self.ffn_norm = nn.LayerNorm(config.embed_dim)
        
        # QBNN Flash Attention
        self.attention = QBNNFlashAttention(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            dropout=config.attention_dropout,
            lambda_min=config.lambda_min,
            lambda_max=config.lambda_max,
            entangle_heads=config.entangle_heads
        )
        
        # QBNN Feed Forward
        self.ffn = QBNNFeedForward(
            embed_dim=config.embed_dim,
            hidden_dim=config.hidden_dim,
            dropout=config.dropout,
            lambda_min=config.lambda_min,
            lambda_max=config.lambda_max
        )
        
        # å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
        self.layer_entangle = QBNNEntanglement(
            config.embed_dim,
            config.lambda_min,
            config.lambda_max
        )
        
        # å‰å±¤ã®å‡ºåŠ›ã‚’ä¿æŒ
        self.register_buffer('prev_layer_output', None)
    
    def forward(
        self,
        x: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None,
        is_causal: bool = True,
        prev_layer_output: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            x: (batch, seq_len, embed_dim)
            attn_mask: attention mask
            is_causal: use causal masking
            prev_layer_output: å‰å±¤ã®å‡ºåŠ›ï¼ˆå±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ç”¨ï¼‰
        
        Returns:
            output: (batch, seq_len, embed_dim)
        """
        # å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
        if prev_layer_output is not None:
            x = self.layer_entangle(x, prev_layer_output)
        
        # Self-Attention (Pre-Norm + Residual)
        residual = x
        x = self.attn_norm(x)
        x = self.attention(x, attn_mask=attn_mask, is_causal=is_causal)
        x = residual + x
        
        # FFN (Pre-Norm + Residual)
        residual = x
        x = self.ffn_norm(x)
        x = self.ffn(x)
        x = residual + x
        
        return x


# ========================================================================
# QBNN Flash Transformer (å®Œå…¨ãƒ¢ãƒ‡ãƒ«)
# ========================================================================

class QBNNFlashTransformer(nn.Module):
    """
    QBNN Flash Transformer
    
    å®Œå…¨ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«:
    - Token Embedding + Quantum Phase Encoding
    - N x QBNN Transformer Blocks
    - Output Head
    """
    
    def __init__(self, config: QBNNFlashConfig):
        super().__init__()
        self.config = config
        
        # Token Embedding
        self.token_embedding = nn.Embedding(config.vocab_size, config.embed_dim)
        
        # Quantum Phase Position Encoding
        if config.use_quantum_phase:
            self.pos_encoding = QuantumPhaseEncoding(config.embed_dim, config.max_seq_len)
        else:
            self.pos_encoding = nn.Embedding(config.max_seq_len, config.embed_dim)
        
        # Embedding Dropout
        self.embed_dropout = nn.Dropout(config.dropout)
        
        # Transformer Blocks
        self.layers = nn.ModuleList([
            QBNNTransformerBlock(config, layer_idx=i)
            for i in range(config.num_layers)
        ])
        
        # Output
        self.final_norm = nn.LayerNorm(config.embed_dim)
        self.output_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)
        
        # Weight tying (embedding and output)
        self.output_head.weight = self.token_embedding.weight
        
        # Initialize weights
        self.apply(self._init_weights)
        
        # çµ±è¨ˆ
        self._print_model_info()
    
    def _init_weights(self, module: nn.Module):
        """é‡ã¿åˆæœŸåŒ–"""
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def _print_model_info(self):
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f"\nğŸ“Š QBNN Flash Transformer æ§‹æˆ:")
        print(f"   èªå½™ã‚µã‚¤ã‚º: {self.config.vocab_size:,}")
        print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {self.config.embed_dim}")
        print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {self.config.hidden_dim}")
        print(f"   ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰: {self.config.num_heads}")
        print(f"   ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å±¤: {self.config.num_layers}")
        print(f"   é‡å­ã‚‚ã¤ã‚Œå¼·åº¦: [{self.config.lambda_min}, {self.config.lambda_max}]")
        print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        print(f"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params:,}")
        print()
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None,
        is_causal: bool = True,
        return_hidden: bool = False
    ) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            input_ids: (batch, seq_len) token IDs
            attn_mask: optional attention mask
            is_causal: use causal masking (default: True for language modeling)
            return_hidden: return hidden states instead of logits
        
        Returns:
            logits: (batch, seq_len, vocab_size)
        """
        batch_size, seq_len = input_ids.shape
        
        # Token embedding
        x = self.token_embedding(input_ids)
        
        # Position encoding
        if self.config.use_quantum_phase:
            x = self.pos_encoding(x)
        else:
            positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
            x = x + self.pos_encoding(positions)
        
        x = self.embed_dropout(x)
        
        # Transformer layers with inter-layer entanglement
        prev_output = None
        for layer in self.layers:
            x = layer(x, attn_mask=attn_mask, is_causal=is_causal, prev_layer_output=prev_output)
            prev_output = x.detach()  # å‹¾é…ã‚’åˆ‡æ–­ã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„
        
        # Output
        x = self.final_norm(x)
        
        if return_hidden:
            return x
        
        logits = self.output_head(x)
        return logits
    
    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.Tensor,
        max_new_tokens: int = 100,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.9,
        repetition_penalty: float = 1.1
    ) -> torch.Tensor:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            input_ids: (batch, seq_len) å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ID
            max_new_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
            top_k: Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            top_p: Top-P (Nucleus) ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            repetition_penalty: ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        Returns:
            generated_ids: (batch, seq_len + max_new_tokens)
        """
        self.eval()
        device = input_ids.device
        
        generated = input_ids.clone()
        
        for _ in range(max_new_tokens):
            # æœ€å¤§é•·ã‚’è¶…ãˆãªã„ã‚ˆã†ã«
            if generated.size(1) >= self.config.max_seq_len:
                # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
                context = generated[:, -self.config.max_seq_len:]
            else:
                context = generated
            
            # Forward
            logits = self.forward(context, is_causal=True)
            next_logits = logits[:, -1, :] / temperature
            
            # Repetition penalty
            if repetition_penalty != 1.0:
                for i in range(generated.size(0)):
                    for token_id in set(generated[i].tolist()):
                        next_logits[i, token_id] /= repetition_penalty
            
            # Top-K filtering
            if top_k > 0:
                indices_to_remove = next_logits < torch.topk(next_logits, top_k)[0][..., -1, None]
                next_logits[indices_to_remove] = float('-inf')
            
            # Top-P (Nucleus) filtering
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                indices_to_remove = sorted_indices_to_remove.scatter(
                    1, sorted_indices, sorted_indices_to_remove
                )
                next_logits[indices_to_remove] = float('-inf')
            
            # Sample
            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # Append
            generated = torch.cat([generated, next_token], dim=1)
            
            # EOS check (assuming token_id=1 is EOS)
            if (next_token == 1).all():
                break
        
        return generated


# ========================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================================================

def create_qbnn_flash_transformer(
    vocab_size: int = 8000,
    embed_dim: int = 256,
    hidden_dim: int = 512,
    num_heads: int = 8,
    num_layers: int = 6,
    max_seq_len: int = 512,
    **kwargs
) -> QBNNFlashTransformer:
    """
    QBNN Flash Transformerã‚’ä½œæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    """
    config = QBNNFlashConfig(
        vocab_size=vocab_size,
        embed_dim=embed_dim,
        hidden_dim=hidden_dim,
        num_heads=num_heads,
        num_layers=num_layers,
        max_seq_len=max_seq_len,
        **kwargs
    )
    return QBNNFlashTransformer(config)


def test_model():
    """ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°"""
    print("\nğŸ§ª QBNN Flash Transformer ãƒ†ã‚¹ãƒˆ\n")
    
    # å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    config = QBNNFlashConfig(
        vocab_size=1000,
        embed_dim=64,
        hidden_dim=128,
        num_heads=4,
        num_layers=2,
        max_seq_len=128
    )
    
    model = QBNNFlashTransformer(config)
    
    # ãƒ†ã‚¹ãƒˆå…¥åŠ›
    batch_size = 2
    seq_len = 32
    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    
    # Forward pass
    print("ğŸ“Š Forward pass ãƒ†ã‚¹ãƒˆ...")
    logits = model(input_ids)
    print(f"   å…¥åŠ›å½¢çŠ¶: {input_ids.shape}")
    print(f"   å‡ºåŠ›å½¢çŠ¶: {logits.shape}")
    print(f"   æœŸå¾…å½¢çŠ¶: ({batch_size}, {seq_len}, {config.vocab_size})")
    
    assert logits.shape == (batch_size, seq_len, config.vocab_size), "å‡ºåŠ›å½¢çŠ¶ãŒä¸æ­£ã§ã™"
    print("   âœ… Forward pass æˆåŠŸ!\n")
    
    # Generate test
    print("ğŸ“Š Generate ãƒ†ã‚¹ãƒˆ...")
    generated = model.generate(input_ids[:1, :5], max_new_tokens=10)
    print(f"   å…¥åŠ›é•·: 5")
    print(f"   ç”Ÿæˆå¾Œé•·: {generated.shape[1]}")
    print("   âœ… Generate æˆåŠŸ!\n")
    
    print("=" * 70)
    print("âœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã«æˆåŠŸã—ã¾ã—ãŸ!")
    print("=" * 70)
    
    return model


# ========================================================================
# ãƒ¡ã‚¤ãƒ³
# ========================================================================

if __name__ == "__main__":
    test_model()
