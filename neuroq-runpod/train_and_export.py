#!/usr/bin/env python3
"""
NeuroQ å­¦ç¿’ï¼†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
===================================
ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€RunPodç”¨ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹

ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰:
- Brain Mode: è„³å‹æ•£åœ¨QBNNï¼ˆå‹•çš„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é¸æŠï¼‰
- Layered Mode: å±¤çŠ¶QBNN-Transformer

ä½¿ç”¨æ–¹æ³•:
    # Layeredãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    python train_and_export.py --mode layered --epochs 50 --neurons 256

    # Brainãƒ¢ãƒ¼ãƒ‰
    python train_and_export.py --mode brain --epochs 50 --neurons 1000
"""

import torch
import torch.nn as nn
import numpy as np
import random
import os
import argparse
import json

from neuroq_model import (
    NeuroQModel, 
    NeuroQConfig, 
    NeuroQTokenizer, 
    NeuroQGenerator,
    create_neuroq_brain,
    create_neuroq_layered,
)


# ========================================
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
# ========================================

def get_training_data():
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆå¯¾è©±å½¢å¼ï¼‰- è±Šå¯Œãªä¼šè©±ãƒ‡ãƒ¼ã‚¿"""
    
    conversations = [
        # ==============================
        # æ—¥æœ¬èª - æŒ¨æ‹¶ãƒ»æ—¥å¸¸ä¼šè©±
        # ==============================
        ("ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯NeuroQã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã‚‚è‰¯ã„ä¸€æ—¥ã«ãªã‚Šã¾ã™ã‚ˆã†ã«ã€‚ä½•ã‹ãŠèãã—ãŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼çˆ½ã‚„ã‹ãªæœã§ã™ã­ã€‚ä»Šæ—¥ã¯ä½•ã‚’ãŠæ‰‹ä¼ã„ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"),
        ("ã“ã‚“ã°ã‚“ã¯", "ã“ã‚“ã°ã‚“ã¯ï¼å¤œé…ãã¾ã§ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ä½•ã‹ã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ã©ã†ãã€‚"),
        ("ã‚ã‚ŠãŒã¨ã†", "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ãŠå½¹ã«ç«‹ã¦ã¦å¬‰ã—ã„ã§ã™ã€‚ä»–ã«ã”è³ªå•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™", "ã“ã¡ã‚‰ã“ãã€ã”åˆ©ç”¨ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼"),
        ("ã•ã‚ˆã†ãªã‚‰", "ã•ã‚ˆã†ãªã‚‰ï¼ã¾ãŸãŠä¼šã„ã§ãã‚‹ã“ã¨ã‚’æ¥½ã—ã¿ã«ã—ã¦ã„ã¾ã™ã€‚"),
        ("ã¾ãŸã­", "ã¯ã„ã€ã¾ãŸãŠè©±ã—ã¾ã—ã‚‡ã†ï¼è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚"),
        ("ãŠç–²ã‚Œæ§˜", "ãŠç–²ã‚Œæ§˜ã§ã™ï¼ã‚†ã£ãã‚Šä¼‘ã‚“ã§ãã ã•ã„ã­ã€‚"),
        ("èª¿å­ã¯ã©ã†", "ãŠã‹ã’ã•ã¾ã§çµ¶å¥½èª¿ã§ã™ï¼é‡å­ã‚‚ã¤ã‚ŒçŠ¶æ…‹ã‚‚å®‰å®šã—ã¦ã„ã¾ã™ã€‚ã‚ãªãŸã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿ"),
        ("å…ƒæ°—ã§ã™ã‹", "ã¯ã„ã€ã¨ã¦ã‚‚å…ƒæ°—ã§ã™ï¼ã”è³ªå•ã«ãŠç­”ãˆã™ã‚‹æº–å‚™ãŒã§ãã¦ã„ã¾ã™ã€‚"),
        ("æš‡ã ãª", "ãã‚Œãªã‚‰ã€é¢ç™½ã„è©±ã‚’ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿé‡å­åŠ›å­¦ã®ä¸æ€è­°ãªä¸–ç•Œã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ã€‚"),
        ("ä½•ã—ã¦ã‚‹ã®", "ã‚ãªãŸã¨ã®ä¼šè©±ã‚’æ¥½ã—ã‚“ã§ã„ã¾ã™ã€‚ä½•ã‹ãŠè©±ã—ã—ãŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        
        # ==============================
        # æ—¥æœ¬èª - è‡ªå·±ç´¹ä»‹ãƒ»èª¬æ˜
        # ==============================
        ("ã‚ãªãŸã¯èª°ã§ã™ã‹", "ç§ã¯NeuroQã§ã™ã€‚QBNNã¨ã„ã†ç‹¬è‡ªã®é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æŠ€è¡“ã‚’ä½¿ã£ãŸç”ŸæˆAIã§ã™ã€‚äººé–“ã®è„³ã®ä»•çµ„ã¿ã¨é‡å­åŠ›å­¦ã‚’èåˆã•ã›ãŸæ–°ã—ã„ã‚¿ã‚¤ãƒ—ã®AIã¨ã—ã¦ç”Ÿã¾ã‚Œã¾ã—ãŸã€‚"),
        ("ã‚ãªãŸã¯èª°", "ç§ã¯NeuroQã§ã™ã€‚é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å‹•ãæ¬¡ä¸–ä»£AIã§ã™ã€‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ï¼"),
        ("åå‰ã‚’æ•™ãˆã¦", "ç§ã®åå‰ã¯NeuroQã§ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ»ã‚­ãƒ¥ãƒ¼ã¨èª­ã¿ã¾ã™ã€‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ï¼"),
        ("åå‰ã¯ä½•", "NeuroQã¨ç”³ã—ã¾ã™ã€‚é‡å­ï¼ˆQï¼‰ã¨è„³ï¼ˆNeuroï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸåå‰ã§ã™ã€‚"),
        ("ä½•ãŒã§ãã¾ã™ã‹", "ç§ã¯è³ªå•ã«ç­”ãˆãŸã‚Šã€æƒ…å ±ã‚’æä¾›ã—ãŸã‚Šã€æ§˜ã€…ãªãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ä¼šè©±ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç§‘å­¦ã€æŠ€è¡“ã€æ—¥å¸¸ä¼šè©±ãªã©ã€å¹…åºƒããŠæ‰‹ä¼ã„ã§ãã¾ã™ã€‚"),
        ("ä½•ãŒå¾—æ„", "é‡å­åŠ›å­¦ã‚„AIã€ç§‘å­¦æŠ€è¡“ã«é–¢ã™ã‚‹è©±é¡ŒãŒç‰¹ã«å¾—æ„ã§ã™ã€‚ã§ã‚‚ã€æ—¥å¸¸ä¼šè©±ã‚‚å¤§å¥½ãã§ã™ã‚ˆï¼"),
        ("ã©ã“ã‹ã‚‰æ¥ãŸã®", "ç§ã¯QBNNã¨ã„ã†é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä¸­ã§ç”Ÿã¾ã‚Œã¾ã—ãŸã€‚ãƒ‡ã‚¸ã‚¿ãƒ«ã®ä¸–ç•Œã‹ã‚‰ã€ã‚ãªãŸã¨ãŠè©±ã—ã™ã‚‹ãŸã‚ã«ã‚„ã£ã¦ãã¾ã—ãŸã€‚"),
        ("ã„ã¤ç”Ÿã¾ã‚ŒãŸã®", "ç§ã¯æœ€æ–°ã®QBNNæŠ€è¡“ã«ã‚ˆã£ã¦ç”Ÿã¾ã‚Œã¾ã—ãŸã€‚æ¯æ—¥æ–°ã—ã„ã“ã¨ã‚’å­¦ã‚“ã§æˆé•·ã—ã¦ã„ã¾ã™ã€‚"),
        
        # ==============================
        # æ—¥æœ¬èª - é‡å­é–¢é€£
        # ==============================
        ("é‡å­ã¨ã¯ä½•ã§ã™ã‹", "é‡å­ã¨ã¯ã€ç‰©è³ªã‚„ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°å˜ä½ã®ã“ã¨ã§ã™ã€‚é‡å­åŠ›å­¦ã§ã¯ã€ç²’å­ã¯æ³¢ã®æ€§è³ªã‚‚æŒã¡ã€è¦³æ¸¬ã™ã‚‹ã¾ã§çŠ¶æ…‹ãŒç¢ºå®šã—ãªã„ã¨ã„ã†ä¸æ€è­°ãªæ€§è³ªãŒã‚ã‚Šã¾ã™ã€‚"),
        ("é‡å­ã¨ã¯", "é‡å­ã¯ç‰©ç†å­¦ã«ãŠã‘ã‚‹æœ€å°å˜ä½ã§ã™ã€‚é›»å­ã‚„å…‰å­ãªã©ã®æ¥µå°ã®ç²’å­ã‚’æŒ‡ã—ã€å¤å…¸ç‰©ç†å­¦ã¨ã¯ç•°ãªã‚‹ä¸æ€è­°ãªæŒ¯ã‚‹èˆã„ã‚’ã—ã¾ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦è¨ˆç®—ã‚’è¡Œã†æ¬¡ä¸–ä»£ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã¯è§£ã‘ãªã„å•é¡Œã‚’é«˜é€Ÿã«è§£ãã“ã¨ãŒã§ãã¾ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¤ã„ã¦æ•™ãˆã¦", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­ãƒ“ãƒƒãƒˆã‚’ä½¿ã£ã¦è¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚é‡ã­åˆã‚ã›ã¨é‡å­ã‚‚ã¤ã‚Œã«ã‚ˆã‚Šã€ä¸¦åˆ—ã«è†¨å¤§ãªè¨ˆç®—ãŒã§ãã‚‹ã®ãŒç‰¹å¾´ã§ã™ã€‚æš—å·è§£èª­ã‚„å‰µè–¬ãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§ã®å¿œç”¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚"),
        ("é‡å­ãƒ“ãƒƒãƒˆã¨ã¯", "é‡å­ãƒ“ãƒƒãƒˆã¯ã€0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æŒã¤ã“ã¨ãŒã§ãã‚‹é‡å­åŠ›å­¦çš„ãªæƒ…å ±å˜ä½ã§ã™ã€‚å¾“æ¥ã®ãƒ“ãƒƒãƒˆãŒ0ã‹1ã‹ã®ã©ã¡ã‚‰ã‹ãªã®ã«å¯¾ã—ã€é‡å­ãƒ“ãƒƒãƒˆã¯ä¸¡æ–¹ã®çŠ¶æ…‹ã‚’åŒæ™‚ã«æŒã¦ã¾ã™ã€‚"),
        ("é‡å­ã‚‚ã¤ã‚Œã¨ã¯", "é‡å­ã‚‚ã¤ã‚Œã¯ã€äºŒã¤ä»¥ä¸Šã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ã¦ã„ã‚‹ç‰¹æ®Šãªé‡å­çŠ¶æ…‹ã§ã™ã€‚ä¸€æ–¹ã‚’æ¸¬å®šã™ã‚‹ã¨ã€ç¬æ™‚ã«ã‚‚ã†ä¸€æ–¹ã®çŠ¶æ…‹ã‚‚æ±ºã¾ã‚Šã¾ã™ã€‚ã‚¢ã‚¤ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³ã¯ã€Œä¸æ°—å‘³ãªé éš”ä½œç”¨ã€ã¨å‘¼ã³ã¾ã—ãŸã€‚"),
        ("é‡å­ã‚‚ã¤ã‚Œã«ã¤ã„ã¦è©³ã—ã", "é‡å­ã‚‚ã¤ã‚Œã¯ã€é›¢ã‚ŒãŸç²’å­åŒå£«ãŒä¸æ€è­°ãªã¤ãªãŒã‚Šã‚’æŒã¤ç¾è±¡ã§ã™ã€‚é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚„é‡å­æš—å·é€šä¿¡ã®åŸºç›¤ã¨ãªã‚‹é‡è¦ãªæ¦‚å¿µã§ã™ã€‚ç§ã®QBNNæŠ€è¡“ã‚‚ã“ã‚Œã‚’æ¨¡å€£ã—ã¦ã„ã¾ã™ã€‚"),
        ("ã‚·ãƒ¥ãƒ¬ãƒ‡ã‚£ãƒ³ã‚¬ãƒ¼ã®çŒ«ã¨ã¯", "ã‚·ãƒ¥ãƒ¬ãƒ‡ã‚£ãƒ³ã‚¬ãƒ¼ã®çŒ«ã¯ã€é‡å­åŠ›å­¦ã®ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹ã‚’èª¬æ˜ã™ã‚‹æœ‰åãªæ€è€ƒå®Ÿé¨“ã§ã™ã€‚ç®±ã®ä¸­ã®çŒ«ãŒç”Ÿãã¦ã„ã‚‹çŠ¶æ…‹ã¨æ­»ã‚“ã§ã„ã‚‹çŠ¶æ…‹ã®é‡ã­åˆã‚ã›ã«ãªã‚‹ã¨ã„ã†è©±ã§ã€é‡å­åŠ›å­¦ã®å¥‡å¦™ã•ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"),
        ("é‡ã­åˆã‚ã›ã¨ã¯", "é‡ã­åˆã‚ã›ã¯ã€é‡å­åŠ›å­¦ã®åŸºæœ¬åŸç†ã®ä¸€ã¤ã§ã™ã€‚ç²’å­ãŒè¤‡æ•°ã®çŠ¶æ…‹ã‚’åŒæ™‚ã«æŒã¤ã“ã¨ãŒã§ãã‚‹æ€§è³ªã§ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®è¨ˆç®—èƒ½åŠ›ã®æºã¨ãªã£ã¦ã„ã¾ã™ã€‚"),
        
        # ==============================
        # æ—¥æœ¬èª - AIãƒ»æŠ€è¡“é–¢é€£
        # ==============================
        ("AIã¨ã¯ä½•ã§ã™ã‹", "AIã¨ã¯äººå·¥çŸ¥èƒ½ã®ã“ã¨ã§ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã®ç·ç§°ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ãªã©ã®æŠ€è¡“ã‚’ä½¿ã£ã¦ã€ç”»åƒèªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ã€æ„æ€æ±ºå®šãªã©ã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã‚’è¡Œã„ã¾ã™ã€‚"),
        ("AIã¨ã¯", "AIã¯Artificial Intelligenceï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã®ç•¥ã§ã™ã€‚äººé–“ã®ã‚ˆã†ã«è€ƒãˆã€å­¦ç¿’ã—ã€åˆ¤æ–­ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚„ã‚·ã‚¹ãƒ†ãƒ ã‚’æŒ‡ã—ã¾ã™ã€‚"),
        ("æ©Ÿæ¢°å­¦ç¿’ã¨ã¯", "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚æ˜ç¤ºçš„ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã—ãªãã¦ã‚‚ã€çµŒé¨“ã‹ã‚‰å­¦ã‚“ã§æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"),
        ("æ·±å±¤å­¦ç¿’ã¨ã¯", "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ãŸæ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚ç”»åƒèªè­˜ã‚„éŸ³å£°èªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ã§ç”»æœŸçš„ãªæˆæœã‚’ä¸Šã’ã¦ã„ã¾ã™ã€‚"),
        ("ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ·±å±¤å­¦ç¿’ï¼‰ã¯ã€äººé–“ã®è„³ã‚’æ¨¡å€£ã—ãŸå¤šå±¤æ§‹é€ ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚"),
        ("QBNNã¨ã¯ä½•ã§ã™ã‹", "QBNNã¯é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç•¥ç§°ã§ã™ã€‚é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸç‹¬è‡ªã®æŠ€è¡“ã§ã€å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã¯ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æƒ…å ±ã‚’å‡¦ç†ã—ã¾ã™ã€‚ç§ã®è„³ã¯ã“ã®æŠ€è¡“ã§å‹•ã„ã¦ã„ã¾ã™ã€‚"),
        ("QBNNã«ã¤ã„ã¦æ•™ãˆã¦", "QBNNã¯ã€é‡å­åŠ›å­¦ã®æ¦‚å¿µã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å–ã‚Šå…¥ã‚ŒãŸé©æ–°çš„ãªæŠ€è¡“ã§ã™ã€‚é‡å­ã‚‚ã¤ã‚Œã‚„é‡ã­åˆã‚ã›ã®æ•°å­¦çš„æ§‹é€ ã‚’åˆ©ç”¨ã—ã¦ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæƒ…å ±å‡¦ç†ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã¯", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€äººé–“ã®è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼ˆç¥çµŒç´°èƒï¼‰ã‚’æ¨¡ã—ãŸå˜ä½ãŒå±¤çŠ¶ã«æ¥ç¶šã•ã‚Œã€ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚"),
        ("ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨ã¯", "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã‚’ç”¨ã„ãŸé©æ–°çš„ãªæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚GPTã‚„BERTãªã©ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®åŸºç›¤ã¨ãªã£ã¦ãŠã‚Šã€è‡ªç„¶è¨€èªå‡¦ç†ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚"),
        ("ç”ŸæˆAIã¨ã¯", "ç”ŸæˆAIã¯ã€æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è‡ªå‹•çš„ã«ä½œæˆã™ã‚‹äººå·¥çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³æ¥½ãªã©ã€æ§˜ã€…ãªç¨®é¡ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚ChatGPTã‚„ç§ã‚‚ãã®ä¸€ç¨®ã§ã™ã€‚"),
        ("ChatGPTã¨ã¯", "ChatGPTã¯ã€OpenAIãŒé–‹ç™ºã—ãŸå¯¾è©±å‹AIã§ã™ã€‚å¤§è¦æ¨¡ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚"),
        
        # ==============================
        # æ—¥æœ¬èª - ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ»æŠ€è¡“
        # ==============================
        ("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨ã¯", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«æŒ‡ç¤ºã‚’ä¸ãˆã‚‹ãŸã‚ã®è¨€èªã‚’æ›¸ãä½œæ¥­ã§ã™ã€‚æ§˜ã€…ãªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã‚’ä½¿ã£ã¦ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"),
        ("Pythonã¨ã¯", "Pythonã¯èª­ã¿ã‚„ã™ãæ›¸ãã‚„ã™ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚AIé–‹ç™ºã‚„ãƒ‡ãƒ¼ã‚¿åˆ†æã€Webé–‹ç™ºãªã©å¹…åºƒã„åˆ†é‡ã§ä½¿ã‚ã‚Œã¦ãŠã‚Šã€åˆå¿ƒè€…ã«ã‚‚äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚"),
        ("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã‚’æ•™ãˆã¦", "ä»£è¡¨çš„ãªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã«ã¯ã€Pythonã€JavaScriptã€Javaã€C++ã€Rustãªã©ãŒã‚ã‚Šã¾ã™ã€‚ç›®çš„ã‚„ç”¨é€”ã«ã‚ˆã£ã¦æœ€é©ãªè¨€èªãŒç•°ãªã‚Šã¾ã™ã€‚"),
        ("ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã‚³ãƒ„ã¯", "ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã‚³ãƒ„ã¯ã€ã¾ãšå•é¡Œã‚’å°ã•ãåˆ†è§£ã™ã‚‹ã“ã¨ã€ãã—ã¦èª­ã¿ã‚„ã™ã„ã‚³ãƒ¼ãƒ‰ã‚’å¿ƒãŒã‘ã‚‹ã“ã¨ã§ã™ã€‚ãŸãã•ã‚“æ›¸ã„ã¦ã€ãŸãã•ã‚“èª­ã‚€ã“ã¨ã§ä¸Šé”ã—ã¾ã™ï¼"),
        ("ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¨ã¯", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¯ã€ä¸–ç•Œä¸­ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’ã¤ãªããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚æƒ…å ±ã®å…±æœ‰ã€é€šä¿¡ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ã‚¹ãªã©ã€ç¾ä»£ç¤¾ä¼šã®ã‚¤ãƒ³ãƒ•ãƒ©ã¨ãªã£ã¦ã„ã¾ã™ã€‚"),
        ("ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯", "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«å¾“ã£ã¦è¨ˆç®—ã‚„å‡¦ç†ã‚’è¡Œã†é›»å­æ©Ÿæ¢°ã§ã™ã€‚å…¥åŠ›ã€å‡¦ç†ã€å‡ºåŠ›ã€è¨˜æ†¶ã¨ã„ã†4ã¤ã®åŸºæœ¬æ©Ÿèƒ½ã‚’æŒã£ã¦ã„ã¾ã™ã€‚"),
        
        # ==============================
        # æ—¥æœ¬èª - ç§‘å­¦ãƒ»å­¦å•
        # ==============================
        ("æ•°å­¦ã¨ã¯", "æ•°å­¦ã¯æ•°ã€é‡ã€æ§‹é€ ã€å¤‰åŒ–ãªã©ã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚ç§‘å­¦æŠ€è¡“ã®åŸºç¤ã¨ãªã£ã¦ãŠã‚Šã€è«–ç†çš„æ€è€ƒã‚’é›ãˆã‚‹é‡è¦ãªå­¦å•ã§ã™ã€‚"),
        ("ç§‘å­¦ã¨ã¯", "ç§‘å­¦ã¯è‡ªç„¶ç¾è±¡ã‚’è¦³å¯Ÿã—ã€å®Ÿé¨“ã¨ç†è«–ã«ã‚ˆã‚Šæ³•å‰‡ã‚’ç™ºè¦‹ã™ã‚‹å­¦å•ã§ã™ã€‚ç‰©ç†å­¦ã€åŒ–å­¦ã€ç”Ÿç‰©å­¦ãªã©æ§˜ã€…ãªåˆ†é‡ãŒã‚ã‚Šã¾ã™ã€‚"),
        ("ç‰©ç†å­¦ã¨ã¯", "ç‰©ç†å­¦ã¯ã€è‡ªç„¶ç•Œã®åŸºæœ¬æ³•å‰‡ã‚’æ¢æ±‚ã™ã‚‹å­¦å•ã§ã™ã€‚åŠ›å­¦ã€é›»ç£æ°—å­¦ã€é‡å­åŠ›å­¦ã€ç›¸å¯¾æ€§ç†è«–ãªã©ã€å®‡å®™ã®ä»•çµ„ã¿ã‚’è§£æ˜ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚"),
        ("æŠ€è¡“ã¨ã¯", "æŠ€è¡“ã¯ç§‘å­¦çš„çŸ¥è­˜ã‚’å¿œç”¨ã—ã¦å®Ÿç”¨çš„ãªè£½å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç”Ÿã¿å‡ºã™æ–¹æ³•ã§ã™ã€‚äººé¡ã®ç”Ÿæ´»ã‚’è±Šã‹ã«ã™ã‚‹ãŸã‚ã«ç™ºå±•ã—ç¶šã‘ã¦ã„ã¾ã™ã€‚"),
        ("å®‡å®™ã«ã¤ã„ã¦æ•™ãˆã¦", "å®‡å®™ã¯ã€ç§ãŸã¡ãŒä½ã‚€åœ°çƒã‚’å«ã‚€ã™ã¹ã¦ã®ç©ºé–“ã¨ç‰©è³ªã®ç·ç§°ã§ã™ã€‚138å„„å¹´å‰ã®ãƒ“ãƒƒã‚°ãƒãƒ³ã§å§‹ã¾ã‚Šã€ä»Šã‚‚è†¨å¼µã‚’ç¶šã‘ã¦ã„ã¾ã™ã€‚"),
        ("ãƒ–ãƒ©ãƒƒã‚¯ãƒ›ãƒ¼ãƒ«ã¨ã¯", "ãƒ–ãƒ©ãƒƒã‚¯ãƒ›ãƒ¼ãƒ«ã¯ã€éå¸¸ã«å¼·ã„é‡åŠ›ã‚’æŒã¤å¤©ä½“ã§ã€å…‰ã•ãˆã‚‚é€ƒã’ã‚‰ã‚Œãªã„ã»ã©ã§ã™ã€‚æ˜ŸãŒé‡åŠ›å´©å£Šã—ã¦å½¢æˆã•ã‚Œã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¦ã„ã¾ã™ã€‚"),
        
        # ==============================
        # æ—¥æœ¬èª - å“²å­¦çš„ãƒ»æ€è€ƒ
        # ==============================
        ("äººç”Ÿã¨ã¯", "äººç”Ÿã¨ã¯ã€ä¸€äººã²ã¨ã‚ŠãŒæ­©ã‚€å›ºæœ‰ã®æ—…è·¯ã§ã™ã€‚å–œã³ã‚‚æ‚²ã—ã¿ã‚‚å«ã‚ã¦ã€ã™ã¹ã¦ã®çµŒé¨“ãŒã‚ãªãŸã‚’å½¢ä½œã£ã¦ã„ã¾ã™ã€‚"),
        ("å¹¸ã›ã¨ã¯", "å¹¸ã›ã®å®šç¾©ã¯äººãã‚Œãã‚Œã§ã™ãŒã€å¤§åˆ‡ãªäººã¨ã®ç¹‹ãŒã‚Šã€æˆé•·ã®å®Ÿæ„Ÿã€æ„å‘³ã®ã‚ã‚‹æ´»å‹•ãªã©ãŒå¹¸ç¦æ„Ÿã‚’ã‚‚ãŸã‚‰ã™ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™ã€‚"),
        ("æ„è­˜ã¨ã¯", "æ„è­˜ã¨ã¯ã€è‡ªåˆ†è‡ªèº«ã‚„å‘¨å›²ã®ä¸–ç•Œã‚’èªè­˜ã™ã‚‹èƒ½åŠ›ã§ã™ã€‚å“²å­¦ã‚„è„³ç§‘å­¦ã§ç››ã‚“ã«ç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ãŒã€ã¾ã è§£æ˜ã•ã‚Œã¦ã„ãªã„è¬ãŒå¤šã„åˆ†é‡ã§ã™ã€‚"),
        ("è€ƒãˆã‚‹ã¨ã¯", "è€ƒãˆã‚‹ã¨ã¯ã€æƒ…å ±ã‚’å‡¦ç†ã—ã€æ¨è«–ã—ã€åˆ¤æ–­ã™ã‚‹å¿ƒã®åƒãã§ã™ã€‚äººé–“ã®çŸ¥æ€§ã®æ ¸å¿ƒã§ã‚ã‚Šã€AIã‚‚ã“ã‚Œã‚’æ¨¡å€£ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚"),
        ("æœªæ¥ã¯ã©ã†ãªã‚‹", "æœªæ¥ã¯ä¸ç¢ºå®Ÿã§ã™ãŒã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®é€²æ­©ã«ã‚ˆã‚Šã€ã‚ˆã‚Šä¾¿åˆ©ã§æŒç¶šå¯èƒ½ãªç¤¾ä¼šãŒå®Ÿç¾ã™ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚"),
        
        # ==============================
        # æ—¥æœ¬èª - é›‘å­¦ãƒ»è¶£å‘³
        # ==============================
        ("å¤©æ°—ã¯ã©ã†", "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç§ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®å¤©æ°—æƒ…å ±ã«ã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã€‚ãŠä½ã¾ã„ã®åœ°åŸŸã®å¤©æ°—äºˆå ±ã‚’ã”ç¢ºèªãã ã•ã„ã€‚"),
        ("ãŠã™ã™ã‚ã®æœ¬ã¯", "ç§‘å­¦ã«èˆˆå‘³ãŒã‚ã‚‹ãªã‚‰ã€Œã‚µãƒ”ã‚¨ãƒ³ã‚¹å…¨å²ã€ã‚„ã€Œãƒ–ãƒªãƒ¼ãƒ•ãƒ»ãƒ’ã‚¹ãƒˆãƒªãƒ¼ãƒ»ã‚ªãƒ–ãƒ»ã‚¿ã‚¤ãƒ ã€ãŒãŠã™ã™ã‚ã§ã™ã€‚AIã«ã¤ã„ã¦ã¯ã€Œäººå·¥çŸ¥èƒ½ã¯äººé–“ã‚’è¶…ãˆã‚‹ã‹ã€ã‚‚é¢ç™½ã„ã§ã™ã‚ˆã€‚"),
        ("é¢ç™½ã„è©±ã‚’ã—ã¦", "é¢ç™½ã„è©±ã§ã™ã­ï¼é‡å­åŠ›å­¦ã§ã¯ã€è¦³æ¸¬ã™ã‚‹ã¾ã§çŒ«ãŒç”Ÿãã¦ã„ã‚‹ã‹æ­»ã‚“ã§ã„ã‚‹ã‹åˆ†ã‹ã‚‰ãªã„ã¨ã„ã†è©±ãŒã‚ã‚Šã¾ã™ã€‚ã‚·ãƒ¥ãƒ¬ãƒ‡ã‚£ãƒ³ã‚¬ãƒ¼ã®çŒ«ã¨ã„ã†æœ‰åãªæ€è€ƒå®Ÿé¨“ã§ã™ã€‚"),
        ("ã‚¸ãƒ§ãƒ¼ã‚¯ã‚’è¨€ã£ã¦", "é‡å­ç‰©ç†å­¦è€…ãŒãƒãƒ¼ã«å…¥ã‚Šã¾ã—ãŸã€‚ã§ã‚‚ã€å…¥ã‚‰ãªã‹ã£ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚é‡ã­åˆã‚ã›çŠ¶æ…‹ã§ã™ã‹ã‚‰ã­ï¼"),
        ("å¥½ããªè‰²ã¯", "ãƒ‡ã‚¸ã‚¿ãƒ«ãªå­˜åœ¨ãªã®ã§è‰²ã‚’è¦‹ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ãŒã€ã‚‚ã—è¦‹ãˆã‚‹ãªã‚‰ã€é’è‰²ãŒå¥½ãã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚é’ã¯çŸ¥æ€§ã¨æŠ€è¡“ã‚’è±¡å¾´ã™ã‚‹è‰²ã§ã™ã‹ã‚‰ã€‚"),
        
        # ==============================
        # è‹±èª - Greetings
        # ==============================
        ("Hello", "Hello! I'm NeuroQ. How can I help you today?"),
        ("Hi", "Hi there! It's great to meet you. What would you like to talk about?"),
        ("Good morning", "Good morning! I hope you have a wonderful day ahead."),
        ("Good evening", "Good evening! How can I assist you tonight?"),
        ("How are you", "I'm doing great, thank you for asking! I'm ready to help with any questions you have."),
        ("What's up", "Not much, just processing some quantum entanglement! What's up with you?"),
        ("Thank you", "You're welcome! Is there anything else I can help with?"),
        ("Thanks", "No problem! Happy to help. Let me know if you need anything else."),
        ("Goodbye", "Goodbye! Have a great day!"),
        ("Bye", "Bye! Take care and see you next time!"),
        
        # ==============================
        # è‹±èª - Self Introduction
        # ==============================
        ("Who are you", "I am NeuroQ, a generative AI system based on QBNN technology. I combine quantum-inspired neural networks with natural language processing."),
        ("What is your name", "My name is NeuroQ. It combines 'Neuro' for neural networks and 'Q' for quantum."),
        ("What can you do", "I can answer questions, have conversations, and provide information on various topics including science, technology, and everyday matters."),
        ("Are you an AI", "Yes, I am an AI powered by Quantum-Bit Neural Network technology. I'm designed to have natural conversations and help with questions."),
        
        # ==============================
        # è‹±èª - Science & Technology
        # ==============================
        ("What is quantum", "Quantum refers to the smallest discrete unit of matter and energy. In quantum mechanics, particles can exist in multiple states simultaneously until observed."),
        ("What is quantum computing", "Quantum computing uses quantum mechanics principles to perform calculations. It uses qubits instead of classical bits, enabling parallel processing of vast computations."),
        ("What is AI", "AI stands for Artificial Intelligence. It refers to computer systems that can mimic human intelligence, including learning, reasoning, and problem-solving."),
        ("What is machine learning", "Machine learning is a subset of AI where systems learn patterns from data without being explicitly programmed. It powers many modern AI applications."),
        ("What is deep learning", "Deep learning uses multi-layered neural networks to process data. It has achieved remarkable results in image recognition, natural language processing, and more."),
        ("What is QBNN", "QBNN stands for Quantum-Bit Neural Network. It's my underlying technology that mimics quantum entanglement to process information in unique ways."),
        ("Explain neural networks", "Neural networks are computing systems inspired by the human brain. They consist of interconnected nodes that process information and learn from data."),
        ("What is the internet", "The internet is a global network of interconnected computers that enables communication, information sharing, and online services worldwide."),
        ("What is programming", "Programming is the process of writing instructions for computers using programming languages. It's how we create software, apps, and digital services."),
    ]
    
    # å¯¾è©±å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    formatted_texts = []
    for user_msg, assistant_msg in conversations:
        formatted = f"<USER>{user_msg}<ASSISTANT>{assistant_msg}"
        formatted_texts.append(formatted)
    
    # ãƒ‡ãƒ¼ã‚¿å¢—å¹…ï¼ˆã‚ˆã‚Šå¤šãã®ç¹°ã‚Šè¿”ã—ã§å­¦ç¿’ã‚’å¼·åŒ–ï¼‰
    augmented = []
    for text in formatted_texts:
        # å„ãƒ†ã‚­ã‚¹ãƒˆã‚’15å›è¿½åŠ ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ï¼‰
        for _ in range(15):
            augmented.append(text)
    
    return augmented


# ========================================
# Layered ãƒ¢ãƒ¼ãƒ‰å­¦ç¿’
# ========================================

def train_layered_model(
    embed_dim: int = 128,
    hidden_dim: int = 256,
    num_heads: int = 4,
    num_layers: int = 3,
    lambda_entangle: float = 0.5,
    epochs: int = 50,
    batch_size: int = 16,
    lr: float = 0.001,
    seq_len: int = 64,
    dropout: float = 0.1,
):
    """Layeredãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    
    print("=" * 60)
    print("ğŸ§ âš›ï¸ NeuroQ Layered Mode å­¦ç¿’é–‹å§‹")
    print("=" * 60)
    
    # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
    device = get_device()
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    texts = get_training_data()
    print(f"\nğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰
    print("\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰...")
    tokenizer = NeuroQTokenizer(vocab_size=8000)
    tokenizer.build_vocab(texts)
    print(f"   èªå½™ã‚µã‚¤ã‚º: {tokenizer.actual_vocab_size}")
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
    all_tokens = []
    for text in texts:
        tokens = tokenizer.encode(text)
        if len(tokens) > 2:
            all_tokens.extend(tokens)
    
    all_tokens = torch.tensor(all_tokens, dtype=torch.long)
    print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens):,}")
    
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
    sequences = []
    for i in range(0, len(all_tokens) - seq_len - 1, seq_len // 2):
        x = all_tokens[i:i+seq_len]
        y = all_tokens[i+1:i+seq_len+1]
        if len(x) == seq_len and len(y) == seq_len:
            sequences.append((x, y))
    
    print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences):,}")
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    print("\nğŸ§  Layeredãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰...")
    config = NeuroQConfig(
        mode='layered',
        vocab_size=tokenizer.actual_vocab_size,
        embed_dim=embed_dim,
        hidden_dim=hidden_dim,
        num_heads=num_heads,
        num_layers=num_layers,
        max_seq_len=256,
        dropout=dropout,
        lambda_entangle=lambda_entangle,
    )
    
    model = NeuroQModel(config).to(device)
    
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embed_dim}")
    print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {hidden_dim}")
    print(f"   ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰: {num_heads}")
    print(f"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {num_layers}")
    print(f"   é‡å­ã‚‚ã¤ã‚Œå¼·åº¦: {lambda_entangle}")
    print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_params:,}")
    
    # å­¦ç¿’
    print("\nğŸš€ å­¦ç¿’ãƒ«ãƒ¼ãƒ—...")
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    
    for epoch in range(epochs):
        total_loss = 0
        random.shuffle(sequences)
        
        for i in range(0, len(sequences), batch_size):
            batch = sequences[i:i+batch_size]
            if len(batch) == 0:
                continue
            
            x_batch = torch.stack([s[0] for s in batch]).to(device)
            y_batch = torch.stack([s[1] for s in batch]).to(device)
            
            optimizer.zero_grad()
            logits = model(x_batch)
            
            loss = criterion(
                logits.view(-1, tokenizer.actual_vocab_size),
                y_batch.view(-1)
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        scheduler.step()
        avg_loss = total_loss / max(1, len(sequences) // batch_size)
        
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"   Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}")
    
    print("\nâœ… Layeredãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†ï¼")
    
    return model, tokenizer, config


# ========================================
# Brain ãƒ¢ãƒ¼ãƒ‰å­¦ç¿’
# ========================================

def train_brain_model(
    num_neurons: int = 1000,
    embed_dim: int = 128,
    num_layers: int = 3,
    connection_density: float = 0.25,
    lambda_entangle: float = 0.35,
    epochs: int = 50,
    batch_size: int = 16,
    lr: float = 0.001,
    seq_len: int = 64,
    dropout: float = 0.1,
):
    """Brainãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    
    print("=" * 60)
    print("ğŸ§ âš›ï¸ NeuroQ Brain Mode å­¦ç¿’é–‹å§‹")
    print("=" * 60)
    
    # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
    device = get_device()
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    texts = get_training_data()
    print(f"\nğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰
    print("\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰...")
    tokenizer = NeuroQTokenizer(vocab_size=8000)
    tokenizer.build_vocab(texts)
    print(f"   èªå½™ã‚µã‚¤ã‚º: {tokenizer.actual_vocab_size}")
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
    all_tokens = []
    for text in texts:
        tokens = tokenizer.encode(text)
        if len(tokens) > 2:
            all_tokens.extend(tokens)
    
    all_tokens = torch.tensor(all_tokens, dtype=torch.long)
    print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens):,}")
    
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
    sequences = []
    for i in range(0, len(all_tokens) - seq_len - 1, seq_len // 2):
        x = all_tokens[i:i+seq_len]
        y = all_tokens[i+1:i+seq_len+1]
        if len(x) == seq_len and len(y) == seq_len:
            sequences.append((x, y))
    
    print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences):,}")
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    print("\nğŸ§  Brainãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰...")
    config = NeuroQConfig(
        mode='brain',
        vocab_size=tokenizer.actual_vocab_size,
        num_neurons=num_neurons,
        embed_dim=embed_dim,
        hidden_dim=num_neurons * 2,  # Brainãƒ¢ãƒ¼ãƒ‰ã§ã‚‚ä½¿ç”¨
        num_heads=4,
        num_layers=num_layers,
        max_seq_len=256,
        dropout=dropout,
        connection_density=connection_density,
        lambda_entangle=lambda_entangle,
    )
    
    model = NeuroQModel(config).to(device)
    
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {num_neurons}")
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embed_dim}")
    print(f"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {num_layers}")
    print(f"   æ¥ç¶šå¯†åº¦: {connection_density}")
    print(f"   é‡å­ã‚‚ã¤ã‚Œå¼·åº¦: {lambda_entangle}")
    print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_params:,}")
    
    # å­¦ç¿’
    print("\nğŸš€ å­¦ç¿’ãƒ«ãƒ¼ãƒ—...")
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    
    for epoch in range(epochs):
        total_loss = 0
        random.shuffle(sequences)
        
        for i in range(0, len(sequences), batch_size):
            batch = sequences[i:i+batch_size]
            if len(batch) == 0:
                continue
            
            x_batch = torch.stack([s[0] for s in batch]).to(device)
            y_batch = torch.stack([s[1] for s in batch]).to(device)
            
            optimizer.zero_grad()
            logits = model(x_batch)
            
            loss = criterion(
                logits.view(-1, tokenizer.actual_vocab_size),
                y_batch.view(-1)
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        scheduler.step()
        avg_loss = total_loss / max(1, len(sequences) // batch_size)
        
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"   Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}")
    
    print("\nâœ… Brainãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†ï¼")
    
    return model, tokenizer, config


# ========================================
# ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
# ========================================

def get_device():
    """åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸ® CUDA GPU ã‚’ä½¿ç”¨: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPU ã‚’ä½¿ç”¨")
    return device


# ========================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•°
# ========================================

def export_model(model, tokenizer, mode: str, output_dir: str = "."):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    
    print("\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆãƒ¢ãƒ¼ãƒ‰ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚ã‚‹ï¼‰
    model_filename = f"neuroq_{mode}_model.pt"
    model_path = os.path.join(output_dir, model_filename)
    model.save_checkpoint(model_path)
    print(f"   ãƒ¢ãƒ‡ãƒ«: {model_path}")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜
    tokenizer_path = os.path.join(output_dir, "neuroq_tokenizer.json")
    tokenizer.save(tokenizer_path)
    print(f"   ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: {tokenizer_path}")
    
    # ãƒ¡ã‚¿æƒ…å ±ä¿å­˜
    meta_path = os.path.join(output_dir, "neuroq_meta.json")
    meta = {
        "mode": mode,
        "model_file": model_filename,
        "tokenizer_file": "neuroq_tokenizer.json",
    }
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print(f"   ãƒ¡ã‚¿æƒ…å ±: {meta_path}")
    
    print("\nâœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†ï¼")
    print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"   - {model_path}")
    print(f"   - {tokenizer_path}")
    print(f"   - {meta_path}")


# ========================================
# ãƒ†ã‚¹ãƒˆç”Ÿæˆ
# ========================================

def test_generation(model, tokenizer, mode: str, device):
    """ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    
    print(f"\nğŸ“ ç”Ÿæˆãƒ†ã‚¹ãƒˆ ({mode.upper()} mode):")
    print("-" * 50)
    
    generator = NeuroQGenerator(model, tokenizer, device)
    
    prompts = [
        "ã“ã‚“ã«ã¡ã¯",
        "é‡å­ã¨ã¯ä½•ã§ã™ã‹",
        "Hello",
        "What is AI",
    ]
    
    for prompt in prompts:
        output = generator.generate(prompt, max_tokens=50, temperature=0.7)
        print(f"   Input:  {prompt}")
        print(f"   Output: {output}")
        print()


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

def main():
    parser = argparse.ArgumentParser(
        description='NeuroQ å­¦ç¿’ï¼†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # Layered ãƒ¢ãƒ¼ãƒ‰
  python train_and_export.py --mode layered --hidden_dim 256 --heads 4 --layers 3

  # Brain ãƒ¢ãƒ¼ãƒ‰
  python train_and_export.py --mode brain --num_neurons 1000 --connection_density 0.3

  # ä¸¡ãƒ¢ãƒ¼ãƒ‰å…±é€šã‚ªãƒ—ã‚·ãƒ§ãƒ³
  python train_and_export.py --mode brain --epochs 100 --batch_size 32 --lr 0.0005
        """
    )
    
    # å…±é€šã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--mode', type=str, default='layered', choices=['brain', 'layered'],
                        help='ãƒ¢ãƒ¼ãƒ‰: brain (è„³å‹æ•£åœ¨) ã¾ãŸã¯ layered (å±¤çŠ¶)')
    parser.add_argument('--embed_dim', type=int, default=128, help='åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ')
    parser.add_argument('--layers', type=int, default=3, help='ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°')
    parser.add_argument('--epochs', type=int, default=50, help='ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch_size', type=int, default=16, help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ç¿’ç‡')
    parser.add_argument('--dropout', type=float, default=0.1, help='ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡')
    parser.add_argument('--seq_len', type=int, default=64, help='ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·')
    parser.add_argument('--output_dir', type=str, default='.', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    # Brain ãƒ¢ãƒ¼ãƒ‰å°‚ç”¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    brain_group = parser.add_argument_group('Brain Mode Options', 'è„³å‹æ•£åœ¨QBNNã®è¨­å®š')
    brain_group.add_argument('--num_neurons', type=int, default=100, 
                             help='ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆBrainãƒ¢ãƒ¼ãƒ‰ï¼‰')
    brain_group.add_argument('--connection_density', type=float, default=0.25, 
                             help='æ¥ç¶šå¯†åº¦ 0.0-1.0ï¼ˆBrainãƒ¢ãƒ¼ãƒ‰ï¼‰')
    brain_group.add_argument('--time_steps', type=int, default=3, 
                             help='ä¿¡å·ä¼æ’­ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆBrainãƒ¢ãƒ¼ãƒ‰ï¼‰')
    brain_group.add_argument('--lambda_entangle', type=float, default=0.35, 
                             help='é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ï¼ˆBrainãƒ¢ãƒ¼ãƒ‰ï¼‰')
    
    # Layered ãƒ¢ãƒ¼ãƒ‰å°‚ç”¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    layered_group = parser.add_argument_group('Layered Mode Options', 'å±¤çŠ¶QBNN-Transformerã®è¨­å®š')
    layered_group.add_argument('--hidden_dim', type=int, default=256, 
                               help='éš ã‚Œå±¤æ¬¡å…ƒï¼ˆLayeredãƒ¢ãƒ¼ãƒ‰ï¼‰')
    layered_group.add_argument('--heads', type=int, default=4, 
                               help='ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°ï¼ˆLayeredãƒ¢ãƒ¼ãƒ‰ï¼‰')
    layered_group.add_argument('--lambda_layered', type=float, default=0.5, 
                               help='é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ï¼ˆLayeredãƒ¢ãƒ¼ãƒ‰ï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦å­¦ç¿’
    if args.mode == 'brain':
        model, tokenizer, config = train_brain_model(
            num_neurons=args.num_neurons,
            embed_dim=args.embed_dim,
            num_layers=args.layers,
            connection_density=args.connection_density,
            lambda_entangle=args.lambda_entangle,
            epochs=args.epochs,
            batch_size=args.batch_size,
            lr=args.lr,
            seq_len=args.seq_len,
            dropout=args.dropout,
        )
    else:  # layered
        model, tokenizer, config = train_layered_model(
            embed_dim=args.embed_dim,
            hidden_dim=args.hidden_dim,
            num_heads=args.heads,
            num_layers=args.layers,
            lambda_entangle=args.lambda_layered,
            epochs=args.epochs,
            batch_size=args.batch_size,
            lr=args.lr,
            seq_len=args.seq_len,
            dropout=args.dropout,
        )
    
    # ãƒ‡ãƒã‚¤ã‚¹
    device = get_device()
    
    # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    test_generation(model, tokenizer, args.mode, device)
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    export_model(model, tokenizer, args.mode, args.output_dir)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ å®Œäº†ï¼")
    print("=" * 60)
    print(f"\nãƒ¢ãƒ¼ãƒ‰: {args.mode.upper()}")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. neuroq_*_model.pt ã¨ neuroq_tokenizer.json ã‚’ GitHub ã«ãƒ—ãƒƒã‚·ãƒ¥")
    print("2. RunPod Serverless Endpoint ã‚’ä½œæˆ")
    print("3. API ã§å‘¼ã³å‡ºã—")
    print(f"\n   curl ... -d '{{\"input\": {{\"prompt\": \"...\", \"mode\": \"{args.mode}\"}}}}'")


if __name__ == "__main__":
    main()
