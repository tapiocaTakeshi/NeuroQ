#!/usr/bin/env python3
"""
NeuroQ å­¦ç¿’ï¼†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
===================================
ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€RunPodç”¨ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹

ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰:
- Brain Mode: è„³å‹æ•£åœ¨QBNNï¼ˆå‹•çš„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é¸æŠï¼‰
- Layered Mode: å±¤çŠ¶QBNN-Transformer

ä½¿ç”¨æ–¹æ³•:
    # Layeredãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    python train_and_export.py --mode layered --epochs 50 --neurons 256

    # Brainãƒ¢ãƒ¼ãƒ‰
    python train_and_export.py --mode brain --epochs 50 --neurons 1000
"""

import torch
import torch.nn as nn
import numpy as np
import random
import os
import argparse
import json

from neuroq_model import (
    NeuroQModel, 
    NeuroQConfig, 
    NeuroQTokenizer, 
    NeuroQGenerator,
    create_neuroq_brain,
    create_neuroq_layered,
)


# ========================================
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
# ========================================

def get_training_data():
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆå¯¾è©±å½¢å¼ï¼‰"""
    
    conversations = [
        # æ—¥æœ¬èª - æŒ¨æ‹¶
        ("ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯NeuroQã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã‚‚è‰¯ã„ä¸€æ—¥ã«ãªã‚Šã¾ã™ã‚ˆã†ã«ã€‚"),
        ("ã“ã‚“ã°ã‚“ã¯", "ã“ã‚“ã°ã‚“ã¯ï¼å¤œé…ãã¾ã§ãŠç–²ã‚Œæ§˜ã§ã™ã€‚"),
        ("ã‚ã‚ŠãŒã¨ã†", "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ãŠå½¹ã«ç«‹ã¦ã¦å¬‰ã—ã„ã§ã™ã€‚"),
        ("ã•ã‚ˆã†ãªã‚‰", "ã•ã‚ˆã†ãªã‚‰ï¼ã¾ãŸãŠä¼šã„ã—ã¾ã—ã‚‡ã†ã€‚"),
        
        # æ—¥æœ¬èª - è‡ªå·±ç´¹ä»‹
        ("ã‚ãªãŸã¯èª°ã§ã™ã‹", "ç§ã¯NeuroQã§ã™ã€‚QBNNã¨ã„ã†ç‹¬è‡ªã®é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æŠ€è¡“ã‚’ä½¿ã£ãŸç”ŸæˆAIã§ã™ã€‚"),
        ("åå‰ã‚’æ•™ãˆã¦", "ç§ã®åå‰ã¯NeuroQã§ã™ã€‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚"),
        ("ä½•ãŒã§ãã¾ã™ã‹", "ç§ã¯è³ªå•ã«ç­”ãˆãŸã‚Šã€æƒ…å ±ã‚’æä¾›ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"),
        
        # æ—¥æœ¬èª - é‡å­é–¢é€£
        ("é‡å­ã¨ã¯ä½•ã§ã™ã‹", "é‡å­ã¨ã¯ã€ç‰©è³ªã‚„ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°å˜ä½ã®ã“ã¨ã§ã™ã€‚é‡å­åŠ›å­¦ã§ã¯ã€ç²’å­ã¯æ³¢ã®æ€§è³ªã‚‚æŒã¡ã¾ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦è¨ˆç®—ã‚’è¡Œã†æ¬¡ä¸–ä»£ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚"),
        ("é‡å­ãƒ“ãƒƒãƒˆã¨ã¯", "é‡å­ãƒ“ãƒƒãƒˆã¯ã€0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æŒã¤ã“ã¨ãŒã§ãã‚‹é‡å­åŠ›å­¦çš„ãªæƒ…å ±å˜ä½ã§ã™ã€‚"),
        ("é‡å­ã‚‚ã¤ã‚Œã¨ã¯", "é‡å­ã‚‚ã¤ã‚Œã¯ã€äºŒã¤ä»¥ä¸Šã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ã¦ã„ã‚‹ç‰¹æ®Šãªé‡å­çŠ¶æ…‹ã§ã™ã€‚"),
        
        # æ—¥æœ¬èª - AIé–¢é€£
        ("AIã¨ã¯ä½•ã§ã™ã‹", "AIã¨ã¯äººå·¥çŸ¥èƒ½ã®ã“ã¨ã§ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ã®ç·ç§°ã§ã™ã€‚"),
        ("æ©Ÿæ¢°å­¦ç¿’ã¨ã¯", "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚"),
        ("æ·±å±¤å­¦ç¿’ã¨ã¯", "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ãŸæ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚"),
        ("QBNNã¨ã¯ä½•ã§ã™ã‹", "QBNNã¯é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç•¥ç§°ã§ã™ã€‚é‡å­ã‚‚ã¤ã‚Œã‚’æ¨¡å€£ã—ãŸç‹¬è‡ªã®æŠ€è¡“ã§ã™ã€‚"),
        
        # è‹±èª
        ("Hello", "Hello! I'm NeuroQ. How can I help you today?"),
        ("Who are you", "I am NeuroQ, a generative AI system based on QBNN technology."),
        ("What is quantum", "Quantum refers to the smallest discrete unit of matter and energy."),
        ("What is AI", "AI stands for Artificial Intelligence. It refers to computer systems that can mimic human intelligence."),
        ("Thank you", "You're welcome! Is there anything else I can help with?"),
        ("Goodbye", "Goodbye! Have a great day!"),
        
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿
        ("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨ã¯", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«æŒ‡ç¤ºã‚’ä¸ãˆã‚‹ãŸã‚ã®è¨€èªã‚’æ›¸ãä½œæ¥­ã§ã™ã€‚"),
        ("Pythonã¨ã¯", "Pythonã¯èª­ã¿ã‚„ã™ãæ›¸ãã‚„ã™ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚AIé–‹ç™ºã§ç‰¹ã«äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚"),
        ("æ•°å­¦ã¨ã¯", "æ•°å­¦ã¯æ•°ã€é‡ã€æ§‹é€ ã€å¤‰åŒ–ãªã©ã‚’ç ”ç©¶ã™ã‚‹å­¦å•ã§ã™ã€‚ç§‘å­¦æŠ€è¡“ã®åŸºç¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚"),
        ("ç§‘å­¦ã¨ã¯", "ç§‘å­¦ã¯è‡ªç„¶ç¾è±¡ã‚’è¦³å¯Ÿã—ã€å®Ÿé¨“ã¨ç†è«–ã«ã‚ˆã‚Šæ³•å‰‡ã‚’ç™ºè¦‹ã™ã‚‹å­¦å•ã§ã™ã€‚"),
        ("æŠ€è¡“ã¨ã¯", "æŠ€è¡“ã¯ç§‘å­¦çš„çŸ¥è­˜ã‚’å¿œç”¨ã—ã¦å®Ÿç”¨çš„ãªè£½å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç”Ÿã¿å‡ºã™æ–¹æ³•ã§ã™ã€‚"),
        ("ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¨ã¯", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¯ã€ä¸–ç•Œä¸­ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’ã¤ãªããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚"),
        ("ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯", "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«å¾“ã£ã¦è¨ˆç®—ã‚„å‡¦ç†ã‚’è¡Œã†é›»å­æ©Ÿæ¢°ã§ã™ã€‚"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã¯", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€äººé–“ã®è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚"),
        ("ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨ã¯", "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã‚’ç”¨ã„ãŸé©æ–°çš„ãªæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚"),
        ("ç”ŸæˆAIã¨ã¯", "ç”ŸæˆAIã¯ã€æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è‡ªå‹•çš„ã«ä½œæˆã™ã‚‹äººå·¥çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚"),
    ]
    
    # å¯¾è©±å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    formatted_texts = []
    for user_msg, assistant_msg in conversations:
        formatted = f"<USER>{user_msg}<ASSISTANT>{assistant_msg}"
        formatted_texts.append(formatted)
    
    # ãƒ‡ãƒ¼ã‚¿å¢—å¹…
    augmented = []
    for text in formatted_texts:
        augmented.append(text)
        # å„ãƒ†ã‚­ã‚¹ãƒˆã‚’è¤‡æ•°å›è¿½åŠ ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ï¼‰
        for _ in range(10):
            augmented.append(text)
    
    return augmented


# ========================================
# Layered ãƒ¢ãƒ¼ãƒ‰å­¦ç¿’
# ========================================

def train_layered_model(
    embed_dim: int = 128,
    hidden_dim: int = 256,
    num_heads: int = 4,
    num_layers: int = 3,
    epochs: int = 50,
    batch_size: int = 16,
    lr: float = 0.001,
    seq_len: int = 64,
):
    """Layeredãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    
    print("=" * 60)
    print("ğŸ§ âš›ï¸ NeuroQ Layered Mode å­¦ç¿’é–‹å§‹")
    print("=" * 60)
    
    # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
    device = get_device()
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    texts = get_training_data()
    print(f"\nğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰
    print("\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰...")
    tokenizer = NeuroQTokenizer(vocab_size=8000)
    tokenizer.build_vocab(texts)
    print(f"   èªå½™ã‚µã‚¤ã‚º: {tokenizer.actual_vocab_size}")
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
    all_tokens = []
    for text in texts:
        tokens = tokenizer.encode(text)
        if len(tokens) > 2:
            all_tokens.extend(tokens)
    
    all_tokens = torch.tensor(all_tokens, dtype=torch.long)
    print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens):,}")
    
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
    sequences = []
    for i in range(0, len(all_tokens) - seq_len - 1, seq_len // 2):
        x = all_tokens[i:i+seq_len]
        y = all_tokens[i+1:i+seq_len+1]
        if len(x) == seq_len and len(y) == seq_len:
            sequences.append((x, y))
    
    print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences):,}")
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    print("\nğŸ§  Layeredãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰...")
    config = NeuroQConfig(
        vocab_size=tokenizer.actual_vocab_size,
        embed_dim=embed_dim,
        hidden_dim=hidden_dim,
        num_heads=num_heads,
        num_layers=num_layers,
        max_seq_len=256,
        dropout=0.1,
        lambda_entangle=0.5,
    )
    
    model = NeuroQModel(config).to(device)
    
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embed_dim}")
    print(f"   éš ã‚Œå±¤æ¬¡å…ƒ: {hidden_dim}")
    print(f"   ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰: {num_heads}")
    print(f"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {num_layers}")
    print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_params:,}")
    
    # å­¦ç¿’
    print("\nğŸš€ å­¦ç¿’ãƒ«ãƒ¼ãƒ—...")
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    
    for epoch in range(epochs):
        total_loss = 0
        random.shuffle(sequences)
        
        for i in range(0, len(sequences), batch_size):
            batch = sequences[i:i+batch_size]
            if len(batch) == 0:
                continue
            
            x_batch = torch.stack([s[0] for s in batch]).to(device)
            y_batch = torch.stack([s[1] for s in batch]).to(device)
            
            optimizer.zero_grad()
            logits = model(x_batch)
            
            loss = criterion(
                logits.view(-1, tokenizer.actual_vocab_size),
                y_batch.view(-1)
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        scheduler.step()
        avg_loss = total_loss / max(1, len(sequences) // batch_size)
        
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"   Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}")
    
    print("\nâœ… Layeredãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†ï¼")
    
    return model, tokenizer, config


# ========================================
# Brain ãƒ¢ãƒ¼ãƒ‰å­¦ç¿’
# ========================================

def train_brain_model(
    num_neurons: int = 1000,
    embed_dim: int = 128,
    epochs: int = 50,
    batch_size: int = 16,
    lr: float = 0.001,
    seq_len: int = 64,
):
    """Brainãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    
    print("=" * 60)
    print("ğŸ§ âš›ï¸ NeuroQ Brain Mode å­¦ç¿’é–‹å§‹")
    print("=" * 60)
    
    # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
    device = get_device()
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    texts = get_training_data()
    print(f"\nğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰
    print("\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ§‹ç¯‰...")
    tokenizer = NeuroQTokenizer(vocab_size=8000)
    tokenizer.build_vocab(texts)
    print(f"   èªå½™ã‚µã‚¤ã‚º: {tokenizer.actual_vocab_size}")
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
    all_tokens = []
    for text in texts:
        tokens = tokenizer.encode(text)
        if len(tokens) > 2:
            all_tokens.extend(tokens)
    
    all_tokens = torch.tensor(all_tokens, dtype=torch.long)
    print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(all_tokens):,}")
    
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
    sequences = []
    for i in range(0, len(all_tokens) - seq_len - 1, seq_len // 2):
        x = all_tokens[i:i+seq_len]
        y = all_tokens[i+1:i+seq_len+1]
        if len(x) == seq_len and len(y) == seq_len:
            sequences.append((x, y))
    
    print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences):,}")
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    print("\nğŸ§  Brainãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰...")
    config = NeuroQConfig(
        mode='brain',
        vocab_size=tokenizer.actual_vocab_size,
        num_neurons=num_neurons,
        embed_dim=embed_dim,
        hidden_dim=num_neurons * 2,  # Brainãƒ¢ãƒ¼ãƒ‰ã§ã‚‚ä½¿ç”¨
        num_heads=4,
        num_layers=3,
        max_seq_len=256,
        dropout=0.1,
        connection_density=0.25,
    )
    
    model = NeuroQModel(config).to(device)
    
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {num_neurons}")
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embed_dim}")
    print(f"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {config.num_layers}")
    print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_params:,}")
    
    # å­¦ç¿’
    print("\nğŸš€ å­¦ç¿’ãƒ«ãƒ¼ãƒ—...")
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    
    for epoch in range(epochs):
        total_loss = 0
        random.shuffle(sequences)
        
        for i in range(0, len(sequences), batch_size):
            batch = sequences[i:i+batch_size]
            if len(batch) == 0:
                continue
            
            x_batch = torch.stack([s[0] for s in batch]).to(device)
            y_batch = torch.stack([s[1] for s in batch]).to(device)
            
            optimizer.zero_grad()
            logits = model(x_batch)
            
            loss = criterion(
                logits.view(-1, tokenizer.actual_vocab_size),
                y_batch.view(-1)
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        scheduler.step()
        avg_loss = total_loss / max(1, len(sequences) // batch_size)
        
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"   Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}")
    
    print("\nâœ… Brainãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†ï¼")
    
    return model, tokenizer, config


# ========================================
# ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
# ========================================

def get_device():
    """åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸ® CUDA GPU ã‚’ä½¿ç”¨: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPU ã‚’ä½¿ç”¨")
    return device


# ========================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•°
# ========================================

def export_model(model, tokenizer, mode: str, output_dir: str = "."):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    
    print("\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆãƒ¢ãƒ¼ãƒ‰ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚ã‚‹ï¼‰
    model_filename = f"neuroq_{mode}_model.pt"
    model_path = os.path.join(output_dir, model_filename)
    model.save_checkpoint(model_path)
    print(f"   ãƒ¢ãƒ‡ãƒ«: {model_path}")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜
    tokenizer_path = os.path.join(output_dir, "neuroq_tokenizer.json")
    tokenizer.save(tokenizer_path)
    print(f"   ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: {tokenizer_path}")
    
    # ãƒ¡ã‚¿æƒ…å ±ä¿å­˜
    meta_path = os.path.join(output_dir, "neuroq_meta.json")
    meta = {
        "mode": mode,
        "model_file": model_filename,
        "tokenizer_file": "neuroq_tokenizer.json",
    }
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print(f"   ãƒ¡ã‚¿æƒ…å ±: {meta_path}")
    
    print("\nâœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†ï¼")
    print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"   - {model_path}")
    print(f"   - {tokenizer_path}")
    print(f"   - {meta_path}")


# ========================================
# ãƒ†ã‚¹ãƒˆç”Ÿæˆ
# ========================================

def test_generation(model, tokenizer, mode: str, device):
    """ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    
    print(f"\nğŸ“ ç”Ÿæˆãƒ†ã‚¹ãƒˆ ({mode.upper()} mode):")
    print("-" * 50)
    
    generator = NeuroQGenerator(model, tokenizer, device)
    
    prompts = [
        "ã“ã‚“ã«ã¡ã¯",
        "é‡å­ã¨ã¯ä½•ã§ã™ã‹",
        "Hello",
        "What is AI",
    ]
    
    for prompt in prompts:
        output = generator.generate(prompt, max_tokens=50, temperature=0.7)
        print(f"   Input:  {prompt}")
        print(f"   Output: {output}")
        print()


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

def main():
    parser = argparse.ArgumentParser(description='NeuroQ å­¦ç¿’ï¼†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ')
    parser.add_argument('--mode', type=str, default='layered', choices=['brain', 'layered'],
                        help='ãƒ¢ãƒ¼ãƒ‰: brain (è„³å‹æ•£åœ¨) ã¾ãŸã¯ layered (å±¤çŠ¶)')
    parser.add_argument('--embed_dim', type=int, default=128, help='åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ')
    parser.add_argument('--neurons', type=int, default=256, 
                        help='ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (brainãƒ¢ãƒ¼ãƒ‰) ã¾ãŸã¯éš ã‚Œå±¤æ¬¡å…ƒ (layeredãƒ¢ãƒ¼ãƒ‰)')
    parser.add_argument('--heads', type=int, default=4, help='ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•° (layeredãƒ¢ãƒ¼ãƒ‰ã®ã¿)')
    parser.add_argument('--layers', type=int, default=3, help='ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•° (layeredãƒ¢ãƒ¼ãƒ‰ã®ã¿)')
    parser.add_argument('--epochs', type=int, default=50, help='ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch_size', type=int, default=16, help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ç¿’ç‡')
    parser.add_argument('--output_dir', type=str, default='.', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    args = parser.parse_args()
    
    # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦å­¦ç¿’
    if args.mode == 'brain':
        model, tokenizer, config = train_brain_model(
            num_neurons=args.neurons,
            embed_dim=args.embed_dim,
            epochs=args.epochs,
            batch_size=args.batch_size,
            lr=args.lr,
        )
    else:  # layered
        model, tokenizer, config = train_layered_model(
            embed_dim=args.embed_dim,
            hidden_dim=args.neurons,
            num_heads=args.heads,
            num_layers=args.layers,
            epochs=args.epochs,
            batch_size=args.batch_size,
            lr=args.lr,
        )
    
    # ãƒ‡ãƒã‚¤ã‚¹
    device = get_device()
    
    # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    test_generation(model, tokenizer, args.mode, device)
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    export_model(model, tokenizer, args.mode, args.output_dir)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ å®Œäº†ï¼")
    print("=" * 60)
    print(f"\nãƒ¢ãƒ¼ãƒ‰: {args.mode.upper()}")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. neuroq_*_model.pt ã¨ neuroq_tokenizer.json ã‚’ GitHub ã«ãƒ—ãƒƒã‚·ãƒ¥")
    print("2. RunPod Serverless Endpoint ã‚’ä½œæˆ")
    print("3. API ã§å‘¼ã³å‡ºã—")
    print(f"\n   curl ... -d '{{\"input\": {{\"prompt\": \"...\", \"mode\": \"{args.mode}\"}}}}'")


if __name__ == "__main__":
    main()
