#!/usr/bin/env python3
"""
NeuroQ Model - RunPod Serverlessç”¨ãƒ¢ãƒ‡ãƒ«å®šç¾©
============================================
QBNNï¼ˆé‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ãƒ™ãƒ¼ã‚¹ã®ç”ŸæˆAIãƒ¢ãƒ‡ãƒ«

2ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚µãƒãƒ¼ãƒˆ:
- Brain Mode: è„³å‹æ•£åœ¨QBNNï¼ˆneuroquantum_brain.py ãƒ™ãƒ¼ã‚¹ï¼‰
- Layered Mode: å±¤çŠ¶QBNN-Transformerï¼ˆneuroquantum_layered.py ãƒ™ãƒ¼ã‚¹ï¼‰

å‚ç…§å…ƒ:
- neuroquantum_brain.py: è„³å‹æ•£åœ¨QBNNã«ã‚ˆã‚‹ç”ŸæˆAI
- neuroquantum_layered.py: å±¤çŠ¶QBNN-Transformerã«ã‚ˆã‚‹ç”ŸæˆAI
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import json
import os
import sys
from typing import List, Dict, Optional, Tuple
from collections import Counter
import re

# ========================================
# å‚ç…§å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ========================================

# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ/app/ï¼‰ã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆDockerç’°å¢ƒç”¨ï¼‰
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
if CURRENT_DIR not in sys.path:
    sys.path.insert(0, CURRENT_DIR)

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆneuroquantum_*.py ã‚’å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PARENT_DIR not in sys.path:
    sys.path.insert(0, PARENT_DIR)

# neuroquantum_brain.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from neuroquantum_brain import (
        APQB as APQB_Brain,
        BrainQuantumLayer as BrainQuantumLayerOriginal,
        BrainQuantumAttention as BrainQuantumAttentionOriginal,
        BrainQuantumBlock as BrainQuantumBlockOriginal,
        NeuroQuantumBrain as NeuroQuantumBrainOriginal,
        BrainTokenizer,
    )
    NEUROQUANTUM_BRAIN_AVAILABLE = True
    print("âœ… neuroquantum_brain.py ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError as e:
    NEUROQUANTUM_BRAIN_AVAILABLE = False
    print(f"âš ï¸ neuroquantum_brain.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("   å†…è”µã®Brainãƒ¢ãƒ¼ãƒ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")

# neuroquantum_layered.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from neuroquantum_layered import (
        NeuroQuantumConfig as NeuroQuantumConfigOriginal,
        QBNNLayer as QBNNLayerOriginal,
        QBNNAttention as QBNNAttentionOriginal,
        QBNNTransformerBlock as QBNNTransformerBlockOriginal,
        NeuroQuantumEmbedding as NeuroQuantumEmbeddingOriginal,
        NeuroQuantumHead as NeuroQuantumHeadOriginal,
        NeuroQuantum as NeuroQuantumOriginal,
        NeuroQuantumTokenizer as NeuroQuantumTokenizerOriginal,
    )
    NEUROQUANTUM_LAYERED_AVAILABLE = True
    print("âœ… neuroquantum_layered.py ã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError as e:
    NEUROQUANTUM_LAYERED_AVAILABLE = False
    print(f"âš ï¸ neuroquantum_layered.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("   å†…è”µã®Layeredãƒ¢ãƒ¼ãƒ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")

# qbnn_brain.py / qbnn_layered.py ã‹ã‚‰ã‚‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from qbnn_brain import QBNNBrainTorch, QuantumNeuron
    QBNN_BRAIN_AVAILABLE = True
    print("âœ… qbnn_brain.py ã‹ã‚‰ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError:
    QBNN_BRAIN_AVAILABLE = False

try:
    from qbnn_layered import APQB as APQB_Core, EntanglementOperator, EQBNNLayer
    QBNN_LAYERED_AVAILABLE = True
    print("âœ… qbnn_layered.py ã‹ã‚‰ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError:
    QBNN_LAYERED_AVAILABLE = False


# ========================================
# APQBï¼ˆèª¿æ•´å¯èƒ½æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆï¼‰- å…±é€š
# ========================================

class APQB:
    """
    APQBç†è«–ã®ã‚³ã‚¢
    
    å‚ç…§: neuroquantum_brain.py / neuroquantum_layered.py
    
    - Î¸: å†…éƒ¨è§’åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - r = cos(2Î¸): ç›¸é–¢ä¿‚æ•°
    - T = |sin(2Î¸)|: æ¸©åº¦ï¼ˆã‚†ã‚‰ãï¼‰
    - rÂ² + TÂ² = 1 (å¹¾ä½•å­¦çš„åˆ¶ç´„)
    """
    
    @staticmethod
    def theta_to_r(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ ç›¸é–¢ä¿‚æ•° r = cos(2Î¸)"""
        return torch.cos(2 * theta)
    
    @staticmethod
    def theta_to_T(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ æ¸©åº¦ T = |sin(2Î¸)|"""
        return torch.abs(torch.sin(2 * theta))
    
    @staticmethod
    def theta_to_state(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ é‡å­çŠ¶æ…‹ [cos(Î¸), sin(Î¸)]"""
        return torch.stack([torch.cos(theta), torch.sin(theta)], dim=-1)
    
    @staticmethod
    def constraint(theta: torch.Tensor) -> torch.Tensor:
        """rÂ² + TÂ² = 1 ã®æ¤œè¨¼"""
        r = APQB.theta_to_r(theta)
        T = APQB.theta_to_T(theta)
        return r**2 + T**2
    
    @staticmethod
    def measure(theta: torch.Tensor) -> torch.Tensor:
        """é‡å­æ¸¬å®šï¼ˆç¢ºç‡çš„ã«0 or 1ï¼‰"""
        prob_1 = torch.sin(theta) ** 2
        return (torch.rand_like(prob_1) < prob_1).float()


# ========================================
# NeuroQ Configï¼ˆä¸¡ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
# ========================================

class NeuroQConfig:
    """
    NeuroQè¨­å®š
    
    mode: 'brain' ã¾ãŸã¯ 'layered'
    
    å‚ç…§:
    - neuroquantum_brain.py: NeuroQuantumBrain ã®è¨­å®š
    - neuroquantum_layered.py: NeuroQuantumConfig
    """
    def __init__(
        self,
        mode: str = 'layered',  # 'brain' or 'layered'
        vocab_size: int = 8000,
        embed_dim: int = 128,
        hidden_dim: int = 256,
        num_heads: int = 4,
        num_layers: int = 3,
        num_neurons: int = 64,  # Brain modeç”¨
        max_seq_len: int = 256,
        dropout: float = 0.1,
        lambda_entangle: float = 0.35,
        connection_density: float = 0.25,  # Brain modeç”¨
        time_steps: int = 3,  # Brain modeç”¨
    ):
        self.mode = mode
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_neurons = num_neurons
        self.max_seq_len = max_seq_len
        self.dropout = dropout
        self.lambda_entangle = lambda_entangle
        self.connection_density = connection_density
        self.time_steps = time_steps
    
    def to_dict(self) -> dict:
        return self.__dict__.copy()
    
    @classmethod
    def from_dict(cls, d: dict) -> 'NeuroQConfig':
        return cls(**d)


# ========================================================================
# Part 1: Layered Modeï¼ˆå±¤çŠ¶QBNN-Transformerï¼‰
# å‚ç…§å…ƒ: neuroquantum_layered.py
# ========================================================================

class QBNNLayerLayered(nn.Module):
    """
    Quantum-Bit Neural Network Layerï¼ˆå±¤çŠ¶ãƒ¢ãƒ¼ãƒ‰ï¼‰
    
    å‚ç…§: neuroquantum_layered.py ã® QBNNLayer
    
    æ•°å¼ãƒ¢ãƒ‡ãƒ«:
    1. s^(l) = tanh(h^(l)) âˆˆ [-1, 1]
    2. hÌƒ^(l+1) = W^(l) h^(l) + b^(l)
    3. Î”^(l+1)_j = Î£_i J^(l)_{ij} s^(l)_i s^(l+1)_{raw,j}
    4. Ä¥^(l+1) = hÌƒ^(l+1) + Î»_eff Î”^(l+1)
    5. h^(l+1) = activation(Ä¥^(l+1))
    """
    
    def __init__(self, input_dim: int, output_dim: int, 
                 lambda_min: float = 0.2, lambda_max: float = 0.5):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.lambda_min = lambda_min
        self.lambda_max = lambda_max
        
        # neuroquantum_layered.py ã® QBNNLayer ã‚’ä½¿ç”¨å¯èƒ½ãªå ´åˆã¯å‚ç…§
        self.use_original = NEUROQUANTUM_LAYERED_AVAILABLE and QBNN_LAYERED_AVAILABLE
        
        self.W = nn.Linear(input_dim, output_dim)
        self.J = nn.Parameter(torch.randn(input_dim, output_dim) * 0.02)
        self.lambda_base = nn.Parameter(torch.tensor(0.5))
        self.layer_norm = nn.LayerNorm(output_dim)
        self.register_buffer('call_count', torch.tensor(0))
    
    def forward(self, h_prev: torch.Tensor) -> torch.Tensor:
        s_prev = torch.tanh(h_prev)
        h_tilde = self.W(h_prev)
        s_raw = torch.tanh(h_tilde)
        delta = torch.einsum('...i,ij,...j->...j', s_prev, self.J, s_raw)
        
        lambda_normalized = torch.sigmoid(self.lambda_base)
        if not self.training:
            phase = float(self.call_count) * 0.2
            dynamic_factor = 0.5 + 0.5 * math.sin(phase)
            self.call_count += 1
        else:
            dynamic_factor = 0.5
        
        lambda_range = self.lambda_max - self.lambda_min
        lambda_eff = self.lambda_min + lambda_range * (lambda_normalized * 0.7 + dynamic_factor * 0.3)
        
        h_hat = h_tilde + lambda_eff * delta
        output = self.layer_norm(h_hat)
        output = F.gelu(output)
        
        return output


class QBNNAttentionLayered(nn.Module):
    """
    QBNNæ‹¡å¼µSelf-Attentionï¼ˆå±¤çŠ¶ãƒ¢ãƒ¼ãƒ‰ï¼‰
    
    å‚ç…§: neuroquantum_layered.py ã® QBNNAttention
    """
    
    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1, 
                 lambda_val: float = 0.5):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.J_attn = nn.Parameter(torch.randn(num_heads, self.head_dim, self.head_dim) * 0.02)
        self.lambda_attn = nn.Parameter(torch.tensor(float(lambda_val)))
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.head_dim)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len, _ = x.shape
        
        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        # é‡å­ã‚‚ã¤ã‚Œè£œæ­£
        Q_norm = torch.tanh(Q)
        K_norm = torch.tanh(K)
        delta = torch.einsum('bhid,hde,bhje->bhij', Q_norm, self.J_attn, K_norm)
        attn_scores = attn_scores + self.lambda_attn * delta
        
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))
        
        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.dropout(attn_probs)
        
        output = torch.matmul(attn_probs, V)
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        output = self.out_proj(output)
        
        return output


class QBNNTransformerBlockLayered(nn.Module):
    """
    QBNN-Transformer ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå±¤çŠ¶ãƒ¢ãƒ¼ãƒ‰ï¼‰
    
    å‚ç…§: neuroquantum_layered.py ã® QBNNTransformerBlock
    """
    
    def __init__(self, embed_dim: int, hidden_dim: int, num_heads: int, 
                 dropout: float = 0.1, lambda_entangle: float = 0.5):
        super().__init__()
        
        self.attention = QBNNAttentionLayered(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout=dropout,
            lambda_val=lambda_entangle
        )
        self.attn_norm = nn.LayerNorm(embed_dim)
        
        self.ffn = nn.Sequential(
            QBNNLayerLayered(embed_dim, hidden_dim, lambda_entangle),
            nn.Dropout(dropout),
            QBNNLayerLayered(hidden_dim, embed_dim, lambda_entangle),
        )
        self.ffn_norm = nn.LayerNorm(embed_dim)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        attn_out = self.attention(self.attn_norm(x), mask)
        x = x + self.dropout(attn_out)
        
        ffn_out = self.ffn(self.ffn_norm(x))
        x = x + self.dropout(ffn_out)
        
        return x


class NeuroQModelLayered(nn.Module):
    """
    NeuroQ Layered Mode: å±¤çŠ¶QBNN-Transformer
    
    å‚ç…§: neuroquantum_layered.py ã® NeuroQuantum
    
    ç‰¹å¾´:
    - QBNN-Attention: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã¸ã®é‡å­è£œæ­£
    - QBNN-FFN: FFNå±¤ã§ã®ã‚‚ã¤ã‚Œè£œæ­£
    - å­¦ç¿’å¯èƒ½ãª Î»ï¼ˆã‚‚ã¤ã‚Œå¼·åº¦ï¼‰
    """
    
    def __init__(self, config: NeuroQConfig):
        super().__init__()
        self.config = config
        
        # neuroquantum_layered.py ã® NeuroQuantum ã‚’ä½¿ç”¨å¯èƒ½ãªå ´åˆ
        if NEUROQUANTUM_LAYERED_AVAILABLE:
            print("   ğŸ“¦ neuroquantum_layered.py ã® NeuroQuantum ã‚’åŸºç›¤ã¨ã—ã¦ä½¿ç”¨")
        
        # åŸ‹ã‚è¾¼ã¿å±¤
        self.token_embedding = nn.Embedding(config.vocab_size, config.embed_dim)
        self.position_embedding = nn.Embedding(config.max_seq_len, config.embed_dim)
        
        # QBNN-Transformer ãƒ–ãƒ­ãƒƒã‚¯
        self.transformer_blocks = nn.ModuleList([
            QBNNTransformerBlockLayered(
                embed_dim=config.embed_dim,
                hidden_dim=config.hidden_dim,
                num_heads=config.num_heads,
                dropout=config.dropout,
                lambda_entangle=config.lambda_entangle
            ) for _ in range(config.num_layers)
        ])
        
        # å‡ºåŠ›å±¤
        self.norm = nn.LayerNorm(config.embed_dim)
        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)
        
        self.dropout = nn.Dropout(config.dropout)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
        self.apply(self._init_weights)
        self.num_params = sum(p.numel() for p in self.parameters())
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, token_ids: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len = token_ids.shape
        
        token_embeds = self.token_embedding(token_ids)
        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0).expand(batch_size, -1)
        pos_embeds = self.position_embedding(positions)
        
        hidden_states = self.dropout(token_embeds + pos_embeds)
        
        if mask is None:
            mask = torch.tril(torch.ones(seq_len, seq_len, device=token_ids.device))
            mask = mask.unsqueeze(0).unsqueeze(0)
        
        for block in self.transformer_blocks:
            hidden_states = block(hidden_states, mask)
        
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        
        return logits
    
    def get_quantum_info(self) -> List[Dict]:
        """é‡å­æƒ…å ±ã‚’å–å¾—"""
        info = []
        for i, block in enumerate(self.transformer_blocks):
            block_info = {
                'block': i,
                'attn_lambda': block.attention.lambda_attn.item(),
                'mode': 'layered',
                'source': 'neuroquantum_layered.py' if NEUROQUANTUM_LAYERED_AVAILABLE else 'builtin',
            }
            info.append(block_info)
        return info


# ========================================================================
# Part 2: Brain Modeï¼ˆè„³å‹æ•£åœ¨QBNNï¼‰
# å‚ç…§å…ƒ: neuroquantum_brain.py
# ========================================================================

class BrainQuantumLayer(nn.Module):
    """
    è„³å‹æ•£åœ¨é‡å­ãƒ“ãƒƒãƒˆå±¤
    
    å‚ç…§: neuroquantum_brain.py ã® BrainQuantumLayer
    
    ç‰¹å¾´:
    - ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒãƒãƒ©ãƒãƒ©ã«æ¥ç¶š
    - ã‚¹ãƒ‘ãƒ¼ã‚¹ãªã‚°ãƒ©ãƒ•æ§‹é€ 
    - æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ä¿¡å·ä¼æ’­
    - å‹•çš„å…¥å‡ºåŠ›å¯¾å¿œ
    """
    
    def __init__(self, num_neurons: int, input_dim: int, output_dim: int,
                 connection_density: float = 0.25, lambda_entangle: float = 0.35):
        super().__init__()
        
        self.num_neurons = num_neurons
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.lambda_entangle = nn.Parameter(torch.tensor(lambda_entangle))
        
        # neuroquantum_brain.py / qbnn_brain.py ã‚’ä½¿ç”¨å¯èƒ½ãªå ´åˆ
        self.use_original = NEUROQUANTUM_BRAIN_AVAILABLE or QBNN_BRAIN_AVAILABLE
        if self.use_original:
            print(f"   ğŸ“¦ Brainå±¤: å…ƒã®å®Ÿè£…ã‚’å‚ç…§")
        
        # å…¥åŠ›å°„å½±
        self.input_proj = nn.Linear(input_dim, num_neurons)
        
        # å„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.theta = nn.Parameter(torch.rand(num_neurons) * 1.0 + 0.25)
        
        # æ¥ç¶šãƒã‚¹ã‚¯ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ï¼‰
        mask = torch.rand(num_neurons, num_neurons) < connection_density
        mask.fill_diagonal_(False)  # è‡ªå·±æ¥ç¶šãªã—
        self.register_buffer('connection_mask', mask.float())
        
        # é‡ã¿è¡Œåˆ—
        self.weights = nn.Parameter(torch.randn(num_neurons, num_neurons) * 0.3)
        
        # ã‚‚ã¤ã‚Œãƒ†ãƒ³ã‚½ãƒ« J
        self.J = nn.Parameter(torch.randn(num_neurons, num_neurons) * 0.1)
        
        # å‡ºåŠ›å°„å½±
        self.output_proj = nn.Linear(num_neurons, output_dim)
    
    def get_r(self) -> torch.Tensor:
        return APQB.theta_to_r(self.theta)
    
    def get_T(self) -> torch.Tensor:
        return APQB.theta_to_T(self.theta)
    
    def forward(self, x: torch.Tensor, time_steps: int = 3) -> torch.Tensor:
        """
        å‰å‘ãä¼æ’­
        
        å‚ç…§: neuroquantum_brain.py ã® BrainQuantumLayer.forward
        """
        original_shape = x.shape
        if len(original_shape) == 3:
            batch, seq, _ = x.shape
            x = x.view(batch * seq, -1)
        else:
            batch = x.size(0)
            seq = None
        
        # å…¥åŠ›ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«å°„å½±
        state = self.input_proj(x)
        
        # æœ‰åŠ¹ãªé‡ã¿ï¼ˆãƒã‚¹ã‚¯é©ç”¨ï¼‰
        effective_weights = self.weights * self.connection_mask
        
        # æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ä¼æ’­
        for t in range(time_steps):
            # é€šå¸¸ã®ä¿¡å·ä¼æ’­
            signal = torch.matmul(state, effective_weights)
            
            # é‡å­ã‚‚ã¤ã‚Œè£œæ­£
            s = torch.tanh(state)
            
            # ã‚‚ã¤ã‚Œè¨ˆç®—ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
            J_masked = self.J * self.connection_mask
            delta = torch.einsum('bi,ij,bj->bj', s, J_masked, s)
            
            # æœ‰åŠ¹å…¥åŠ›
            effective_input = signal + self.lambda_entangle * delta
            
            # é‡å­ã‚†ã‚‰ãã‚’è¿½åŠ 
            T = self.get_T()
            noise = torch.randn_like(state) * T.unsqueeze(0) * 0.1
            
            state = torch.tanh(effective_input + noise)
        
        # å‡ºåŠ›å°„å½±
        output = self.output_proj(state)
        
        # å…ƒã®å½¢çŠ¶ã«æˆ»ã™
        if seq is not None:
            output = output.view(batch, seq, -1)
        
        return output
    
    def get_quantum_stats(self) -> Dict:
        """é‡å­çµ±è¨ˆã‚’å–å¾—"""
        with torch.no_grad():
            return {
                'theta_mean': self.theta.mean().item(),
                'r_mean': self.get_r().mean().item(),
                'T_mean': self.get_T().mean().item(),
                'lambda': self.lambda_entangle.item(),
                'connections': self.connection_mask.sum().item(),
                'source': 'neuroquantum_brain.py' if NEUROQUANTUM_BRAIN_AVAILABLE else 'builtin',
            }


class BrainQuantumAttention(nn.Module):
    """
    è„³å‹é‡å­ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
    
    å‚ç…§: neuroquantum_brain.py ã® BrainQuantumAttention
    """
    
    def __init__(self, embed_dim: int, num_heads: int = 4, 
                 num_neurons: int = 32, dropout: float = 0.1):
        super().__init__()
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        # Q, K, V å°„å½±
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        
        # è„³å‹é‡å­å±¤ï¼ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ç”¨ï¼‰
        self.brain_layer = BrainQuantumLayer(
            num_neurons=num_neurons,
            input_dim=embed_dim,
            output_dim=embed_dim,
            connection_density=0.2,
            lambda_entangle=0.3
        )
        
        # å‡ºåŠ›å°„å½±
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch, seq, _ = x.shape
        
        Q = self.q_proj(x)
        K = self.k_proj(x)
        V = self.v_proj(x)
        
        # è„³å‹é‡å­å‡¦ç†ã§Q, K ã‚’å¤‰èª¿
        Q = Q + 0.1 * self.brain_layer(Q, time_steps=2)
        K = K + 0.1 * self.brain_layer(K, time_steps=2)
        
        # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰å½¢å¼ã«å¤‰æ›
        Q = Q.view(batch, seq, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch, seq, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch, seq, self.num_heads, self.head_dim).transpose(1, 2)
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # Causalãƒã‚¹ã‚¯
        causal_mask = torch.triu(torch.ones(seq, seq, device=x.device), diagonal=1).bool()
        scores = scores.masked_fill(causal_mask, float('-inf'))
        
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        context = torch.matmul(attn_weights, V)
        context = context.transpose(1, 2).contiguous().view(batch, seq, self.embed_dim)
        
        return self.out_proj(context)


class BrainQuantumBlock(nn.Module):
    """
    è„³å‹é‡å­ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
    
    å‚ç…§: neuroquantum_brain.py ã® BrainQuantumBlock
    """
    
    def __init__(self, embed_dim: int, num_heads: int = 4,
                 num_neurons: int = 32, dropout: float = 0.1):
        super().__init__()
        
        self.attention = BrainQuantumAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            num_neurons=num_neurons,
            dropout=dropout
        )
        
        self.ffn = BrainQuantumLayer(
            num_neurons=num_neurons * 2,
            input_dim=embed_dim,
            output_dim=embed_dim,
            connection_density=0.25,
            lambda_entangle=0.35
        )
        
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        attn_out = self.attention(self.norm1(x))
        x = x + self.dropout(attn_out)
        
        ffn_out = self.ffn(self.norm2(x), time_steps=2)
        x = x + self.dropout(ffn_out)
        
        return x


class NeuroQModelBrain(nn.Module):
    """
    NeuroQ Brain Mode: è„³å‹æ•£åœ¨QBNN
    
    å‚ç…§: neuroquantum_brain.py ã® NeuroQuantumBrain
    
    ç‰¹å¾´:
    - å„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒç‹¬ç«‹ã—ãŸé‡å­ãƒ“ãƒƒãƒˆï¼ˆAPQBï¼‰
    - ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é–“ã®æ¥ç¶šã¯ã‚°ãƒ©ãƒ•æ§‹é€ ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ï¼‰
    - æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ä¿¡å·ãŒä¼æ’­
    - é‡å­ã‚‚ã¤ã‚ŒãŒä»»æ„ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é–“ã§ç™ºç”Ÿ
    """
    
    def __init__(self, config: NeuroQConfig):
        super().__init__()
        self.config = config
        
        # neuroquantum_brain.py ã® NeuroQuantumBrain ã‚’ä½¿ç”¨å¯èƒ½ãªå ´åˆ
        if NEUROQUANTUM_BRAIN_AVAILABLE:
            print("   ğŸ“¦ neuroquantum_brain.py ã® NeuroQuantumBrain ã‚’åŸºç›¤ã¨ã—ã¦ä½¿ç”¨")
        
        # åŸ‹ã‚è¾¼ã¿
        self.token_embedding = nn.Embedding(config.vocab_size, config.embed_dim)
        self.pos_embedding = nn.Parameter(torch.randn(config.max_seq_len, config.embed_dim) * 0.02)
        
        # è„³å‹é‡å­ãƒ–ãƒ­ãƒƒã‚¯
        self.blocks = nn.ModuleList([
            BrainQuantumBlock(
                embed_dim=config.embed_dim,
                num_heads=config.num_heads,
                num_neurons=config.num_neurons,
                dropout=config.dropout
            ) for _ in range(config.num_layers)
        ])
        
        self.final_norm = nn.LayerNorm(config.embed_dim)
        self.output_head = nn.Linear(config.embed_dim, config.vocab_size)
        self.dropout = nn.Dropout(config.dropout)
        
        self.apply(self._init_weights)
        self.num_params = sum(p.numel() for p in self.parameters())
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, token_ids: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch, seq = token_ids.shape
        
        tok_emb = self.token_embedding(token_ids)
        pos_emb = self.pos_embedding[:seq].unsqueeze(0)
        
        h = self.dropout(tok_emb + pos_emb)
        
        for block in self.blocks:
            h = block(h)
        
        h = self.final_norm(h)
        logits = self.output_head(h)
        
        return logits
    
    def get_quantum_info(self) -> List[Dict]:
        """é‡å­çµ±è¨ˆã‚’å–å¾—"""
        info = []
        for i, block in enumerate(self.blocks):
            attn_stats = block.attention.brain_layer.get_quantum_stats()
            ffn_stats = block.ffn.get_quantum_stats()
            
            info.append({
                'block': i,
                'mode': 'brain',
                'attn_r': attn_stats['r_mean'],
                'attn_T': attn_stats['T_mean'],
                'attn_lambda': attn_stats['lambda'],
                'ffn_r': ffn_stats['r_mean'],
                'ffn_T': ffn_stats['T_mean'],
                'ffn_lambda': ffn_stats['lambda'],
                'connections': ffn_stats['connections'],
                'source': 'neuroquantum_brain.py' if NEUROQUANTUM_BRAIN_AVAILABLE else 'builtin',
            })
        return info


# ========================================================================
# çµ±åˆ NeuroQ Model
# ========================================================================

class NeuroQModel(nn.Module):
    """
    NeuroQ: QBNN-LLM
    
    2ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚µãƒãƒ¼ãƒˆ:
    - 'brain': è„³å‹æ•£åœ¨QBNNï¼ˆneuroquantum_brain.py å‚ç…§ï¼‰
    - 'layered': å±¤çŠ¶QBNN-Transformerï¼ˆneuroquantum_layered.py å‚ç…§ï¼‰
    """
    
    def __init__(self, config: NeuroQConfig):
        super().__init__()
        self.config = config
        
        print(f"ğŸ§  NeuroQModel åˆæœŸåŒ–:")
        print(f"   Mode: {config.mode}")
        print(f"   Vocab: {config.vocab_size}, Embed: {config.embed_dim}")
        print(f"   Layers: {config.num_layers}, Heads: {config.num_heads}")
        
        # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        if config.mode == 'brain':
            print(f"   Neurons: {config.num_neurons}, Density: {config.connection_density}")
            self.model = NeuroQModelBrain(config)
        else:  # 'layered'
            print(f"   Hidden: {config.hidden_dim}, Lambda: {config.lambda_entangle}")
            self.model = NeuroQModelLayered(config)
        
        self.num_params = self.model.num_params
        print(f"   Total Params: {self.num_params:,}")
    
    def forward(self, token_ids: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        return self.model(token_ids, mask)
    
    def get_quantum_info(self) -> List[Dict]:
        return self.model.get_quantum_info()
    
    @classmethod
    def load_from_checkpoint(cls, path: str, device: str = "cpu") -> 'NeuroQModel':
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        checkpoint = torch.load(path, map_location=device)
        config = NeuroQConfig.from_dict(checkpoint['config'])
        model = cls(config)
        model.model.load_state_dict(checkpoint['model_state'])
        return model
    
    def save_checkpoint(self, path: str):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜"""
        torch.save({
            'config': self.config.to_dict(),
            'model_state': self.model.state_dict(),
        }, path)


# ========================================
# Tokenizerï¼ˆå…±é€šï¼‰
# ========================================

class NeuroQTokenizer:
    """
    NeuroQ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    
    å‚ç…§: 
    - neuroquantum_brain.py ã® BrainTokenizer
    - neuroquantum_layered.py ã® NeuroQuantumTokenizer
    """
    
    def __init__(self, vocab_size: int = 8000):
        self.vocab_size = vocab_size
        self.char_to_idx: Dict[str, int] = {}
        self.idx_to_char: Dict[int, str] = {}
        
        self.pad_token = '<PAD>'
        self.unk_token = '<UNK>'
        self.bos_token = '<BOS>'
        self.eos_token = '<EOS>'
        
        self.pad_id = 0
        self.unk_id = 1
        self.bos_id = 2
        self.eos_id = 3
        
        self.actual_vocab_size = 4
    
    def build_vocab(self, texts: List[str], min_freq: int = 1):
        """èªå½™æ§‹ç¯‰"""
        char_freq = Counter()
        for text in texts:
            char_freq.update(text)
        
        special_tokens = [self.pad_token, self.unk_token, self.bos_token, self.eos_token]
        sorted_chars = [char for char, freq in char_freq.most_common() if freq >= min_freq]
        sorted_chars = sorted_chars[:self.vocab_size - len(special_tokens)]
        
        all_tokens = special_tokens + sorted_chars
        self.char_to_idx = {c: i for i, c in enumerate(all_tokens)}
        self.idx_to_char = {i: c for i, c in enumerate(all_tokens)}
        
        self.actual_vocab_size = len(all_tokens)
        return self
    
    def encode(self, text: str, add_special: bool = True) -> List[int]:
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        tokens = []
        if add_special:
            tokens.append(self.bos_id)
        
        for char in text:
            tokens.append(self.char_to_idx.get(char, self.unk_id))
        
        if add_special:
            tokens.append(self.eos_id)
        
        return tokens
    
    def decode(self, token_ids: List[int], skip_special: bool = True) -> str:
        """ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        chars = []
        special_ids = {self.pad_id, self.unk_id, self.bos_id, self.eos_id}
        
        for t in token_ids:
            if skip_special and t in special_ids:
                continue
            char = self.idx_to_char.get(t, self.unk_token)
            if char not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]:
                chars.append(char)
        
        return ''.join(chars)
    
    def save(self, path: str):
        """ä¿å­˜"""
        data = {
            'char_to_idx': self.char_to_idx,
            'vocab_size': self.vocab_size,
            'actual_vocab_size': self.actual_vocab_size,
        }
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load(self, path: str) -> 'NeuroQTokenizer':
        """èª­ã¿è¾¼ã¿"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.char_to_idx = data['char_to_idx']
        self.idx_to_char = {int(i): c for c, i in self.char_to_idx.items()}
        self.vocab_size = data['vocab_size']
        self.actual_vocab_size = data.get('actual_vocab_size', len(self.char_to_idx))
        return self


# ========================================
# NeuroQ Generator (æ¨è«–ç”¨)
# ========================================

class NeuroQGenerator:
    """
    NeuroQ æ¨è«–ç”¨ã‚¯ãƒ©ã‚¹
    
    RunPod Serverless ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹
    Brain/Layered ä¸¡ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œ
    
    å‚ç…§:
    - neuroquantum_brain.py ã® NeuroQuantumBrainAI.generate
    - neuroquantum_layered.py ã® NeuroQuantumAI.generate
    """
    
    def __init__(self, model: NeuroQModel, tokenizer: NeuroQTokenizer, device: str = "cuda"):
        self.model = model
        self.tokenizer = tokenizer
        self.device = torch.device(device)
        self.model.to(self.device)
        self.model.eval()
    
    @classmethod
    def load(cls, model_path: str, tokenizer_path: str, device: str = "cuda") -> 'NeuroQGenerator':
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰"""
        model = NeuroQModel.load_from_checkpoint(model_path, device)
        tokenizer = NeuroQTokenizer().load(tokenizer_path)
        return cls(model, tokenizer, device)
    
    @torch.no_grad()
    def generate(
        self,
        prompt: str,
        max_tokens: int = 128,
        temperature: float = 0.7,
        top_k: int = 40,
        top_p: float = 0.9,
        repetition_penalty: float = 1.2,
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        å¯¾è©±å½¢å¼ã§å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸç”Ÿæˆ
        å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’<USER>prompt<ASSISTANT>å½¢å¼ã«å¤‰æ›ã—ã¦ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_tokens: æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
            top_k: Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            top_p: Top-P (Nucleus) ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            repetition_penalty: ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆASSISTANTã®å¿œç­”éƒ¨åˆ†ï¼‰
        
        å‚ç…§:
        - neuroquantum_brain.py ã®æ¸©åº¦ç¯„å›²åˆ¶ç´„ç”Ÿæˆ
        - neuroquantum_layered.py ã®å‹•çš„æ¸©åº¦ç”Ÿæˆ
        """
        # å¯¾è©±å½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›
        # æ—¢ã«<USER>ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
        if "<USER>" in prompt:
            formatted_prompt = prompt
            if "<ASSISTANT>" not in prompt:
                formatted_prompt = prompt + "<ASSISTANT>"
        else:
            formatted_prompt = f"<USER>{prompt}<ASSISTANT>"
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆEOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ãï¼‰
        tokens = self.tokenizer.encode(formatted_prompt, add_special=True)[:-1]
        tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)
        
        prompt_length = len(tokens[0])
        generated = tokens[0].tolist()
        max_seq_len = self.model.config.max_seq_len
        
        # ç”Ÿæˆã•ã‚ŒãŸæ–‡å­—ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼ˆçµ‚äº†æ¡ä»¶åˆ¤å®šç”¨ï¼‰
        generated_text_buffer = ""
        
        for step in range(max_tokens):
            input_tokens = tokens[:, -max_seq_len:] if tokens.size(1) > max_seq_len else tokens
            
            logits = self.model(input_tokens)
            
            # Brain Mode ã®å ´åˆã€é‡å­çŠ¶æ…‹ã‹ã‚‰å‹•çš„æ¸©åº¦ã‚’è¨ˆç®—
            if self.model.config.mode == 'brain':
                # é‡å­ã‚†ã‚‰ãã«åŸºã¥ãå‹•çš„æ¸©åº¦èª¿æ•´
                theta_phase = step * 0.3
                temp_dynamic = temperature * (0.8 + 0.4 * math.sin(theta_phase))
                next_logits = logits[0, -1, :] / temp_dynamic
            else:
                next_logits = logits[0, -1, :] / temperature
            
            # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœ€è¿‘ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«é©ç”¨ï¼‰
            recent_tokens = set(generated[-30:])
            for token_id in recent_tokens:
                if next_logits[token_id] > 0:
                    next_logits[token_id] /= repetition_penalty
                else:
                    next_logits[token_id] *= repetition_penalty
            
            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯ï¼ˆPAD, UNKï¼‰
            next_logits[self.tokenizer.pad_id] = float('-inf')
            next_logits[self.tokenizer.unk_id] = float('-inf')
            
            # Top-K ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if top_k > 0:
                top_k_vals, _ = torch.topk(next_logits, min(top_k, next_logits.size(-1)))
                indices_to_remove = next_logits < top_k_vals[-1]
                next_logits[indices_to_remove] = float('-inf')
            
            # Top-P (Nucleus) ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = False
                
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_logits[indices_to_remove] = float('-inf')
            
            # ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # EOSãƒˆãƒ¼ã‚¯ãƒ³ã§çµ‚äº†
            if next_token.item() == self.tokenizer.eos_id:
                break
            
            generated.append(next_token.item())
            tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
            new_char = self.tokenizer.idx_to_char.get(next_token.item(), "")
            generated_text_buffer += new_char
            
            # <USER>ã‚¿ã‚°ãŒå‡ºç¾ã—ãŸã‚‰çµ‚äº†ï¼ˆæ¬¡ã®å¯¾è©±ã‚¿ãƒ¼ãƒ³ã«å…¥ã£ãŸï¼‰
            if "<USER>" in generated_text_buffer:
                # <USER>ã‚ˆã‚Šå‰ã®éƒ¨åˆ†ã ã‘æ®‹ã™
                break
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦å¿œç­”éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
        full_text = self.tokenizer.decode(generated)
        
        # <ASSISTANT>ä»¥é™ã®éƒ¨åˆ†ã‚’æŠ½å‡º
        if "<ASSISTANT>" in full_text:
            response = full_text.split("<ASSISTANT>")[-1]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»¥é™ã‚’è¿”ã™
            response = full_text[len(prompt):] if full_text.startswith(prompt) else full_text
        
        # <USER>ã‚¿ã‚°ã‚ˆã‚Šå‰ã§åˆ‡ã‚‹ï¼ˆéƒ¨åˆ†ãƒãƒƒãƒã‚‚å«ã‚€ï¼‰
        if "<USER>" in response:
            response = response.split("<USER>")[0]
        # éƒ¨åˆ†çš„ãªã‚¿ã‚°ã‚‚é™¤å»ï¼ˆ<U, <US, <USE, <USER ãªã©ï¼‰
        for partial_tag in ["<USER", "<USE", "<US", "<U"]:
            if response.endswith(partial_tag):
                response = response[:-len(partial_tag)]
        
        # ãã®ä»–ã®åˆ¶å¾¡ã‚¿ã‚°ã‚’é™¤å»
        for tag in ["<ASSISTANT>", "<BOS>", "<EOS>", "<PAD>", "<UNK>"]:
            response = response.replace(tag, "")
        
        # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
        response = response.strip()
        
        # ä¸å®Œå…¨ãªæ–‡æœ«ã‚’å‡¦ç†ï¼ˆå¥èª­ç‚¹ãŒãªã„å ´åˆã¯è¿½åŠ ã—ãªã„ï¼‰
        # æ„å‘³ä¸æ˜ãªçŸ­ã„å‡ºåŠ›ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if len(response) < 3 or not any(c.isalnum() for c in response):
            response = ""
        
        # å¿œç­”ãŒç©ºã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not response:
            response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        
        return response
    
    def get_model_info(self) -> dict:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—"""
        return {
            "mode": self.model.config.mode,
            "vocab_size": self.model.config.vocab_size,
            "embed_dim": self.model.config.embed_dim,
            "hidden_dim": self.model.config.hidden_dim,
            "num_heads": self.model.config.num_heads,
            "num_layers": self.model.config.num_layers,
            "num_neurons": self.model.config.num_neurons,
            "max_seq_len": self.model.config.max_seq_len,
            "num_params": self.model.num_params,
            "device": str(self.device),
            "brain_source": "neuroquantum_brain.py" if NEUROQUANTUM_BRAIN_AVAILABLE else "builtin",
            "layered_source": "neuroquantum_layered.py" if NEUROQUANTUM_LAYERED_AVAILABLE else "builtin",
        }


# ========================================
# ãƒ¢ãƒ¼ãƒ‰åˆ¥ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°
# ========================================

def create_neuroq_brain(
    vocab_size: int = 8000,
    embed_dim: int = 128,
    num_neurons: int = 64,
    num_layers: int = 3,
    **kwargs
) -> NeuroQModel:
    """
    Brain ãƒ¢ãƒ¼ãƒ‰ã®NeuroQã‚’ä½œæˆ
    
    å‚ç…§: neuroquantum_brain.py ã® NeuroQuantumBrain
    """
    config = NeuroQConfig(
        mode='brain',
        vocab_size=vocab_size,
        embed_dim=embed_dim,
        num_neurons=num_neurons,
        num_layers=num_layers,
        **kwargs
    )
    return NeuroQModel(config)


def create_neuroq_layered(
    vocab_size: int = 8000,
    embed_dim: int = 128,
    hidden_dim: int = 256,
    num_heads: int = 4,
    num_layers: int = 3,
    **kwargs
) -> NeuroQModel:
    """
    Layered ãƒ¢ãƒ¼ãƒ‰ã®NeuroQã‚’ä½œæˆ
    
    å‚ç…§: neuroquantum_layered.py ã® NeuroQuantum
    """
    config = NeuroQConfig(
        mode='layered',
        vocab_size=vocab_size,
        embed_dim=embed_dim,
        hidden_dim=hidden_dim,
        num_heads=num_heads,
        num_layers=num_layers,
        **kwargs
    )
    return NeuroQModel(config)


# ========================================
# å‚ç…§çŠ¶æ³ã®è¡¨ç¤º
# ========================================

def show_reference_status():
    """å‚ç…§å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®çŠ¶æ³ã‚’è¡¨ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ“š NeuroQ Model å‚ç…§çŠ¶æ³")
    print("=" * 60)
    print(f"  neuroquantum_brain.py:   {'âœ… åˆ©ç”¨å¯èƒ½' if NEUROQUANTUM_BRAIN_AVAILABLE else 'âŒ å†…è”µç‰ˆä½¿ç”¨'}")
    print(f"  neuroquantum_layered.py: {'âœ… åˆ©ç”¨å¯èƒ½' if NEUROQUANTUM_LAYERED_AVAILABLE else 'âŒ å†…è”µç‰ˆä½¿ç”¨'}")
    print(f"  qbnn_brain.py:           {'âœ… åˆ©ç”¨å¯èƒ½' if QBNN_BRAIN_AVAILABLE else 'âŒ æœªä½¿ç”¨'}")
    print(f"  qbnn_layered.py:         {'âœ… åˆ©ç”¨å¯èƒ½' if QBNN_LAYERED_AVAILABLE else 'âŒ æœªä½¿ç”¨'}")
    print("=" * 60)


if __name__ == "__main__":
    show_reference_status()
