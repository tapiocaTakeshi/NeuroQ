#!/usr/bin/env python3
"""
NeuroQ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
====================================
äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨åˆæœŸåŒ–ã‚’æä¾›ã—ã¾ã™ã€‚

ä½¿ã„æ–¹:
    from neuroq_pretrained import load_pretrained_model

    model = load_pretrained_model(device='cuda')
"""

import torch
import os
import sys
from pathlib import Path


# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«è¨­å®š
DEFAULT_CONFIG = {
    'vocab_size': 8000,
    'embed_dim': 256,
    'hidden_dim': 512,
    'num_heads': 8,
    'num_layers': 6,
    'max_seq_len': 256,
    'dropout': 0.1,
    'lambda_entangle': 0.5,
}


def get_model_path():
    """
    äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—

    Returns:
        str: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ/home/user/ï¼‰ã‹ã‚‰æ¢ã™
    current_dir = Path(__file__).parent

    # å€™è£œãƒ‘ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
    candidate_paths = [
        current_dir / 'NeuroQ' / 'neuroq_pretrained.pt',
        current_dir / 'neuroq_pretrained.pt',
    ]

    for path in candidate_paths:
        if path.exists():
            return str(path)

    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’è¿”ã™
    return str(current_dir / 'NeuroQ' / 'neuroq_pretrained.pt')


def is_lfs_pointer_file(file_path):
    """
    Check if a file is a Git LFS pointer file.

    Args:
        file_path: Path to the file to check

    Returns:
        bool: True if the file is an LFS pointer, False otherwise
    """
    try:
        # LFS pointer files are typically very small (< 200 bytes)
        file_size = os.path.getsize(file_path)
        if file_size > 1024:
            return False

        # Read the first few bytes to check for LFS marker
        with open(file_path, 'rb') as f:
            header = f.read(50).decode('utf-8', errors='ignore')
            return header.startswith('version https://git-lfs.github.com/spec/')
    except:
        return False


def validate_checkpoint(checkpoint):
    """
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å¦¥å½“æ€§ã‚’æ¤œè¨¼

    Args:
        checkpoint: ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

    Returns:
        tuple: (is_valid, error_message)
    """
    if not isinstance(checkpoint, dict):
        return False, f"Invalid checkpoint format. Expected dict, got {type(checkpoint).__name__}"

    required_keys = ['config', 'model_state_dict']
    missing_keys = [key for key in required_keys if key not in checkpoint]

    if missing_keys:
        return False, f"Missing required keys: {missing_keys}"

    config = checkpoint['config']
    if not isinstance(config, dict):
        return False, f"Invalid config format. Expected dict, got {type(config).__name__}"

    required_config_keys = ['vocab_size', 'embed_dim', 'hidden_dim', 'num_heads', 'num_layers']
    missing_config_keys = [key for key in required_config_keys if key not in config]

    if missing_config_keys:
        return False, f"Missing required config keys: {missing_config_keys}"

    return True, None


def load_pretrained_model(model_path=None, device='cpu', verbose=True):
    """
    äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰

    Args:
        model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ¤œå‡ºï¼‰
        device: ãƒ‡ãƒã‚¤ã‚¹ï¼ˆ'cpu', 'cuda', 'mps'ï¼‰
        verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹

    Returns:
        tuple: (model, config, checkpoint) ã¾ãŸã¯ (None, None, None) if failed
    """
    try:
        # ãƒ‘ã‚¹ã®æ±ºå®š
        if model_path is None:
            model_path = get_model_path()

        if verbose:
            print(f"ğŸ“¦ Loading pretrained model from: {model_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(model_path):
            if verbose:
                print(f"âŒ Model file not found: {model_path}")
            return None, None, None

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = os.path.getsize(model_path)
        if verbose:
            print(f"ğŸ“Š File size: {file_size / (1024 * 1024):.2f} MB")

        # Check for Git LFS pointer file
        if file_size < 1024:  # 1KBæœªæº€
            if is_lfs_pointer_file(model_path):
                if verbose:
                    print(f"âŒ Git LFS pointer file detected ({file_size} bytes)")
                    print(f"")
                    print(f"   The model file has not been downloaded from Git LFS.")
                    print(f"   Please run one of the following:")
                    print(f"")
                    print(f"   1. Use the helper script:")
                    print(f"      $ python download_model.py")
                    print(f"")
                    print(f"   2. Pull from Git LFS manually:")
                    print(f"      $ git lfs install")
                    print(f"      $ git lfs pull")
                    print(f"")
                    print(f"   3. Use Docker with automatic LFS pull:")
                    print(f"      $ docker build \\")
                    print(f"          --build-arg GIT_REPO_URL=https://github.com/tapiocaTakeshi/NeuroQ.git \\")
                    print(f"          -t neuroq:latest .")
                    print(f"")
                return None, None, None
            else:
                if verbose:
                    print(f"âŒ File too small ({file_size} bytes). Possibly corrupted.")
                return None, None, None

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
        checkpoint = torch.load(model_path, map_location=device)

        # æ¤œè¨¼
        is_valid, error_msg = validate_checkpoint(checkpoint)
        if not is_valid:
            if verbose:
                print(f"âŒ Checkpoint validation failed: {error_msg}")
            return None, None, None

        # è¨­å®šã‚’å–å¾—
        config = checkpoint['config']

        if verbose:
            print(f"âœ… Model loaded successfully")
            print(f"   vocab_size: {config['vocab_size']}")
            print(f"   embed_dim: {config['embed_dim']}")
            print(f"   hidden_dim: {config['hidden_dim']}")
            print(f"   num_heads: {config['num_heads']}")
            print(f"   num_layers: {config['num_layers']}")

        # neuroquantum_layered ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        # Note: ã“ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯ã“ã“ã§è¡Œã†ï¼ˆé…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
        neuroq_module_path = Path(__file__).parent / 'NeuroQ' / 'neuroq-runpod'
        if str(neuroq_module_path) not in sys.path:
            sys.path.insert(0, str(neuroq_module_path))

        from neuroquantum_layered import NeuroQuantum, NeuroQuantumConfig

        # Configã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        model_config = NeuroQuantumConfig(
            vocab_size=config['vocab_size'],
            embed_dim=config['embed_dim'],
            hidden_dim=config['hidden_dim'],
            num_heads=config['num_heads'],
            num_layers=config['num_layers'],
            max_seq_len=config['max_seq_len'],
            dropout=config.get('dropout', 0.1),
            lambda_entangle=config.get('lambda_entangle', 0.5),
        )

        # ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
        model = NeuroQuantum(model_config).to(device)

        # ã‚¦ã‚§ã‚¤ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        if verbose:
            print(f"âœ… Model initialized on device: {device}")

        return model, config, checkpoint

    except Exception as e:
        if verbose:
            print(f"âŒ Error loading model: {e}")
            import traceback
            traceback.print_exc()
        return None, None, None


def create_model_from_config(config=None, device='cpu'):
    """
    è¨­å®šã‹ã‚‰æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆäº‹å‰å­¦ç¿’ãªã—ï¼‰

    Args:
        config: ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ï¼‰
        device: ãƒ‡ãƒã‚¤ã‚¹

    Returns:
        model: åˆæœŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
    """
    if config is None:
        config = DEFAULT_CONFIG.copy()

    # neuroquantum_layered ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    neuroq_module_path = Path(__file__).parent / 'NeuroQ' / 'neuroq-runpod'
    if str(neuroq_module_path) not in sys.path:
        sys.path.insert(0, str(neuroq_module_path))

    from neuroquantum_layered import NeuroQuantum, NeuroQuantumConfig

    model_config = NeuroQuantumConfig(
        vocab_size=config['vocab_size'],
        embed_dim=config['embed_dim'],
        hidden_dim=config['hidden_dim'],
        num_heads=config['num_heads'],
        num_layers=config['num_layers'],
        max_seq_len=config.get('max_seq_len', 256),
        dropout=config.get('dropout', 0.1),
        lambda_entangle=config.get('lambda_entangle', 0.5),
    )

    model = NeuroQuantum(model_config).to(device)
    model.eval()

    print(f"âœ… New model created with config: {config}")

    return model


if __name__ == "__main__":
    """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    print("=" * 60)
    print("ğŸ§  NeuroQ Pretrained Model Loader - Test")
    print("=" * 60)

    # ãƒ‡ãƒã‚¤ã‚¹ã®æ±ºå®š
    if torch.cuda.is_available():
        device = 'cuda'
        print(f"ğŸ® Using GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = 'cpu'
        print("ğŸ’» Using CPU")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model, config, checkpoint = load_pretrained_model(device=device)

    if model is not None:
        print("\nâœ… Test successful!")
        print(f"   Model type: {type(model).__name__}")
        print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")
    else:
        print("\nâŒ Test failed - model could not be loaded")
        print("   Falling back to default config...")
        model = create_model_from_config(device=device)

    print("=" * 60)
