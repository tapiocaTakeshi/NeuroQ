#!/usr/bin/env python3
"""
APQB Generative AI v2 - APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ä½¿ç”¨ã—ãŸç”ŸæˆAI

APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã‚’çµ„ã¿è¾¼ã‚“ã ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§
ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹é‡å­ç¢ºç‡çš„ç”ŸæˆAI

å¤§è¦æ¨¡ãƒˆãƒ¼ã‚¯ãƒ³å¯¾å¿œç‰ˆ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from collections import defaultdict, Counter
import re
import os

# ========== è¨­å®š ==========
DEFAULT_CONFIG = {
    'embed_dim': 128,        # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    'num_heads': 8,          # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
    'num_layers': 4,         # ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å±¤æ•°
    'seq_len': 128,          # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    'max_tokens': 10000,     # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    'batch_size': 32,        # ãƒãƒƒãƒã‚µã‚¤ã‚º
    'epochs': 50,            # ã‚¨ãƒãƒƒã‚¯æ•°
    'dropout_r': -0.3,       # APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ r
}

# ========== APQB (Adjustable Pseudo Quantum Bit) ==========
class APQB:
    """èª¿æ•´å¯èƒ½ãªæ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆ"""
    
    def __init__(self, r: float = 0.0):
        self.r = np.clip(r, -1, 1)
    
    @property
    def theta(self) -> float:
        return np.pi * (1 - self.r) / 2
    
    @property
    def p_keep(self) -> float:
        return np.sin(self.theta / 2) ** 2
    
    def measure(self) -> int:
        return 1 if np.random.random() < self.p_keep else 0


# ========== APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ ==========
class APQBDropout(nn.Module):
    """APQBãƒ™ãƒ¼ã‚¹ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤"""
    
    def __init__(self, r: float = 0.0, learnable: bool = False):
        super().__init__()
        if learnable:
            self._r = nn.Parameter(torch.tensor(float(r)))
        else:
            self.register_buffer('_r', torch.tensor(float(r)))
        self.learnable = learnable
    
    @property
    def r(self) -> float:
        return float(self._r.clamp(-1, 1))
    
    @r.setter
    def r(self, value: float):
        with torch.no_grad():
            self._r.fill_(np.clip(value, -1, 1))
    
    def get_keep_probability(self) -> float:
        r = self.r
        theta = np.pi * (1 - r) / 2
        return np.sin(theta / 2) ** 2
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if not self.training:
            return x
        
        r = self._r.clamp(-1, 1)
        theta = np.pi * (1 - r.item()) / 2
        p_keep = np.sin(theta / 2) ** 2
        
        # é‡å­ã‚†ã‚‰ãã‚’å«ã‚€ãƒã‚¹ã‚¯ç”Ÿæˆ
        rand = torch.rand_like(x)
        quantum_noise = torch.zeros_like(x)
        for i in range(4):
            phase = torch.rand_like(x) * 2 * np.pi
            quantum_noise += torch.sin(phase) * 0.05
        
        mask = ((rand + quantum_noise) < p_keep).float()
        
        if p_keep > 0:
            return x * mask / p_keep
        return x * 0


# ========== APQBã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ ==========
class APQBAttention(nn.Module):
    """APQBãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹"""
    
    def __init__(self, embed_dim: int, num_heads: int = 4, dropout_r: float = 0.0):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.apqb_dropout = APQBDropout(r=dropout_r)
        self.scale = self.head_dim ** -0.5
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        batch_size, seq_len, _ = x.shape
        
        # Q, K, V ã‚’è¨ˆç®—
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        attn = F.softmax(scores, dim=-1)
        attn = self.apqb_dropout(attn)  # APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆé©ç”¨
        
        # å‡ºåŠ›
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        return self.out_proj(out)


# ========== APQBãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ ==========
class APQBTransformerBlock(nn.Module):
    """APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯"""
    
    def __init__(self, embed_dim: int, num_heads: int = 4, 
                 ff_dim: int = None, dropout_r: float = 0.0):
        super().__init__()
        ff_dim = ff_dim or embed_dim * 4
        
        self.attention = APQBAttention(embed_dim, num_heads, dropout_r)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.GELU(),
            APQBDropout(r=dropout_r),
            nn.Linear(ff_dim, embed_dim),
            APQBDropout(r=dropout_r)
        )
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        # Self-attention with residual
        x = x + self.attention(self.norm1(x), mask)
        # Feed-forward with residual
        x = x + self.ff(self.norm2(x))
        return x


# ========== APQBç”ŸæˆAIãƒ¢ãƒ‡ãƒ« ==========
class APQBGenerativeModel(nn.Module):
    """APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ä½¿ç”¨ã—ãŸç”ŸæˆAIãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, vocab_size: int, embed_dim: int = 128, 
                 num_heads: int = 4, num_layers: int = 3,
                 max_seq_len: int = 128, dropout_r: float = 0.0):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.max_seq_len = max_seq_len
        
        # åŸ‹ã‚è¾¼ã¿å±¤
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)
        self.embed_dropout = APQBDropout(r=dropout_r)
        
        # ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
        self.blocks = nn.ModuleList([
            APQBTransformerBlock(embed_dim, num_heads, dropout_r=dropout_r)
            for _ in range(num_layers)
        ])
        
        # å‡ºåŠ›å±¤
        self.norm = nn.LayerNorm(embed_dim)
        self.output = nn.Linear(embed_dim, vocab_size)
        
        # é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ã®APQB
        self.sampling_r = 0.0
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        batch_size, seq_len = x.shape
        
        # ä½ç½®æƒ…å ±
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)
        
        # åŸ‹ã‚è¾¼ã¿
        x = self.token_embedding(x) + self.position_embedding(positions)
        x = self.embed_dropout(x)
        
        # å› æœãƒã‚¹ã‚¯ï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ï¼‰
        if mask is None:
            mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).unsqueeze(0).unsqueeze(0)
        
        # ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
        for block in self.blocks:
            x = block(x, mask)
        
        # å‡ºåŠ›
        x = self.norm(x)
        logits = self.output(x)
        
        return logits
    
    def set_dropout_r(self, r: float):
        """å…¨APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã®rã‚’è¨­å®š"""
        for module in self.modules():
            if isinstance(module, APQBDropout):
                module.r = r
    
    @torch.no_grad()
    def generate(self, prompt_ids: torch.Tensor, max_new_tokens: int = 50,
                 temperature: float = 1.0, top_k: int = 50,
                 quantum_sampling: bool = True) -> torch.Tensor:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        self.eval()
        generated = prompt_ids.clone()
        
        for _ in range(max_new_tokens):
            # æœ€å¾Œã®max_seq_lenãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ä½¿ç”¨
            context = generated[:, -self.max_seq_len:]
            
            # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ã‚’è¨ˆç®—
            logits = self(context)
            logits = logits[:, -1, :] / temperature
            
            # Top-k ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if top_k > 0:
                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
                logits[indices_to_remove] = float('-inf')
            
            probs = F.softmax(logits, dim=-1)
            
            # é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° or é€šå¸¸ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if quantum_sampling:
                next_token = self._quantum_sample(probs)
            else:
                next_token = torch.multinomial(probs, num_samples=1)
            
            generated = torch.cat([generated, next_token], dim=1)
        
        return generated
    
    def _quantum_sample(self, probs: torch.Tensor) -> torch.Tensor:
        """APQBãƒ™ãƒ¼ã‚¹ã®é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        batch_size, vocab_size = probs.shape
        
        # è¤‡æ•°ã®APQBã§é‡å­ä¹±æ•°ã‚’ç”Ÿæˆ
        quantum_random = torch.zeros(batch_size, device=probs.device)
        for i in range(12):  # 12ãƒ“ãƒƒãƒˆç²¾åº¦
            r = torch.rand(batch_size, device=probs.device) * 2 - 1 + self.sampling_r
            r = r.clamp(-1, 1)
            theta = np.pi * (1 - r) / 2
            p_one = torch.sin(theta / 2) ** 2
            bit = (torch.rand(batch_size, device=probs.device) < p_one).float()
            quantum_random += bit * (2 ** -(i + 1))
        
        # ç´¯ç©ç¢ºç‡ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        cumsum = probs.cumsum(dim=-1)
        next_token = (cumsum < quantum_random.unsqueeze(1)).sum(dim=-1, keepdim=True)
        next_token = next_token.clamp(0, vocab_size - 1)
        
        return next_token


# ========== ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ==========
class SubwordTokenizer:
    """BPEãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"""
    
    def __init__(self, max_vocab_size: int = 5000):
        self.max_vocab_size = max_vocab_size
        self.token_to_idx = {}
        self.idx_to_token = {}
        self.merges = []
        self.special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']
        
    def train(self, texts: list, min_freq: int = 2):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å­¦ç¿’"""
        # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®èªå½™ã‚’æ§‹ç¯‰
        char_vocab = set()
        for text in texts:
            char_vocab.update(text)
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ + æ–‡å­—èªå½™
        vocab = list(self.special_tokens) + sorted(char_vocab)
        self.token_to_idx = {t: i for i, t in enumerate(vocab)}
        self.idx_to_token = {i: t for t, i in self.token_to_idx.items()}
        
        # å˜èªã‚’æ–‡å­—ã®ã‚¿ãƒ—ãƒ«ã«åˆ†å‰²
        word_freqs = Counter()
        for text in texts:
            words = text.split()
            for word in words:
                word_freqs[tuple(word)] += 1
        
        # BPEãƒãƒ¼ã‚¸ã‚’å­¦ç¿’
        while len(self.token_to_idx) < self.max_vocab_size:
            # ãƒšã‚¢ã®é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            pair_freqs = Counter()
            for word, freq in word_freqs.items():
                if len(word) < 2:
                    continue
                for i in range(len(word) - 1):
                    pair = (word[i], word[i+1])
                    pair_freqs[pair] += freq
            
            if not pair_freqs:
                break
            
            # æœ€é »å‡ºãƒšã‚¢ã‚’å–å¾—
            best_pair = pair_freqs.most_common(1)[0]
            if best_pair[1] < min_freq:
                break
            
            pair = best_pair[0]
            new_token = ''.join(pair)
            
            # èªå½™ã«è¿½åŠ 
            new_idx = len(self.token_to_idx)
            self.token_to_idx[new_token] = new_idx
            self.idx_to_token[new_idx] = new_token
            self.merges.append(pair)
            
            # å˜èªã‚’æ›´æ–°
            new_word_freqs = Counter()
            for word, freq in word_freqs.items():
                new_word = []
                i = 0
                while i < len(word):
                    if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:
                        new_word.append(new_token)
                        i += 2
                    else:
                        new_word.append(word[i])
                        i += 1
                new_word_freqs[tuple(new_word)] = freq
            word_freqs = new_word_freqs
        
        print(f"ğŸ“š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å­¦ç¿’å®Œäº†: {len(self.token_to_idx)} ãƒˆãƒ¼ã‚¯ãƒ³")
        return self
    
    def encode(self, text: str) -> list:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›"""
        tokens = []
        words = text.split()
        
        for word in words:
            # æ–‡å­—ã«åˆ†å‰²
            chars = list(word)
            
            # ãƒãƒ¼ã‚¸ã‚’é©ç”¨
            for merge in self.merges:
                i = 0
                new_chars = []
                while i < len(chars):
                    if i < len(chars) - 1 and chars[i] == merge[0] and chars[i+1] == merge[1]:
                        new_chars.append(''.join(merge))
                        i += 2
                    else:
                        new_chars.append(chars[i])
                        i += 1
                chars = new_chars
            
            # ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
            for c in chars:
                tokens.append(self.token_to_idx.get(c, self.token_to_idx['<UNK>']))
            
            # ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ 
            tokens.append(self.token_to_idx.get(' ', self.token_to_idx['<UNK>']))
        
        return tokens
    
    def decode(self, ids: list) -> str:
        """ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        tokens = []
        for idx in ids:
            if isinstance(idx, torch.Tensor):
                idx = idx.item()
            token = self.idx_to_token.get(idx, '<UNK>')
            if token not in self.special_tokens:
                tokens.append(token)
        return ''.join(tokens)
    
    @property
    def vocab_size(self) -> int:
        return len(self.token_to_idx)


# ========== æ–‡å­—ãƒ¬ãƒ™ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ==========
class CharTokenizer:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—ãƒ¬ãƒ™ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"""
    
    def __init__(self):
        self.token_to_idx = {}
        self.idx_to_token = {}
        self.special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']
    
    def train(self, texts: list):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å­¦ç¿’"""
        char_vocab = set()
        for text in texts:
            char_vocab.update(text)
        
        vocab = list(self.special_tokens) + sorted(char_vocab)
        self.token_to_idx = {t: i for i, t in enumerate(vocab)}
        self.idx_to_token = {i: t for t, i in self.token_to_idx.items()}
        
        print(f"ğŸ“š æ–‡å­—ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å­¦ç¿’å®Œäº†: {len(self.token_to_idx)} ãƒˆãƒ¼ã‚¯ãƒ³")
        return self
    
    def encode(self, text: str) -> list:
        return [self.token_to_idx.get(c, self.token_to_idx['<UNK>']) for c in text]
    
    def decode(self, ids: list) -> str:
        tokens = []
        for idx in ids:
            if isinstance(idx, torch.Tensor):
                idx = idx.item()
            token = self.idx_to_token.get(idx, '')
            if token not in self.special_tokens:
                tokens.append(token)
        return ''.join(tokens)
    
    @property
    def vocab_size(self) -> int:
        return len(self.token_to_idx)


# ========== ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ==========
class TextDataset(Dataset):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, text: str, tokenizer, seq_len: int = 128):
        self.seq_len = seq_len
        self.tokenizer = tokenizer
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
        self.data = torch.tensor(tokenizer.encode(text), dtype=torch.long)
        self.vocab_size = tokenizer.vocab_size
    
    def __len__(self):
        return max(0, len(self.data) - self.seq_len - 1)
    
    def __getitem__(self, idx):
        x = self.data[idx:idx + self.seq_len]
        y = self.data[idx + 1:idx + self.seq_len + 1]
        return x, y
    
    def encode(self, text: str) -> torch.Tensor:
        return torch.tensor(self.tokenizer.encode(text), dtype=torch.long)
    
    def decode(self, ids: torch.Tensor) -> str:
        return self.tokenizer.decode(ids.tolist() if isinstance(ids, torch.Tensor) else ids)


# ========== APQBç”ŸæˆAI ==========
class APQBGenerativeAI:
    """APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ä½¿ç”¨ã—ãŸç”ŸæˆAIï¼ˆå¤§è¦æ¨¡ãƒˆãƒ¼ã‚¯ãƒ³å¯¾å¿œï¼‰"""
    
    def __init__(self, embed_dim: int = 128, num_heads: int = 8,
                 num_layers: int = 4, dropout_r: float = -0.3,
                 max_vocab_size: int = 5000, use_subword: bool = True):
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.dropout_r = dropout_r
        self.max_vocab_size = max_vocab_size
        self.use_subword = use_subword
        
        self.model = None
        self.dataset = None
        self.tokenizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def train(self, text: str, epochs: int = 50, batch_size: int = 32,
              lr: float = 0.001, seq_len: int = 128, verbose: bool = True):
        """ãƒ†ã‚­ã‚¹ãƒˆã§å­¦ç¿’"""
        if verbose:
            print("ğŸ§ âš›ï¸ APQB Generative AI v2 å­¦ç¿’é–‹å§‹...")
            print(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
            print(f"   ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³: {'æœ‰åŠ¹' if self.use_subword else 'ç„¡åŠ¹'}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä½œæˆ
        if self.use_subword:
            self.tokenizer = SubwordTokenizer(max_vocab_size=self.max_vocab_size)
            self.tokenizer.train([text])
        else:
            self.tokenizer = CharTokenizer()
            self.tokenizer.train([text])
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        self.dataset = TextDataset(text, self.tokenizer, seq_len)
        if verbose:
            print(f"   èªå½™ã‚µã‚¤ã‚º: {self.dataset.vocab_size} ãƒˆãƒ¼ã‚¯ãƒ³")
            print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {seq_len}")
            print(f"   ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(self.dataset)} ã‚µãƒ³ãƒ—ãƒ«")
            print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(self.dataset.data):,}")
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        self.model = APQBGenerativeModel(
            vocab_size=self.dataset.vocab_size,
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            num_layers=self.num_layers,
            max_seq_len=seq_len,
            dropout_r=self.dropout_r
        ).to(self.device)
        
        if verbose:
            total_params = sum(p.numel() for p in self.model.parameters())
            print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
            print(f"   APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ r: {self.dropout_r}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True, 
                                drop_last=True, num_workers=0)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨æå¤±é–¢æ•°
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)
        criterion = nn.CrossEntropyLoss()
        
        # å­¦ç¿’
        self.model.train()
        losses = []
        best_loss = float('inf')
        
        for epoch in range(epochs):
            epoch_loss = 0
            num_batches = 0
            
            for x, y in dataloader:
                x, y = x.to(self.device), y.to(self.device)
                
                optimizer.zero_grad()
                logits = self.model(x)
                loss = criterion(logits.view(-1, self.dataset.vocab_size), y.view(-1))
                loss.backward()
                
                # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            scheduler.step()
            
            avg_loss = epoch_loss / max(num_batches, 1)
            losses.append(avg_loss)
            
            if avg_loss < best_loss:
                best_loss = avg_loss
            
            if verbose and (epoch + 1) % 5 == 0:
                lr_now = scheduler.get_last_lr()[0]
                print(f"   Epoch {epoch+1}/{epochs}: Loss = {avg_loss:.4f}, LR = {lr_now:.6f}")
        
        if verbose:
            print(f"âœ… å­¦ç¿’å®Œäº†ï¼ æœ€è‰¯æå¤±: {best_loss:.4f}")
        
        return losses
    
    def generate(self, prompt: str = "", max_length: int = 200,
                 temperature: float = 1.0, top_k: int = 50, top_p: float = 0.9,
                 dropout_r: float = None, quantum_sampling: bool = True,
                 repetition_penalty: float = 1.1) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        if self.model is None or self.dataset is None:
            raise ValueError("å…ˆã«train()ã§å­¦ç¿’ã—ã¦ãã ã•ã„")
        
        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ã‚’è¨­å®š
        if dropout_r is not None:
            self.model.set_dropout_r(dropout_r)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        if prompt:
            prompt_ids = self.dataset.encode(prompt).unsqueeze(0).to(self.device)
        else:
            # BOSãƒˆãƒ¼ã‚¯ãƒ³ã§é–‹å§‹
            bos_id = self.tokenizer.token_to_idx.get('<BOS>', 0)
            prompt_ids = torch.tensor([[bos_id]], dtype=torch.long).to(self.device)
        
        # ç”Ÿæˆ
        self.model.sampling_r = dropout_r if dropout_r is not None else self.dropout_r
        generated_ids = self.model.generate(
            prompt_ids,
            max_new_tokens=max_length,
            temperature=temperature,
            top_k=top_k,
            quantum_sampling=quantum_sampling
        )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        return self.dataset.decode(generated_ids[0])
    
    def generate_stream(self, prompt: str = "", max_length: int = 200,
                        temperature: float = 1.0):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ"""
        for char in prompt:
            yield char
        
        generated = self.generate(prompt, max_length, temperature)
        for char in generated[len(prompt):]:
            yield char
    
    def get_stats(self) -> dict:
        """ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆã‚’å–å¾—"""
        if self.model is None:
            return {}
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            'vocab_size': self.tokenizer.vocab_size,
            'embed_dim': self.embed_dim,
            'num_heads': self.num_heads,
            'num_layers': self.num_layers,
            'total_params': total_params,
            'trainable_params': trainable_params,
            'dropout_r': self.dropout_r,
            'device': str(self.device)
        }
    
    def save(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        torch.save({
            'model_state': self.model.state_dict(),
            'tokenizer': self.tokenizer,
            'config': {
                'embed_dim': self.embed_dim,
                'num_heads': self.num_heads,
                'num_layers': self.num_layers,
                'dropout_r': self.dropout_r,
            }
        }, path)
        print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {path}")
    
    def load(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        checkpoint = torch.load(path, map_location=self.device)
        self.tokenizer = checkpoint['tokenizer']
        config = checkpoint['config']
        
        self.embed_dim = config['embed_dim']
        self.num_heads = config['num_heads']
        self.num_layers = config['num_layers']
        self.dropout_r = config['dropout_r']
        
        # ãƒ¢ãƒ‡ãƒ«å†æ§‹ç¯‰
        self.model = APQBGenerativeModel(
            vocab_size=self.tokenizer.vocab_size,
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            num_layers=self.num_layers,
            dropout_r=self.dropout_r
        ).to(self.device)
        
        self.model.load_state_dict(checkpoint['model_state'])
        print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {path}")


# ========== OpenAssistantãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ==========
def load_openassistant_data(max_samples: int = 5000, lang: str = 'ja'):
    """OpenAssistant/oasst1ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
    try:
        from datasets import load_dataset
        print("ğŸ“¥ OpenAssistant/oasst1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        
        dataset = load_dataset("OpenAssistant/oasst1", split="train")
        
        # æ—¥æœ¬èªã¾ãŸã¯æŒ‡å®šè¨€èªã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        texts = []
        count = 0
        
        for item in dataset:
            if count >= max_samples:
                break
            
            text = item.get('text', '')
            item_lang = item.get('lang', '')
            
            # è¨€èªãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ'ja'ãªã‚‰æ—¥æœ¬èªã€'all'ãªã‚‰å…¨è¨€èªï¼‰
            if lang == 'all' or item_lang == lang:
                if len(text) > 20:  # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–
                    texts.append(text)
                    count += 1
            elif lang == 'ja' and any(ord(c) > 0x3000 for c in text):
                # æ—¥æœ¬èªæ–‡å­—ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚ã‚‹
                if len(text) > 20:
                    texts.append(text)
                    count += 1
        
        if len(texts) < 100:
            # æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯è‹±èªã‚‚å«ã‚ã‚‹
            print(f"âš ï¸ {lang}ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚ã€è‹±èªãƒ‡ãƒ¼ã‚¿ã‚‚è¿½åŠ ã—ã¾ã™...")
            for item in dataset:
                if count >= max_samples:
                    break
                text = item.get('text', '')
                if len(text) > 20 and text not in texts:
                    texts.append(text)
                    count += 1
        
        combined_text = '\n\n'.join(texts)
        print(f"âœ… {len(texts)} ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({len(combined_text):,} æ–‡å­—)")
        return combined_text
        
    except ImportError:
        print("âŒ datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print("   pip install datasets")
        return None
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def load_local_or_oasst(use_oasst: bool = True, max_samples: int = 3000):
    """OpenAssistantã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    if use_oasst:
        text = load_openassistant_data(max_samples=max_samples, lang='all')
        if text:
            return text
        print("âš ï¸ OpenAssistantãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    return get_large_training_text()


# ========== å¤§è¦æ¨¡å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆ ==========
def get_large_training_text():
    """å¤§è¦æ¨¡å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
    base_texts = [
        """
        äººå·¥çŸ¥èƒ½ã¯ç§ãŸã¡ã®ç”Ÿæ´»ã‚’å¤§ããå¤‰ãˆã¦ã„ã¾ã™ã€‚
        æ©Ÿæ¢°å­¦ç¿’ã‚„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç™ºå±•ã«ã‚ˆã‚Šã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯äººé–“ã®ã‚ˆã†ã«è€ƒãˆã€å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚
        ç”»åƒèªè­˜ã€éŸ³å£°èªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã€AIã®å¿œç”¨åˆ†é‡ã¯æ€¥é€Ÿã«æ‹¡å¤§ã—ã¦ã„ã¾ã™ã€‚
        è‡ªå‹•é‹è»¢è»Šã€åŒ»ç™‚è¨ºæ–­æ”¯æ´ã€é‡‘èäºˆæ¸¬ãªã©ã€æ§˜ã€…ãªç”£æ¥­ã§AIãŒæ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
        AIã®å€«ç†çš„ãªå•é¡Œã«ã¤ã„ã¦ã‚‚ã€ç¤¾ä¼šå…¨ä½“ã§è­°è«–ãŒé€²ã‚“ã§ã„ã¾ã™ã€‚
        """,
        """
        é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€ã“ã®é©å‘½ã‚’ã•ã‚‰ã«åŠ é€Ÿã•ã›ã‚‹å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã¾ã™ã€‚
        é‡å­ãƒ“ãƒƒãƒˆã¯é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’åˆ©ç”¨ã—ã¦ã€å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã¯ä¸å¯èƒ½ã ã£ãŸè¨ˆç®—ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚
        é‡å­ã‚‚ã¤ã‚Œã‚„é‡å­å¹²æ¸‰ãªã©ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸæ–°ã—ã„è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã™ã€‚
        æš—å·è§£èª­ã€åˆ†å­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€æœ€é©åŒ–å•é¡Œãªã©ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®å¿œç”¨ã¯å¤šå²ã«ã‚ãŸã‚Šã¾ã™ã€‚
        ä¸–ç•Œä¸­ã®ç ”ç©¶æ©Ÿé–¢ã‚„ä¼æ¥­ãŒã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®å®Ÿç”¨åŒ–ã«å‘ã‘ã¦ç ”ç©¶ã‚’é€²ã‚ã¦ã„ã¾ã™ã€‚
        """,
        """
        APQBã¯ã€ã“ã®é‡å­ã®æ€§è³ªã‚’æ“¬ä¼¼çš„ã«å†ç¾ã—ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ç¢ºç‡çš„ãªæŒ¯ã‚‹èˆã„ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚
        æ“¬ä¼¼é‡å­ãƒ“ãƒƒãƒˆã¯ã€ç›¸é–¢ä¿‚æ•°ã‹ã‚‰é‡å­çŠ¶æ…‹ã‚’æ§‹ç¯‰ã—ã€ç¢ºç‡çš„ãªæ¸¬å®šã‚’è¡Œã„ã¾ã™ã€‚
        ã“ã®æŠ€è¡“ã«ã‚ˆã‚Šã€å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ‰ãªè¦ç´ ã‚’è¿½åŠ ã§ãã¾ã™ã€‚
        APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã¯ã€å­¦ç¿’ä¸­ã®æ­£å‰‡åŒ–ã«é‡å­çš„ãªã‚†ã‚‰ãã‚’å°å…¥ã—ã¾ã™ã€‚
        é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å‰µé€ çš„ãªå¤šæ§˜æ€§ãŒç”Ÿã¾ã‚Œã¾ã™ã€‚
        """,
        """
        æœªæ¥ã®AIã¯ã€é‡å­ã¨å¤å…¸ã®èåˆã«ã‚ˆã£ã¦ã€ã‚ˆã‚Šå‰µé€ çš„ã§æŸ”è»Ÿãªæ€è€ƒãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã§ã—ã‚‡ã†ã€‚
        æŠ€è¡“ã®é€²æ­©ã¯æ­¢ã¾ã‚‹ã“ã¨ã‚’çŸ¥ã‚Šã¾ã›ã‚“ã€‚ç§ãŸã¡ã¯å¸¸ã«æ–°ã—ã„å¯èƒ½æ€§ã‚’æ¢æ±‚ã—ç¶šã‘ã¦ã„ã¾ã™ã€‚
        äººé–“ã¨AIãŒå”åŠ›ã—ã¦ã€ã‚ˆã‚Šè‰¯ã„ç¤¾ä¼šã‚’ç¯‰ã„ã¦ã„ãã“ã¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚
        ç’°å¢ƒå•é¡Œã€ã‚¨ãƒãƒ«ã‚®ãƒ¼å•é¡Œã€åŒ»ç™‚å•é¡Œãªã©ã€äººé¡ãŒç›´é¢ã™ã‚‹èª²é¡Œã«AIãŒè²¢çŒ®ã§ãã¾ã™ã€‚
        æŒç¶šå¯èƒ½ãªç™ºå±•ã®ãŸã‚ã«ã€æŠ€è¡“ã¯äººé–“ã®å¹¸ç¦ã«è²¢çŒ®ã™ã‚‹æ–¹å‘ã«é€²åŒ–ã—ã¦ã„ãã¹ãã§ã™ã€‚
        """,
        """
        æ—¥æœ¬ã®å››å­£ã¯æœ¬å½“ã«ç¾ã—ã„ã‚‚ã®ã§ã™ã€‚
        æ¡œã®èŠ±ãŒå’²ãæ˜¥ã®å­£ç¯€ã€ç§ãŸã¡ã¯æ–°ã—ã„å§‹ã¾ã‚Šã‚’è¿ãˆã¾ã™ã€‚
        å…¥å­¦å¼ã€å…¥ç¤¾å¼ã€æ–°ç”Ÿæ´»ã®ã‚¹ã‚¿ãƒ¼ãƒˆã€‚æ˜¥ã¯å¸Œæœ›ã«æº€ã¡ãŸå­£ç¯€ã§ã™ã€‚
        èŠ±è¦‹ã®æ–‡åŒ–ã¯æ—¥æœ¬ç‹¬ç‰¹ã®ã‚‚ã®ã§ã€æ¡œã®ä¸‹ã§äººã€…ãŒé›†ã„ã€æ¥½ã—ã„æ™‚é–“ã‚’éã”ã—ã¾ã™ã€‚
        """,
        """
        å¤ã«ã¯å¤ªé™½ãŒè¼ãã€æµ·ã‚„å±±ã§æ¥½ã—ã„æ™‚é–“ã‚’éã”ã—ã¾ã™ã€‚
        èŠ±ç«å¤§ä¼šã€å¤ç¥­ã‚Šã€æµ·æ°´æµ´ã€‚å¤ã¯æ´»æ°—ã«æº€ã¡ãŸå­£ç¯€ã§ã™ã€‚
        æ—¥æœ¬ã®å¤ã¯æš‘ã„ã§ã™ãŒã€å†·ãŸã„ã‹ãæ°·ã‚„ãã†ã‚ã‚“ã§æ¶¼ã‚’ã¨ã‚Šã¾ã™ã€‚
        ãŠç›†ã«ã¯å…ˆç¥–ã®éœŠã‚’è¿ãˆã€å®¶æ—ã§éã”ã™å¤§åˆ‡ãªæ™‚é–“ãŒã‚ã‚Šã¾ã™ã€‚
        """,
        """
        ç§‹ã«ãªã‚‹ã¨ç´…è‘‰ãŒç¾ã—ãè‰²ã¥ãã€ç©ã‚„ã‹ãªé¢¨ãŒå¹ãã¾ã™ã€‚
        èª­æ›¸ã®ç§‹ã€èŠ¸è¡“ã®ç§‹ã€é£Ÿæ¬²ã®ç§‹ã€‚ç§‹ã¯å®Ÿã‚Šã®å­£ç¯€ã§ã™ã€‚
        æ–°ç±³ã€ã•ã‚“ã¾ã€æ —ã€æŸ¿ã€‚ç§‹ã®å‘³è¦šã‚’æ¥½ã—ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚
        ç´…è‘‰ç‹©ã‚Šã¯æ—¥æœ¬ã®ç¾ã—ã„ä¼çµ±æ–‡åŒ–ã®ä¸€ã¤ã§ã™ã€‚
        """,
        """
        å†¬ã¯é›ªãŒé™ã‚Šã€æ¸©ã‹ã„å®¶ã®ä¸­ã§å®¶æ—ã¨éã”ã™æ™‚é–“ãŒå¢—ãˆã¾ã™ã€‚
        ã‚¯ãƒªã‚¹ãƒã‚¹ã€å¤§æ™¦æ—¥ã€ãŠæ­£æœˆã€‚å†¬ã¯ç‰¹åˆ¥ãªã‚¤ãƒ™ãƒ³ãƒˆãŒç¶šãã¾ã™ã€‚
        ã“ãŸã¤ã§ã¿ã‹ã‚“ã‚’é£Ÿã¹ãªãŒã‚‰ã€ã®ã‚“ã³ã‚Šã¨éã”ã™æ™‚é–“ã¯æ ¼åˆ¥ã§ã™ã€‚
        æ–°å¹´ã®æŠ±è² ã‚’ç«‹ã¦ã€æ–°ãŸãªä¸€å¹´ã¸ã®å¸Œæœ›ã‚’èƒ¸ã«æ­©ã¿å§‹ã‚ã¾ã™ã€‚
        """,
        """
        ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ç¾ä»£ç¤¾ä¼šã§é‡è¦ãªã‚¹ã‚­ãƒ«ã§ã™ã€‚
        Pythonã€JavaScriptã€Javaã€C++ãªã©ã€æ§˜ã€…ãªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªãŒã‚ã‚Šã¾ã™ã€‚
        ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯ã€åŠ¹ç‡çš„ãªãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ããŸã‚ã«å¿…è¦ãªçŸ¥è­˜ã§ã™ã€‚
        ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã§ã¯ã€ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒé‡è¦ã§ã™ã€‚
        ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãƒ†ã‚¹ãƒˆã€ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å“è³ªã‚’ä¿ã¤ãŸã‚ã®åŸºæœ¬çš„ãªãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã§ã™ã€‚
        """,
        """
        ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯äººé–“ã®è„³ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
        å…¥åŠ›å±¤ã€éš ã‚Œå±¤ã€å‡ºåŠ›å±¤ã‹ã‚‰æ§‹æˆã•ã‚Œã€é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã§å­¦ç¿’ã—ã¾ã™ã€‚
        æ´»æ€§åŒ–é–¢æ•°ã€æå¤±é–¢æ•°ã€æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æ·±å±¤å­¦ç¿’ã®é‡è¦ãªè¦ç´ ã§ã™ã€‚
        ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ç”»åƒèªè­˜ã«ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«é©ã—ã¦ã„ã¾ã™ã€‚
        ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯è‡ªç„¶è¨€èªå‡¦ç†ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚
        """,
        """
        å®‡å®™ã¯åºƒå¤§ã§ç¥ç§˜çš„ãªç©ºé–“ã§ã™ã€‚
        å¤ªé™½ç³»ã«ã¯8ã¤ã®æƒ‘æ˜ŸãŒã‚ã‚Šã€åœ°çƒã¯å”¯ä¸€ç”Ÿå‘½ãŒç¢ºèªã•ã‚Œã¦ã„ã‚‹æƒ‘æ˜Ÿã§ã™ã€‚
        æ˜Ÿåº§ã¯å¤ä»£ã‹ã‚‰äººã€…ã®æƒ³åƒåŠ›ã‚’åˆºæ¿€ã—ã¦ãã¾ã—ãŸã€‚
        ãƒ–ãƒ©ãƒƒã‚¯ãƒ›ãƒ¼ãƒ«ã€ãƒ€ãƒ¼ã‚¯ãƒã‚¿ãƒ¼ã€ãƒ€ãƒ¼ã‚¯ã‚¨ãƒãƒ«ã‚®ãƒ¼ãªã©ã€å®‡å®™ã«ã¯ã¾ã å¤šãã®è¬ãŒã‚ã‚Šã¾ã™ã€‚
        å®‡å®™æ¢æŸ»æŠ€è¡“ã®ç™ºå±•ã«ã‚ˆã‚Šã€äººé¡ã¯ç«æ˜Ÿã¸ã®æœ‰äººé£›è¡Œã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚
        """,
        """
        éŸ³æ¥½ã¯äººé–“ã®æ„Ÿæƒ…ã«æ·±ãå½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚
        ã‚¯ãƒ©ã‚·ãƒƒã‚¯ã€ã‚¸ãƒ£ã‚ºã€ãƒ­ãƒƒã‚¯ã€ãƒãƒƒãƒ—ã€é›»å­éŸ³æ¥½ãªã©ã€æ§˜ã€…ãªã‚¸ãƒ£ãƒ³ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚
        éŸ³æ¥½ç™‚æ³•ã¯å¿ƒèº«ã®å¥åº·ã«åŠ¹æœãŒã‚ã‚‹ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
        AIã«ã‚ˆã‚‹ä½œæ›²æŠ€è¡“ã‚‚ç™ºå±•ã—ã€æ–°ã—ã„éŸ³æ¥½è¡¨ç¾ã®å¯èƒ½æ€§ãŒåºƒãŒã£ã¦ã„ã¾ã™ã€‚
        ãƒ©ã‚¤ãƒ–æ¼”å¥ã®è‡¨å ´æ„Ÿã¯ã€éŒ²éŸ³ã§ã¯å¾—ã‚‰ã‚Œãªã„ç‰¹åˆ¥ãªä½“é¨“ã§ã™ã€‚
        """,
    ]
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¹°ã‚Šè¿”ã—ã¦å¤§è¦æ¨¡åŒ–
    return '\n'.join(base_texts * 5)


# ========== ãƒ‡ãƒ¢ ==========
def demo(use_oasst: bool = False):
    """ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("=" * 70)
    print("ğŸ§ âš›ï¸ APQB Generative AI v2 - å¤§è¦æ¨¡ãƒˆãƒ¼ã‚¯ãƒ³å¯¾å¿œç‰ˆ")
    if use_oasst:
        print("   ğŸ“š OpenAssistant/oasst1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨")
    print("=" * 70)
    
    # å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
    if use_oasst:
        training_text = load_local_or_oasst(use_oasst=True, max_samples=2000)
    else:
        training_text = get_large_training_text()
    print(f"\nğŸ“Š å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º: {len(training_text):,} æ–‡å­—")
    
    print("\nğŸ“š å­¦ç¿’ä¸­...")
    
    # APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ r=-0.3 (æœ€é©è¨­å®š)
    print("\n--- å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ (ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³) ---")
    ai = APQBGenerativeAI(
        embed_dim=128,
        num_heads=8,
        num_layers=4,
        dropout_r=-0.3,
        max_vocab_size=3000,
        use_subword=True
    )
    losses = ai.train(
        training_text, 
        epochs=40, 
        batch_size=32, 
        seq_len=128
    )
    
    # çµ±è¨ˆæƒ…å ±
    stats = ai.get_stats()
    print("\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ:")
    print(f"   èªå½™ã‚µã‚¤ã‚º: {stats['vocab_size']:,} ãƒˆãƒ¼ã‚¯ãƒ³")
    print(f"   åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {stats['embed_dim']}")
    print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {stats['total_params']:,}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    prompts = ["äººå·¥çŸ¥èƒ½", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿", "æ—¥æœ¬ã®", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "æœªæ¥"]
    
    print("\n" + "=" * 70)
    print("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆçµæœ (max_length=150)")
    print("=" * 70)
    
    for prompt in prompts:
        print(f"\nğŸ”® ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ã€Œ{prompt}ã€")
        print("-" * 50)
        
        text = ai.generate(prompt, max_length=150, temperature=0.8, top_k=50)
        print(f"{text}")
    
    # æ¸©åº¦ã«ã‚ˆã‚‹é•ã„
    print("\n" + "=" * 70)
    print("ğŸŒ¡ï¸ æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹é•ã„")
    print("=" * 70)
    
    for temp in [0.5, 0.8, 1.0, 1.2]:
        text = ai.generate("æŠ€è¡“", max_length=80, temperature=temp)
        print(f"\næ¸©åº¦ {temp}: {text}")
    
    # é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ¯”è¼ƒ
    print("\n" + "=" * 70)
    print("âš›ï¸ é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° vs é€šå¸¸ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")
    print("=" * 70)
    
    print("\n[é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ON]")
    for i in range(3):
        text = ai.generate("å®‡å®™", max_length=60, temperature=0.9, quantum_sampling=True)
        print(f"  {i+1}. {text}")
    
    print("\n[é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° OFF]")
    for i in range(3):
        text = ai.generate("å®‡å®™", max_length=60, temperature=0.9, quantum_sampling=False)
        print(f"  {i+1}. {text}")
    
    print("\nâœ… ãƒ‡ãƒ¢å®Œäº†ï¼")
    
    return ai


# ========== å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ ==========
def interactive_mode(use_oasst: bool = False):
    """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
    print("=" * 70)
    print("ğŸ§ âš›ï¸ APQB Generative AI v2 - å¤§è¦æ¨¡ãƒˆãƒ¼ã‚¯ãƒ³å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
    if use_oasst:
        print("   ğŸ“š OpenAssistant/oasst1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨")
    print("=" * 70)
    
    # å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
    if use_oasst:
        training_text = load_local_or_oasst(use_oasst=True, max_samples=3000)
    else:
        training_text = get_large_training_text()
    print(f"\nğŸ“Š å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º: {len(training_text):,} æ–‡å­—")
    
    print("\nğŸ“š å­¦ç¿’ä¸­...")
    ai = APQBGenerativeAI(
        embed_dim=128,
        num_heads=8,
        num_layers=4,
        dropout_r=-0.3,
        max_vocab_size=3000,
        use_subword=True
    )
    ai.train(training_text, epochs=40, batch_size=32, seq_len=128)
    
    # çµ±è¨ˆæƒ…å ±
    stats = ai.get_stats()
    print(f"\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ:")
    print(f"   èªå½™ã‚µã‚¤ã‚º: {stats['vocab_size']:,} ãƒˆãƒ¼ã‚¯ãƒ³")
    print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {stats['total_params']:,}")
    
    print("\n" + "=" * 70)
    print("ã‚³ãƒãƒ³ãƒ‰:")
    print("  <ãƒ†ã‚­ã‚¹ãƒˆ>    : ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ")
    print("  /temp <å€¤>    : æ¸©åº¦ã‚’è¨­å®š (0.1-2.0)")
    print("  /r <å€¤>       : APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆrã‚’è¨­å®š (-1.0-1.0)")
    print("  /len <å€¤>     : ç”Ÿæˆé•·ã•ã‚’è¨­å®š (10-1000)")
    print("  /topk <å€¤>    : Top-K ã‚’è¨­å®š (1-100)")
    print("  /quantum      : é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ON/OFF åˆ‡æ›¿")
    print("  /stats        : ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆã‚’è¡¨ç¤º")
    print("  /save <path>  : ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜")
    print("  /load <path>  : ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿")
    print("  /quit         : çµ‚äº†")
    print("=" * 70)
    
    temperature = 0.8
    dropout_r = -0.3
    max_length = 200
    top_k = 50
    quantum_sampling = True
    
    while True:
        try:
            user_input = input("\nğŸ”® > ").strip()
            
            if not user_input:
                continue
            
            if user_input.startswith("/quit"):
                print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™")
                break
            
            elif user_input.startswith("/temp"):
                try:
                    temp = float(user_input.split()[1])
                    temperature = max(0.1, min(2.0, temp))
                    print(f"ğŸŒ¡ï¸ æ¸©åº¦ã‚’ {temperature} ã«è¨­å®šã—ã¾ã—ãŸ")
                except:
                    print("âŒ ä½¿ç”¨æ³•: /temp <0.1-2.0>")
            
            elif user_input.startswith("/r "):
                try:
                    r = float(user_input.split()[1])
                    dropout_r = max(-1.0, min(1.0, r))
                    print(f"âš›ï¸ APQBãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ r ã‚’ {dropout_r} ã«è¨­å®šã—ã¾ã—ãŸ")
                except:
                    print("âŒ ä½¿ç”¨æ³•: /r <-1.0-1.0>")
            
            elif user_input.startswith("/len"):
                try:
                    length = int(user_input.split()[1])
                    max_length = max(10, min(1000, length))
                    print(f"ğŸ“ ç”Ÿæˆé•·ã•ã‚’ {max_length} ãƒˆãƒ¼ã‚¯ãƒ³ã«è¨­å®šã—ã¾ã—ãŸ")
                except:
                    print("âŒ ä½¿ç”¨æ³•: /len <10-1000>")
            
            elif user_input.startswith("/topk"):
                try:
                    k = int(user_input.split()[1])
                    top_k = max(1, min(100, k))
                    print(f"ğŸ¯ Top-K ã‚’ {top_k} ã«è¨­å®šã—ã¾ã—ãŸ")
                except:
                    print("âŒ ä½¿ç”¨æ³•: /topk <1-100>")
            
            elif user_input.startswith("/quantum"):
                quantum_sampling = not quantum_sampling
                status = "ON âš›ï¸" if quantum_sampling else "OFF ğŸ“Š"
                print(f"é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {status}")
            
            elif user_input.startswith("/stats"):
                stats = ai.get_stats()
                print("\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ:")
                for key, value in stats.items():
                    if isinstance(value, int) and value > 1000:
                        print(f"   {key}: {value:,}")
                    else:
                        print(f"   {key}: {value}")
            
            elif user_input.startswith("/save"):
                try:
                    path = user_input.split()[1]
                    ai.save(path)
                except:
                    print("âŒ ä½¿ç”¨æ³•: /save <ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
            
            elif user_input.startswith("/load"):
                try:
                    path = user_input.split()[1]
                    ai.load(path)
                except Exception as e:
                    print(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            
            else:
                print("\nğŸ“œ ç”Ÿæˆä¸­... ", end="", flush=True)
                
                generated = ai.generate(
                    prompt=user_input,
                    max_length=max_length,
                    temperature=temperature,
                    top_k=top_k,
                    dropout_r=dropout_r,
                    quantum_sampling=quantum_sampling
                )
                
                print("\n" + "-" * 50)
                print(generated)
                print("-" * 50)
                q_status = "âš›ï¸" if quantum_sampling else "ğŸ“Š"
                print(f"[æ¸©åº¦: {temperature}, r: {dropout_r}, é•·ã•: {max_length}, TopK: {top_k}, {q_status}]")
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™")
            break
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()


# ========== ãƒ¡ã‚¤ãƒ³ ==========
if __name__ == "__main__":
    import sys
    
    use_oasst = "--oasst" in sys.argv or "--openassistant" in sys.argv
    
    if "--demo" in sys.argv:
        demo(use_oasst=use_oasst)
    else:
        interactive_mode(use_oasst=use_oasst)
    
    # ä½¿ã„æ–¹:
    # python apqb_generative_ai_v2.py              # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§å¯¾è©±ãƒ¢ãƒ¼ãƒ‰
    # python apqb_generative_ai_v2.py --oasst     # OpenAssistantã§å¯¾è©±ãƒ¢ãƒ¼ãƒ‰
    # python apqb_generative_ai_v2.py --demo      # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ‡ãƒ¢
    # python apqb_generative_ai_v2.py --demo --oasst  # OpenAssistantã§ãƒ‡ãƒ¢

