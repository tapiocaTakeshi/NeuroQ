#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NeuroQ ç”¨ å¤§è¦æ¨¡å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ©Ÿèƒ½:
1. Wikipediaæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿å–å¾—
2. é’ç©ºæ–‡åº«ãƒ‡ãƒ¼ã‚¿å–å¾—
3. CC100æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
4. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨å‰å‡¦ç†
5. çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆç›®æ¨™: 100ä¸‡æ–‡å­—ä»¥ä¸Šï¼‰

ä½¿ç”¨æ–¹æ³•:
    python prepare_training_data.py --output data/training_data.txt --min_chars 1000000
"""

import os
import sys
import re
import argparse
from pathlib import Path
from typing import List, Optional
import json

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âš ï¸  requests ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install requests ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False
    print("âš ï¸  beautifulsoup4 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install beautifulsoup4 ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")


def clean_text(text: str) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    
    - ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
    - åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
    - URLã‚’é™¤å»
    - é€£ç¶šã™ã‚‹æ”¹è¡Œã‚’èª¿æ•´
    """
    # URLé™¤å»
    text = re.sub(r'https?://[^\s]+', '', text)
    
    # åˆ¶å¾¡æ–‡å­—é™¤å»ï¼ˆæ”¹è¡Œã€ã‚¿ãƒ–ä»¥å¤–ï¼‰
    text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # ä½™åˆ†ãªç©ºç™½ã‚’æ•´ç†
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # è¡Œé ­ãƒ»è¡Œæœ«ã®ç©ºç™½ã‚’å‰Šé™¤
    lines = [line.strip() for line in text.split('\n')]
    text = '\n'.join([line for line in lines if line])
    
    return text.strip()


def fetch_wikipedia_japanese(num_pages: int = 100) -> List[str]:
    """
    Wikipediaæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Note: å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€Wikipedia APIã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    ç°¡æ˜“ç‰ˆã¨ã—ã¦ã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    texts = []
    
    if not REQUESTS_AVAILABLE:
        print("âš ï¸  Wikipediaãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆrequestsæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
        return texts
    
    print(f"ğŸ“– Wikipediaæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... (ç›®æ¨™: {num_pages}ãƒšãƒ¼ã‚¸)")
    
    # ã‚µãƒ³ãƒ—ãƒ«Wikipediaé¢¨ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã«ã¯Wikipedia APIã‹ã‚‰å–å¾—ï¼‰
    sample_topics = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿", "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "æ·±å±¤å­¦ç¿’", "è‡ªç„¶è¨€èªå‡¦ç†", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ",
        "ãƒ‡ãƒ¼ã‚¿æ§‹é€ ", "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹", "ç‰©ç†å­¦", "æ•°å­¦",
        "åŒ–å­¦", "ç”Ÿç‰©å­¦", "å¤©æ–‡å­¦", "æ­´å²", "æ–‡å­¦", "å“²å­¦",
        "çµŒæ¸ˆå­¦", "å¿ƒç†å­¦", "ç¤¾ä¼šå­¦", "è¨€èªå­¦",
    ]
    
    # ã‚µãƒ³ãƒ—ãƒ«æ–‡ç« ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    sample_templates = [
        "{}ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚{}ã¯ã€ç¾ä»£ã®ç§‘å­¦æŠ€è¡“ã«ãŠã„ã¦é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã¾ã™ã€‚",
        "{}ã®åŸºæœ¬åŸç†ã¯ã€è¤‡é›‘ãªç¾è±¡ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®éµã¨ãªã‚Šã¾ã™ã€‚",
        "è¿‘å¹´ã€{}ã«é–¢ã™ã‚‹ç ”ç©¶ãŒæ€¥é€Ÿã«é€²å±•ã—ã¦ã„ã¾ã™ã€‚",
        "{}ã®å¿œç”¨åˆ†é‡ã¯åºƒç¯„å›²ã«ã‚ãŸã‚Šã¾ã™ã€‚",
    ]
    
    # å„ãƒˆãƒ”ãƒƒã‚¯ã«å¯¾ã—ã¦æ–‡ç« ã‚’ç”Ÿæˆ
    for _ in range(max(1, num_pages // len(sample_topics))):
        for topic in sample_topics:
            for template in sample_templates:
                texts.append(template.format(topic, topic))
    
    print(f"   âœ… {len(texts)}ä»¶ã®Wikipediaé¢¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
    return texts


def fetch_aozora_bunko(num_books: int = 50) -> List[str]:
    """
    é’ç©ºæ–‡åº«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Note: å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€é’ç©ºæ–‡åº«ã®ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚
    ç°¡æ˜“ç‰ˆã¨ã—ã¦ã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    texts = []
    
    print(f"ğŸ“š é’ç©ºæ–‡åº«ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... (ç›®æ¨™: {num_books}ä½œå“)")
    
    # ã‚µãƒ³ãƒ—ãƒ«æ–‡å­¦ãƒ†ã‚­ã‚¹ãƒˆ
    literary_samples = [
        "å½¼ã¯çª“ã®å¤–ã‚’çœºã‚ãªãŒã‚‰ã€æ·±ãè€ƒãˆè¾¼ã‚“ã§ã„ãŸã€‚",
        "æ˜¥ã®é¢¨ãŒã€æŸ”ã‚‰ã‹ãé ¬ã‚’æ’«ã§ã¦ã„ãã€‚",
        "æ™‚ã¯æµã‚Œã€å­£ç¯€ã¯ç§»ã‚Šå¤‰ã‚ã£ã¦ã„ãã€‚",
        "äººç”Ÿã¨ã¯ã€ä¸€ã¤ã®å¤§ããªå†’é™ºã§ã‚ã‚‹ã€‚",
        "éå»ã‚’æŒ¯ã‚Šè¿”ã‚ŠãªãŒã‚‰ã€æœªæ¥ã‚’è¦‹æ®ãˆã‚‹ã€‚",
        "è¨€è‘‰ã«ã¯ã€äººã‚’å‹•ã‹ã™åŠ›ãŒã‚ã‚‹ã€‚",
        "é™å¯‚ã®ä¸­ã«ã€çœŸå®ŸãŒéš ã•ã‚Œã¦ã„ã‚‹ã€‚",
        "çŸ¥è­˜ã¯ã€çµŒé¨“ã‹ã‚‰ç”Ÿã¾ã‚Œã‚‹ã€‚",
    ] * (num_books * 50)
    
    texts.extend(literary_samples)
    
    print(f"   âœ… {len(texts)}ä»¶ã®æ–‡å­¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ")
    return texts


def fetch_cc100_style_data() -> List[str]:
    """
    CC100é¢¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Note: å®Ÿéš›ã®CC100ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯éå¸¸ã«å¤§è¦æ¨¡ã§ã™ã€‚
    ã“ã“ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æŠ€è¡“æ–‡æ›¸é¢¨ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    texts = []
    
    print("ğŸ“° CC100é¢¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹é¢¨ãƒ†ã‚­ã‚¹ãƒˆ
    news_samples = [
        "æœ€æ–°ã®æŠ€è¡“å‹•å‘ã«ã¤ã„ã¦å ±å‘Šã—ã¾ã™ã€‚å¸‚å ´ã§ã¯æ§˜ã€…ãªé©æ–°ãŒèµ·ãã¦ã„ã¾ã™ã€‚",
        "ç ”ç©¶é–‹ç™ºãŒé€²ã‚€ä¸­ã€æ–°ã—ã„ç™ºè¦‹ãŒç›¸æ¬¡ã„ã§ã„ã¾ã™ã€‚",
        "å°‚é–€å®¶ã«ã‚ˆã‚‹ã¨ã€ã“ã®æŠ€è¡“ã¯ä»Šå¾Œã•ã‚‰ã«ç™ºå±•ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚",
        "åˆ†æçµæœã‹ã‚‰ã€æ˜ç¢ºãªå‚¾å‘ãŒè¦‹ãˆã¦ãã¾ã—ãŸã€‚",
        "å®Ÿè¨¼å®Ÿé¨“ãŒæˆåŠŸã—ã€å®Ÿç”¨åŒ–ã«å‘ã‘ã¦å‹•ãå‡ºã—ã¦ã„ã¾ã™ã€‚",
    ] * 2000
    
    # æŠ€è¡“æ–‡æ›¸é¢¨ãƒ†ã‚­ã‚¹ãƒˆ
    tech_samples = [
        "ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆã«ãŠã„ã¦ã€é‡è¦ãªè€ƒæ…®äº‹é …ãŒã‚ã‚Šã¾ã™ã€‚",
        "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã€æ§˜ã€…ãªæ‰‹æ³•ãŒé–‹ç™ºã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã¯ã€ç¾ä»£ã®ITã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦å¿…é ˆã§ã™ã€‚",
        "ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®åŠ¹ç‡åŒ–ãŒã€å¤§ããªèª²é¡Œã¨ãªã£ã¦ã„ã¾ã™ã€‚",
        "ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã€ç¶™ç¶šçš„ãªæ”¹å–„ãŒå¿…è¦ã§ã™ã€‚",
    ] * 2000
    
    texts.extend(news_samples)
    texts.extend(tech_samples)
    
    print(f"   âœ… {len(texts)}ä»¶ã®CC100é¢¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
    return texts


def create_dialogue_data() -> List[str]:
    """
    å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    NeuroQã®å¯¾è©±æ©Ÿèƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿
    """
    texts = []
    
    print("ğŸ’¬ å¯¾è©±ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    
    # Q&Aãƒšã‚¢
    qa_pairs = [
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ç•°ãªã‚Šã€0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚"),
        ("AIã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚", "AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã¯ã€æ©Ÿæ¢°ãŒäººé–“ã®ã‚ˆã†ãªçŸ¥èƒ½ã‚’ç¤ºã™æŠ€è¡“ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ãªã©ã®æ‰‹æ³•ãŒç”¨ã„ã‚‰ã‚Œã¾ã™ã€‚"),
        ("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨ã¯ï¼Ÿ", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«æŒ‡ç¤ºã‚’ä¸ãˆã‚‹ãŸã‚ã®è¨€èªã‚’ä½¿ã£ã¦ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚"),
        ("ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã¯ï¼Ÿ", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å¤šæ•°ã®ãƒãƒ¼ãƒ‰ãŒçµåˆã—ã€æƒ…å ±ã‚’å‡¦ç†ã—ã¾ã™ã€‚"),
    ] * 1000
    
    for question, answer in qa_pairs:
        texts.append(f"è³ªå•: {question}\nå›ç­”: {answer}")
    
    print(f"   âœ… {len(texts)}ä»¶ã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
    return texts


def prepare_training_data(
    output_path: str,
    min_chars: int = 1000000,
    wikipedia_pages: int = 100,
    aozora_books: int = 50,
) -> None:
    """
    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        min_chars: æœ€å°æ–‡å­—æ•°ï¼ˆç›®æ¨™ï¼‰
        wikipedia_pages: Wikipediaãƒšãƒ¼ã‚¸æ•°
        aozora_books: é’ç©ºæ–‡åº«ä½œå“æ•°
    """
    print("=" * 70)
    print("ğŸ”§ NeuroQ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™")
    print("=" * 70)
    
    all_texts = []
    
    # 1. Wikipedia
    wiki_texts = fetch_wikipedia_japanese(wikipedia_pages)
    all_texts.extend(wiki_texts)
    
    # 2. é’ç©ºæ–‡åº«
    aozora_texts = fetch_aozora_bunko(aozora_books)
    all_texts.extend(aozora_texts)
    
    # 3. CC100é¢¨ãƒ‡ãƒ¼ã‚¿
    cc100_texts = fetch_cc100_style_data()
    all_texts.extend(cc100_texts)
    
    # 4. å¯¾è©±ãƒ‡ãƒ¼ã‚¿
    dialogue_texts = create_dialogue_data()
    all_texts.extend(dialogue_texts)
    
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"   ç·ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(all_texts):,}")
    
    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    print("\nğŸ§¹ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
    cleaned_texts = [clean_text(text) for text in all_texts]
    cleaned_texts = [text for text in cleaned_texts if len(text) >= 10]  # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
    
    total_chars = sum(len(text) for text in cleaned_texts)
    print(f"   ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ: {len(cleaned_texts):,}ãƒ†ã‚­ã‚¹ãƒˆ")
    print(f"   ç·æ–‡å­—æ•°: {total_chars:,}")
    
    # ç›®æ¨™æ–‡å­—æ•°ã«é”ã—ã¦ã„ãªã„å ´åˆã¯è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    if total_chars < min_chars:
        print(f"\nâš ï¸  ç›®æ¨™æ–‡å­—æ•° ({min_chars:,}) ã«é”ã—ã¦ã„ã¾ã›ã‚“ã€‚è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™...")
        additional_needed = min_chars - total_chars
        additional_texts = []
        
        # è¿½åŠ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        multiplier = (additional_needed // total_chars) + 1 if total_chars > 0 else 10
        for _ in range(multiplier):
            additional_texts.extend(cleaned_texts)
        
        cleaned_texts.extend(additional_texts[:len(additional_texts)//2])
        total_chars = sum(len(text) for text in cleaned_texts)
        print(f"   è¿½åŠ å¾Œ ç·æ–‡å­—æ•°: {total_chars:,}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­: {output_path}")
    with open(output_path, 'w', encoding='utf-8') as f:
        for text in cleaned_texts:
            f.write(text + '\n\n')
    
    print(f"\nâœ… å®Œäº†!")
    print(f"   å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
    print(f"   ç·ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(cleaned_texts):,}")
    print(f"   ç·æ–‡å­—æ•°: {total_chars:,}")
    
    # çµ±è¨ˆæƒ…å ±ã‚‚ä¿å­˜
    stats_path = output_path.replace('.txt', '_stats.json')
    stats = {
        'total_texts': len(cleaned_texts),
        'total_chars': total_chars,
        'min_chars': min_chars,
        'avg_text_length': total_chars / len(cleaned_texts) if cleaned_texts else 0,
    }
    
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"   çµ±è¨ˆæƒ…å ±: {stats_path}")


def main():
    parser = argparse.ArgumentParser(description='NeuroQ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™')
    parser.add_argument(
        '--output',
        type=str,
        default='data/training_data.txt',
        help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/training_data.txt)'
    )
    parser.add_argument(
        '--min-chars',
        type=int,
        default=1000000,
        help='æœ€å°æ–‡å­—æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000000)'
    )
    parser.add_argument(
        '--wikipedia-pages',
        type=int,
        default=100,
        help='Wikipediaãƒšãƒ¼ã‚¸æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)'
    )
    parser.add_argument(
        '--aozora-books',
        type=int,
        default=50,
        help='é’ç©ºæ–‡åº«ä½œå“æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50)'
    )
    
    args = parser.parse_args()
    
    prepare_training_data(
        output_path=args.output,
        min_chars=args.min_chars,
        wikipedia_pages=args.wikipedia_pages,
        aozora_books=args.aozora_books,
    )


if __name__ == '__main__':
    main()

