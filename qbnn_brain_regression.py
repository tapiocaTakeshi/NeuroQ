#!/usr/bin/env python3
"""
QBNN Brain å›å¸°åˆ†æãƒ†ã‚¹ãƒˆ
========================
è„³å‹æ•£åœ¨QBNNã«ã‚ˆã‚‹å„ç¨®å›å¸°ã‚¿ã‚¹ã‚¯
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List
import math

from qbnn_brain import QBNNBrain, QBNNBrainTorch


def test_linear_regression():
    """ç·šå½¢å›å¸°ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ç·šå½¢å›å¸°ãƒ†ã‚¹ãƒˆ: y = 2x + 1")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    X_np = np.linspace(-1, 1, 50).reshape(-1, 1)
    y_np = 2 * X_np + 1 + np.random.randn(50, 1) * 0.1
    
    X = torch.tensor(X_np, dtype=torch.float32)
    y = torch.tensor(y_np, dtype=torch.float32)
    
    # ãƒ¢ãƒ‡ãƒ«
    model = QBNNBrainTorch(
        num_neurons=25,
        input_size=1,
        output_size=1,
        connection_density=0.25
    )
    
    # å­¦ç¿’
    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
    criterion = nn.MSELoss()
    
    print("\nğŸ“š å­¦ç¿’ä¸­...")
    for epoch in range(150):
        optimizer.zero_grad()
        pred = model(X, time_steps=2)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 30 == 0:
            print(f"   Epoch {epoch+1}: MSE = {loss.item():.4f}")
    
    # è©•ä¾¡
    model.eval()
    with torch.no_grad():
        pred = model(X, time_steps=2)
        mse = criterion(pred, y).item()
        
        # RÂ²ã‚¹ã‚³ã‚¢
        ss_res = ((y - pred) ** 2).sum().item()
        ss_tot = ((y - y.mean()) ** 2).sum().item()
        r2 = 1 - (ss_res / ss_tot)
    
    print(f"\nğŸ¯ çµæœ:")
    print(f"   MSE: {mse:.4f}")
    print(f"   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    
    return mse, r2


def test_quadratic_regression():
    """äºŒæ¬¡é–¢æ•°å›å¸°ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ äºŒæ¬¡é–¢æ•°å›å¸°ãƒ†ã‚¹ãƒˆ: y = xÂ² - 0.5x + 0.2")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    X_np = np.linspace(-1, 1, 50).reshape(-1, 1)
    y_np = X_np**2 - 0.5*X_np + 0.2 + np.random.randn(50, 1) * 0.05
    
    X = torch.tensor(X_np, dtype=torch.float32)
    y = torch.tensor(y_np, dtype=torch.float32)
    
    # ãƒ¢ãƒ‡ãƒ«
    model = QBNNBrainTorch(
        num_neurons=35,
        input_size=1,
        output_size=1,
        connection_density=0.3
    )
    
    # å­¦ç¿’
    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
    criterion = nn.MSELoss()
    
    print("\nğŸ“š å­¦ç¿’ä¸­...")
    for epoch in range(200):
        optimizer.zero_grad()
        pred = model(X, time_steps=3)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 40 == 0:
            print(f"   Epoch {epoch+1}: MSE = {loss.item():.4f}")
    
    # è©•ä¾¡
    model.eval()
    with torch.no_grad():
        pred = model(X, time_steps=3)
        mse = criterion(pred, y).item()
        
        ss_res = ((y - pred) ** 2).sum().item()
        ss_tot = ((y - y.mean()) ** 2).sum().item()
        r2 = 1 - (ss_res / ss_tot)
    
    print(f"\nğŸ¯ çµæœ:")
    print(f"   MSE: {mse:.4f}")
    print(f"   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    
    return mse, r2


def test_sin_regression():
    """siné–¢æ•°å›å¸°ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ siné–¢æ•°å›å¸°ãƒ†ã‚¹ãƒˆ: y = sin(Ï€x)")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    X_np = np.linspace(-1, 1, 60).reshape(-1, 1)
    y_np = np.sin(np.pi * X_np) + np.random.randn(60, 1) * 0.05
    
    X = torch.tensor(X_np, dtype=torch.float32)
    y = torch.tensor(y_np, dtype=torch.float32)
    
    # ãƒ¢ãƒ‡ãƒ«
    model = QBNNBrainTorch(
        num_neurons=40,
        input_size=1,
        output_size=1,
        connection_density=0.25
    )
    
    # å­¦ç¿’
    optimizer = torch.optim.Adam(model.parameters(), lr=0.015)
    criterion = nn.MSELoss()
    
    print("\nğŸ“š å­¦ç¿’ä¸­...")
    for epoch in range(200):
        optimizer.zero_grad()
        pred = model(X, time_steps=3)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 40 == 0:
            print(f"   Epoch {epoch+1}: MSE = {loss.item():.4f}")
    
    # è©•ä¾¡
    model.eval()
    with torch.no_grad():
        pred = model(X, time_steps=3)
        mse = criterion(pred, y).item()
        
        ss_res = ((y - pred) ** 2).sum().item()
        ss_tot = ((y - y.mean()) ** 2).sum().item()
        r2 = 1 - (ss_res / ss_tot)
    
    print(f"\nğŸ¯ çµæœ:")
    print(f"   MSE: {mse:.4f}")
    print(f"   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    
    return mse, r2


def test_multivariate_regression():
    """å¤šå¤‰é‡å›å¸°ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ å¤šå¤‰é‡å›å¸°ãƒ†ã‚¹ãƒˆ: y = 0.5xâ‚ + 0.3xâ‚‚ - 0.2xâ‚ƒ + 0.1")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 80
    X_np = np.random.randn(n_samples, 3)
    y_np = (0.5 * X_np[:, 0] + 0.3 * X_np[:, 1] - 0.2 * X_np[:, 2] + 0.1 
            + np.random.randn(n_samples) * 0.1).reshape(-1, 1)
    
    X = torch.tensor(X_np, dtype=torch.float32)
    y = torch.tensor(y_np, dtype=torch.float32)
    
    # ãƒ¢ãƒ‡ãƒ«
    model = QBNNBrainTorch(
        num_neurons=30,
        input_size=3,
        output_size=1,
        connection_density=0.25
    )
    
    # å­¦ç¿’
    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
    criterion = nn.MSELoss()
    
    print("\nğŸ“š å­¦ç¿’ä¸­...")
    for epoch in range(150):
        optimizer.zero_grad()
        pred = model(X, time_steps=3)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 30 == 0:
            print(f"   Epoch {epoch+1}: MSE = {loss.item():.4f}")
    
    # è©•ä¾¡
    model.eval()
    with torch.no_grad():
        pred = model(X, time_steps=3)
        mse = criterion(pred, y).item()
        
        ss_res = ((y - pred) ** 2).sum().item()
        ss_tot = ((y - y.mean()) ** 2).sum().item()
        r2 = 1 - (ss_res / ss_tot)
    
    print(f"\nğŸ¯ çµæœ:")
    print(f"   MSE: {mse:.4f}")
    print(f"   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    
    return mse, r2


def test_polynomial_regression():
    """å¤šé …å¼å›å¸°ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ å¤šé …å¼å›å¸°ãƒ†ã‚¹ãƒˆ: y = xÂ³ - xÂ² + 0.5x - 0.3")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    X_np = np.linspace(-1, 1, 50).reshape(-1, 1)
    y_np = X_np**3 - X_np**2 + 0.5*X_np - 0.3 + np.random.randn(50, 1) * 0.05
    
    X = torch.tensor(X_np, dtype=torch.float32)
    y = torch.tensor(y_np, dtype=torch.float32)
    
    # ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚ˆã‚Šè¤‡é›‘ãªé–¢æ•°ã«å¯¾å¿œã™ã‚‹ãŸã‚å¤§ãã‚ï¼‰
    model = QBNNBrainTorch(
        num_neurons=45,
        input_size=1,
        output_size=1,
        connection_density=0.3
    )
    
    # å­¦ç¿’
    optimizer = torch.optim.Adam(model.parameters(), lr=0.015)
    criterion = nn.MSELoss()
    
    print("\nğŸ“š å­¦ç¿’ä¸­...")
    for epoch in range(250):
        optimizer.zero_grad()
        pred = model(X, time_steps=4)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 50 == 0:
            print(f"   Epoch {epoch+1}: MSE = {loss.item():.4f}")
    
    # è©•ä¾¡
    model.eval()
    with torch.no_grad():
        pred = model(X, time_steps=4)
        mse = criterion(pred, y).item()
        
        ss_res = ((y - pred) ** 2).sum().item()
        ss_tot = ((y - y.mean()) ** 2).sum().item()
        r2 = 1 - (ss_res / ss_tot)
    
    print(f"\nğŸ¯ çµæœ:")
    print(f"   MSE: {mse:.4f}")
    print(f"   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    
    return mse, r2


def test_exponential_regression():
    """æŒ‡æ•°é–¢æ•°å›å¸°ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ æŒ‡æ•°é–¢æ•°å›å¸°ãƒ†ã‚¹ãƒˆ: y = exp(x) / 3")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    X_np = np.linspace(-1, 1, 50).reshape(-1, 1)
    y_np = np.exp(X_np) / 3 + np.random.randn(50, 1) * 0.05
    
    X = torch.tensor(X_np, dtype=torch.float32)
    y = torch.tensor(y_np, dtype=torch.float32)
    
    # ãƒ¢ãƒ‡ãƒ«
    model = QBNNBrainTorch(
        num_neurons=35,
        input_size=1,
        output_size=1,
        connection_density=0.25
    )
    
    # å­¦ç¿’
    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
    criterion = nn.MSELoss()
    
    print("\nğŸ“š å­¦ç¿’ä¸­...")
    for epoch in range(200):
        optimizer.zero_grad()
        pred = model(X, time_steps=3)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 40 == 0:
            print(f"   Epoch {epoch+1}: MSE = {loss.item():.4f}")
    
    # è©•ä¾¡
    model.eval()
    with torch.no_grad():
        pred = model(X, time_steps=3)
        mse = criterion(pred, y).item()
        
        ss_res = ((y - pred) ** 2).sum().item()
        ss_tot = ((y - y.mean()) ** 2).sum().item()
        r2 = 1 - (ss_res / ss_tot)
    
    print(f"\nğŸ¯ çµæœ:")
    print(f"   MSE: {mse:.4f}")
    print(f"   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    
    return mse, r2


def test_multi_output_regression():
    """å¤šå‡ºåŠ›å›å¸°ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ å¤šå‡ºåŠ›å›å¸°ãƒ†ã‚¹ãƒˆ: yâ‚ = sin(x), yâ‚‚ = cos(x)")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    X_np = np.linspace(-np.pi, np.pi, 60).reshape(-1, 1)
    y_np = np.hstack([
        np.sin(X_np) + np.random.randn(60, 1) * 0.05,
        np.cos(X_np) + np.random.randn(60, 1) * 0.05
    ])
    
    X = torch.tensor(X_np / np.pi, dtype=torch.float32)  # æ­£è¦åŒ–
    y = torch.tensor(y_np, dtype=torch.float32)
    
    # ãƒ¢ãƒ‡ãƒ«
    model = QBNNBrainTorch(
        num_neurons=45,
        input_size=1,
        output_size=2,
        connection_density=0.3
    )
    
    # å­¦ç¿’
    optimizer = torch.optim.Adam(model.parameters(), lr=0.015)
    criterion = nn.MSELoss()
    
    print("\nğŸ“š å­¦ç¿’ä¸­...")
    for epoch in range(200):
        optimizer.zero_grad()
        pred = model(X, time_steps=3)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 40 == 0:
            print(f"   Epoch {epoch+1}: MSE = {loss.item():.4f}")
    
    # è©•ä¾¡
    model.eval()
    with torch.no_grad():
        pred = model(X, time_steps=3)
        mse = criterion(pred, y).item()
        
        # å„å‡ºåŠ›ã®RÂ²
        r2_list = []
        for i in range(2):
            ss_res = ((y[:, i] - pred[:, i]) ** 2).sum().item()
            ss_tot = ((y[:, i] - y[:, i].mean()) ** 2).sum().item()
            r2_list.append(1 - (ss_res / ss_tot))
    
    print(f"\nğŸ¯ çµæœ:")
    print(f"   MSE: {mse:.4f}")
    print(f"   RÂ²(sin): {r2_list[0]:.4f}")
    print(f"   RÂ²(cos): {r2_list[1]:.4f}")
    print(f"   RÂ²å¹³å‡: {sum(r2_list)/2:.4f}")
    
    return mse, sum(r2_list)/2


def analyze_quantum_contribution():
    """é‡å­ã‚‚ã¤ã‚Œã®å¯„ä¸åˆ†æ"""
    print("\n" + "=" * 60)
    print("âš›ï¸ é‡å­ã‚‚ã¤ã‚Œã®å¯„ä¸åˆ†æ")
    print("=" * 60)
    
    # åŒã˜ãƒ‡ãƒ¼ã‚¿ã§æ¯”è¼ƒ
    np.random.seed(42)
    X_np = np.linspace(-1, 1, 50).reshape(-1, 1)
    y_np = np.sin(np.pi * X_np) + np.random.randn(50, 1) * 0.05
    
    X = torch.tensor(X_np, dtype=torch.float32)
    y = torch.tensor(y_np, dtype=torch.float32)
    
    results = []
    
    for lambda_val in [0.0, 0.1, 0.25, 0.5, 1.0]:
        print(f"\nğŸ“Œ Î» (ã‚‚ã¤ã‚Œå¼·åº¦) = {lambda_val}")
        
        # ãƒ¢ãƒ‡ãƒ«
        model = QBNNBrainTorch(
            num_neurons=35,
            input_size=1,
            output_size=1,
            connection_density=0.25
        )
        
        # Î»ã‚’å›ºå®š
        with torch.no_grad():
            model.lambda_entangle.fill_(lambda_val)
        
        # å­¦ç¿’
        optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
        criterion = nn.MSELoss()
        
        for epoch in range(150):
            optimizer.zero_grad()
            pred = model(X, time_steps=3)
            loss = criterion(pred, y)
            loss.backward()
            optimizer.step()
            
            # Î»ã‚’å›ºå®šã«ä¿ã¤
            with torch.no_grad():
                model.lambda_entangle.fill_(lambda_val)
        
        # è©•ä¾¡
        model.eval()
        with torch.no_grad():
            pred = model(X, time_steps=3)
            mse = criterion(pred, y).item()
            
            ss_res = ((y - pred) ** 2).sum().item()
            ss_tot = ((y - y.mean()) ** 2).sum().item()
            r2 = 1 - (ss_res / ss_tot)
        
        print(f"   MSE: {mse:.4f}, RÂ²: {r2:.4f}")
        results.append((lambda_val, mse, r2))
    
    print("\nğŸ“Š é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœã¾ã¨ã‚:")
    print("   Î»      | MSE     | RÂ²")
    print("   -------|---------|-------")
    for lam, mse, r2 in results:
        print(f"   {lam:.2f}   | {mse:.4f}  | {r2:.4f}")
    
    return results


def run_all_regression_tests():
    """å…¨å›å¸°ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print("=" * 60)
    print("ğŸ§  QBNN Brain - å›å¸°åˆ†æãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    results = {}
    
    # å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    mse, r2 = test_linear_regression()
    results['ç·šå½¢'] = {'MSE': mse, 'RÂ²': r2}
    
    mse, r2 = test_quadratic_regression()
    results['äºŒæ¬¡é–¢æ•°'] = {'MSE': mse, 'RÂ²': r2}
    
    mse, r2 = test_sin_regression()
    results['siné–¢æ•°'] = {'MSE': mse, 'RÂ²': r2}
    
    mse, r2 = test_multivariate_regression()
    results['å¤šå¤‰é‡'] = {'MSE': mse, 'RÂ²': r2}
    
    mse, r2 = test_polynomial_regression()
    results['å¤šé …å¼'] = {'MSE': mse, 'RÂ²': r2}
    
    mse, r2 = test_exponential_regression()
    results['æŒ‡æ•°é–¢æ•°'] = {'MSE': mse, 'RÂ²': r2}
    
    mse, r2 = test_multi_output_regression()
    results['å¤šå‡ºåŠ›'] = {'MSE': mse, 'RÂ²': r2}
    
    # é‡å­ã‚‚ã¤ã‚Œåˆ†æ
    quantum_results = analyze_quantum_contribution()
    
    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ“Š å›å¸°åˆ†æãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    
    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           QBNN Brain å›å¸°åˆ†æçµæœ                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤""")
    
    for name, res in results.items():
        r2_stars = "â­" * min(5, int(res['RÂ²'] * 5 + 0.5))
        print(f"â”‚ {name:10} | MSE: {res['MSE']:.4f} | RÂ²: {res['RÂ²']:.4f} {r2_stars:5}")
    
    print("""â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚               é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœåˆ†æ                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤""")
    
    best_lambda = max(quantum_results, key=lambda x: x[2])
    print(f"â”‚ æœ€é©Î»: {best_lambda[0]:.2f} (RÂ²={best_lambda[2]:.4f})              â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # å¹³å‡RÂ²
    avg_r2 = sum(r['RÂ²'] for r in results.values()) / len(results)
    print(f"\nâœ… å¹³å‡RÂ²ã‚¹ã‚³ã‚¢: {avg_r2:.4f}")
    
    # è©•ä¾¡
    if avg_r2 >= 0.9:
        grade = "A (å„ªç§€)"
    elif avg_r2 >= 0.8:
        grade = "B (è‰¯å¥½)"
    elif avg_r2 >= 0.7:
        grade = "C (æ™®é€š)"
    else:
        grade = "D (è¦æ”¹å–„)"
    
    print(f"ğŸ“ ç·åˆè©•ä¾¡: {grade}")
    
    return results


if __name__ == '__main__':
    run_all_regression_tests()

