#!/usr/bin/env python3
"""
NeuroQ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
====================================
äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨åˆæœŸåŒ–ã‚’æä¾›ã—ã¾ã™ã€‚

ä½¿ã„æ–¹:
    from neuroq_pretrained import load_pretrained_model

    model = load_pretrained_model(device='cuda')
"""

import torch
import os
import sys
from pathlib import Path


# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«è¨­å®š
DEFAULT_CONFIG = {
    'vocab_size': 8000,
    'embed_dim': 256,
    'hidden_dim': 512,
    'num_heads': 8,
    'num_layers': 6,
    'max_seq_len': 256,
    'dropout': 0.1,
    'lambda_entangle': 0.5,
}


def get_tokenizer_path():
    """
    å­¦ç¿’æ¸ˆã¿SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ‘ã‚¹ã‚’å–å¾—

    Returns:
        str: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ/home/user/ï¼‰ã‹ã‚‰æ¢ã™
    current_dir = Path(__file__).parent

    # å€™è£œãƒ‘ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
    candidate_paths = [
        current_dir / 'NeuroQ' / 'neuroq_tokenizer.model',
        current_dir / 'neuroq_tokenizer.model',
        current_dir / 'NeuroQ' / 'neuroq_tokenizer_8k.model',
        current_dir / 'neuroq_tokenizer_8k.model',
    ]

    for path in candidate_paths:
        if path.exists():
            return str(path)

    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’è¿”ã™
    return str(current_dir / 'NeuroQ' / 'neuroq_tokenizer.model')


def load_pretrained_tokenizer(tokenizer_path=None, verbose=True):
    """
    å­¦ç¿’æ¸ˆã¿SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰

    Args:
        tokenizer_path: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ¤œå‡ºï¼‰
        verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹

    Returns:
        tokenizer: NeuroQuantumTokenizer ã¾ãŸã¯ None if failed
    """
    try:
        # ãƒ‘ã‚¹ã®æ±ºå®š
        if tokenizer_path is None:
            tokenizer_path = get_tokenizer_path()

        if verbose:
            print(f"ğŸ“¦ Loading pretrained tokenizer from: {tokenizer_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(tokenizer_path):
            if verbose:
                print(f"âŒ Tokenizer file not found: {tokenizer_path}")
                print(f"")
                print(f"   å­¦ç¿’æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                print(f"   ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
                print(f"")
                print(f"   1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å­¦ç¿’:")
                print(f"      $ python train_sentencepiece_tokenizer.py")
                print(f"")
                print(f"   2. æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®:")
                print(f"      neuroq_tokenizer.model ã‚’ {Path(__file__).parent} ã«é…ç½®")
                print(f"")
            return None

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = os.path.getsize(tokenizer_path)
        if verbose:
            print(f"ğŸ“Š File size: {file_size / 1024:.2f} KB")

        # neuroquantum_layered ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        neuroq_module_path = Path(__file__).parent / 'NeuroQ' / 'neuroq-runpod'
        if str(neuroq_module_path) not in sys.path:
            sys.path.insert(0, str(neuroq_module_path))

        from neuroquantum_layered import NeuroQuantumTokenizer

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
        tokenizer = NeuroQuantumTokenizer(model_file=tokenizer_path)

        if verbose:
            print(f"âœ… Tokenizer loaded successfully")
            print(f"   Vocabulary size: {tokenizer.vocab_size}")

        return tokenizer

    except Exception as e:
        if verbose:
            print(f"âŒ Error loading tokenizer: {e}")
            import traceback
            traceback.print_exc()
        return None


def create_model_from_config(config=None, tokenizer=None, device='cpu'):
    """
    è¨­å®šã‹ã‚‰æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆé‡ã¿ã¯åˆæœŸåŒ–ã®ã¿ï¼‰

    Args:
        config: ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ï¼‰
        tokenizer: NeuroQuantumTokenizerï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ï¼‰
        device: ãƒ‡ãƒã‚¤ã‚¹

    Returns:
        tuple: (model, tokenizer)
    """
    if config is None:
        config = DEFAULT_CONFIG.copy()

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯ä½¿ç”¨
    if tokenizer is None:
        tokenizer = load_pretrained_tokenizer(verbose=True)
        if tokenizer is None:
            print("âš ï¸ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            from neuroquantum_layered import NeuroQuantumTokenizer
            tokenizer = NeuroQuantumTokenizer(vocab_size=config['vocab_size'])

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™ã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦è¨­å®šã‚’æ›´æ–°
    config['vocab_size'] = tokenizer.vocab_size

    # neuroquantum_layered ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    neuroq_module_path = Path(__file__).parent / 'NeuroQ' / 'neuroq-runpod'
    if str(neuroq_module_path) not in sys.path:
        sys.path.insert(0, str(neuroq_module_path))

    from neuroquantum_layered import NeuroQuantum, NeuroQuantumConfig

    model_config = NeuroQuantumConfig(
        vocab_size=config['vocab_size'],
        embed_dim=config['embed_dim'],
        hidden_dim=config['hidden_dim'],
        num_heads=config['num_heads'],
        num_layers=config['num_layers'],
        max_seq_len=config.get('max_seq_len', 256),
        dropout=config.get('dropout', 0.1),
        lambda_entangle=config.get('lambda_entangle', 0.5),
    )

    model = NeuroQuantum(model_config).to(device)
    model.eval()

    print(f"âœ… æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ")
    print(f"   vocab_size: {config['vocab_size']}")
    print(f"   embed_dim: {config['embed_dim']}")
    print(f"   hidden_dim: {config['hidden_dim']}")

    return model, tokenizer


if __name__ == "__main__":
    """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    print("=" * 60)
    print("ğŸ§  NeuroQ Tokenizer Loader - Test")
    print("=" * 60)

    # ãƒ‡ãƒã‚¤ã‚¹ã®æ±ºå®š
    if torch.cuda.is_available():
        device = 'cuda'
        print(f"ğŸ® Using GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = 'cpu'
        print("ğŸ’» Using CPU")

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
    print("\nğŸ“¦ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    tokenizer = load_pretrained_tokenizer()

    if tokenizer is not None:
        print("\nâœ… Tokenizer test successful!")
        print(f"   Vocabulary size: {tokenizer.vocab_size}")

        # ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰/ãƒ‡ã‚³ãƒ¼ãƒ‰
        test_text = "ã“ã‚“ã«ã¡ã¯ã€NeuroQã§ã™ã€‚"
        encoded = tokenizer.encode(test_text, add_special=False)
        decoded = tokenizer.decode(encoded, skip_special=True)
        print(f"\n   ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ: {test_text}")
        print(f"   ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰: {encoded[:10]}... ({len(encoded)} tokens)")
        print(f"   ãƒ‡ã‚³ãƒ¼ãƒ‰: {decoded}")

        # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        print("\nğŸ§  ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...")
        model, tokenizer = create_model_from_config(tokenizer=tokenizer, device=device)
        print(f"   Model type: {type(model).__name__}")
        print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")
    else:
        print("\nâŒ Tokenizer test failed")
        print("   ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ...")
        model, tokenizer = create_model_from_config(device=device)

    print("=" * 60)
