#!/usr/bin/env python3
"""
APQB Generative AI - è«–æ–‡ã«åŸºã¥ãç”ŸæˆAI

è«–æ–‡ã®ç‰¹æ€§ã‚’æ´»ç”¨:
1. è¤‡ç´ è§’åº¦ç©ºé–“: z = e^{i2Î¸} ã§ã®åŠ¹ç‡çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–
2. å¹¾ä½•å­¦çš„åˆ¶ç´„: rÂ² + TÂ² = 1 ã«ã‚ˆã‚‹è‡ªç„¶ãªæ­£å‰‡åŒ–
3. å¤šä½“ç›¸é–¢: Q_k(Î¸) ã«ã‚ˆã‚‹é«˜æ¬¡ç‰¹å¾´é‡ã®è¡¨ç¾
4. æ§‹é€ çš„åŒå‹æ€§: NN â‰… APQBè¤‡ç´ å¤šé …å¼
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from typing import List, Tuple, Optional
from collections import Counter
import math

print("=" * 70)
print("ğŸ§ âš›ï¸ APQB Generative AI - è«–æ–‡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè£…")
print("=" * 70)

# ========================================================================
# 1. APQBã‚³ã‚¢ - è«–æ–‡ã®æ•°å­¦çš„å®šç¾©
# ========================================================================

class APQBCore:
    """è«–æ–‡ã«åŸºã¥ãAPQBã®æ•°å­¦çš„ã‚³ã‚¢"""
    
    @staticmethod
    def theta_to_r(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ r = cos(2Î¸)"""
        return torch.cos(2 * theta)
    
    @staticmethod
    def theta_to_T(theta: torch.Tensor) -> torch.Tensor:
        """Î¸ â†’ T = |sin(2Î¸)|"""
        return torch.abs(torch.sin(2 * theta))
    
    @staticmethod
    def theta_to_z(theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Î¸ â†’ z = e^{i2Î¸} (å®Ÿéƒ¨ã¨è™šéƒ¨)"""
        return torch.cos(2 * theta), torch.sin(2 * theta)
    
    @staticmethod
    def Q_k(theta: torch.Tensor, k: int) -> torch.Tensor:
        """
        kä½“ç›¸é–¢é–¢æ•°
        Q_k(Î¸) = cos(2kÎ¸) if k is even
        Q_k(Î¸) = sin(2kÎ¸) if k is odd
        """
        if k % 2 == 0:
            return torch.cos(2 * k * theta)
        else:
            return torch.sin(2 * k * theta)
    
    @staticmethod
    def complex_polynomial(theta: torch.Tensor, A_real: torch.Tensor, 
                          A_imag: torch.Tensor, max_order: int) -> torch.Tensor:
        """
        è¤‡ç´ å¤šé …å¼: F(Î¸) = Î£ A_k z^k ã®å®Ÿéƒ¨
        è«–æ–‡ã®æ ¸å¿ƒ: NNã®å¤šé …å¼å±•é–‹ã¨ã®åŒå‹æ€§
        """
        z_real, z_imag = APQBCore.theta_to_z(theta)
        
        result = torch.zeros_like(theta)
        z_k_real = torch.ones_like(theta)
        z_k_imag = torch.zeros_like(theta)
        
        for k in range(max_order + 1):
            # A_k * z^k ã®å®Ÿéƒ¨
            if k < A_real.shape[-1]:
                term = A_real[..., k] * z_k_real - A_imag[..., k] * z_k_imag
                result = result + term
            
            # z^{k+1} = z^k * z
            new_real = z_k_real * z_real - z_k_imag * z_imag
            new_imag = z_k_real * z_imag + z_k_imag * z_real
            z_k_real, z_k_imag = new_real, new_imag
        
        return result


# ========================================================================
# 2. APQBæ³¨æ„æ©Ÿæ§‹ - è¤‡ç´ è§’åº¦ç©ºé–“ã§ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
# ========================================================================

class APQBAttention(nn.Module):
    """
    APQBæ³¨æ„æ©Ÿæ§‹
    
    å¾“æ¥: Attention(Q,K,V) = softmax(QK^T/âˆšd)V
    APQB: è¤‡ç´ è§’åº¦ç©ºé–“ã§ã®æ³¨æ„é‡ã¿è¨ˆç®—
    """
    
    def __init__(self, embed_dim: int, num_heads: int = 4, max_order: int = 4):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.max_order = max_order
        self.scale = self.head_dim ** -0.5
        
        # Q, K, V æŠ•å½±
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        # APQBè§’åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (å­¦ç¿’å¯èƒ½)
        self.theta = nn.Parameter(torch.rand(num_heads) * np.pi / 4)
        
        # å¤šä½“ç›¸é–¢ã®é‡ã¿
        self.correlation_weights = nn.Parameter(torch.ones(max_order + 1) / (max_order + 1))
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        B, T, _ = x.shape
        
        # Q, K, V ã‚’è¨ˆç®—
        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        
        # æ¨™æº–ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        
        # APQBå¤šä½“ç›¸é–¢ã«ã‚ˆã‚‹å¤‰èª¿
        theta = torch.sigmoid(self.theta) * np.pi / 2
        for h in range(self.num_heads):
            for k_order in range(1, self.max_order + 1):
                Q_k = APQBCore.Q_k(theta[h:h+1], k_order)
                weight = self.correlation_weights[k_order]
                # ç›¸é–¢ã«ã‚ˆã‚‹å¤‰èª¿ï¼ˆä½ç›¸ã‚·ãƒ•ãƒˆï¼‰
                scores[:, h] = scores[:, h] + weight * Q_k * 0.1
        
        # ãƒã‚¹ã‚¯é©ç”¨
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã¨å‡ºåŠ›
        attn = F.softmax(scores, dim=-1)
        
        # APQBæ¸©åº¦ã«ã‚ˆã‚‹æ­£å‰‡åŒ–ï¼ˆè«–æ–‡ã®Tï¼‰
        T_vals = APQBCore.theta_to_T(theta)
        dropout_mask = torch.rand_like(attn) > T_vals.mean()
        attn = attn * dropout_mask.float() / (1 - T_vals.mean() + 1e-8)
        
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).contiguous().view(B, T, self.embed_dim)
        
        return self.out_proj(out)


# ========================================================================
# 3. APQBãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
# ========================================================================

class APQBTransformerBlock(nn.Module):
    """
    APQBãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
    
    è«–æ–‡ã®ç‰¹æ€§:
    - è¤‡ç´ å¤šé …å¼ã«ã‚ˆã‚‹ç‰¹å¾´å¤‰æ›
    - å¹¾ä½•å­¦çš„åˆ¶ç´„ã«ã‚ˆã‚‹æ­£å‰‡åŒ–
    - å¤šä½“ç›¸é–¢ã«ã‚ˆã‚‹é«˜æ¬¡ç‰¹å¾´
    """
    
    def __init__(self, embed_dim: int, num_heads: int = 4, 
                 ff_dim: int = None, max_order: int = 4):
        super().__init__()
        ff_dim = ff_dim or embed_dim * 4
        self.max_order = max_order
        
        # APQBæ³¨æ„æ©Ÿæ§‹
        self.attention = APQBAttention(embed_dim, num_heads, max_order)
        
        # Layer Norm
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        # APQB Feed-Forward (è¤‡ç´ å¤šé …å¼å±•é–‹)
        self.theta_ff = nn.Parameter(torch.rand(ff_dim) * np.pi / 4)
        self.A_real = nn.Parameter(torch.randn(ff_dim, max_order + 1) * 0.1)
        self.A_imag = nn.Parameter(torch.randn(ff_dim, max_order + 1) * 0.1)
        
        self.ff_in = nn.Linear(embed_dim, ff_dim)
        self.ff_out = nn.Linear(ff_dim, embed_dim)
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        # Self-attention with residual
        x = x + self.attention(self.norm1(x), mask)
        
        # APQB Feed-Forward with residual
        h = self.ff_in(self.norm2(x))
        
        # è¤‡ç´ å¤šé …å¼å¤‰æ›
        theta = torch.sigmoid(self.theta_ff) * np.pi / 2
        h = h * APQBCore.complex_polynomial(
            theta.unsqueeze(0).unsqueeze(0).expand(x.shape[0], x.shape[1], -1),
            self.A_real.unsqueeze(0).unsqueeze(0).expand(x.shape[0], x.shape[1], -1, -1),
            self.A_imag.unsqueeze(0).unsqueeze(0).expand(x.shape[0], x.shape[1], -1, -1),
            self.max_order
        )
        
        h = F.gelu(h)
        x = x + self.ff_out(h)
        
        return x
    
    def get_constraint_loss(self) -> torch.Tensor:
        """å¹¾ä½•å­¦çš„åˆ¶ç´„ rÂ² + TÂ² = 1 ã‹ã‚‰ã®é€¸è„±"""
        theta = torch.sigmoid(self.theta_ff) * np.pi / 2
        r = APQBCore.theta_to_r(theta)
        T = APQBCore.theta_to_T(theta)
        return ((r**2 + T**2 - 1).abs()).mean()


# ========================================================================
# 4. APQBç”Ÿæˆãƒ¢ãƒ‡ãƒ«
# ========================================================================

class APQBGenerativeModel(nn.Module):
    """
    APQBç”Ÿæˆãƒ¢ãƒ‡ãƒ«
    
    è«–æ–‡ã®æ§‹é€ çš„åŒå‹æ€§ã‚’æ´»ç”¨:
    NNå¤šé …å¼ â‰… APQBè¤‡ç´ å¤šé …å¼
    """
    
    def __init__(self, vocab_size: int, embed_dim: int = 128,
                 num_heads: int = 4, num_layers: int = 4,
                 max_seq_len: int = 256, max_order: int = 4):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.max_seq_len = max_seq_len
        self.max_order = max_order
        
        # åŸ‹ã‚è¾¼ã¿å±¤
        self.token_embed = nn.Embedding(vocab_size, embed_dim)
        self.pos_embed = nn.Embedding(max_seq_len, embed_dim)
        
        # APQBé‡å­ãƒã‚¤ã‚ºï¼ˆå­¦ç¿’å¯èƒ½ãªè§’åº¦ï¼‰
        self.embed_theta = nn.Parameter(torch.rand(embed_dim) * np.pi / 4)
        
        # ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
        self.blocks = nn.ModuleList([
            APQBTransformerBlock(embed_dim, num_heads, max_order=max_order)
            for _ in range(num_layers)
        ])
        
        # å‡ºåŠ›å±¤
        self.norm = nn.LayerNorm(embed_dim)
        self.output = nn.Linear(embed_dim, vocab_size)
        
        # é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.sampling_theta = nn.Parameter(torch.tensor(np.pi / 4))
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        B, T = x.shape
        
        # ä½ç½®æƒ…å ±
        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, -1)
        
        # åŸ‹ã‚è¾¼ã¿
        x = self.token_embed(x) + self.pos_embed(pos)
        
        # APQBé‡å­ãƒã‚¤ã‚ºï¼ˆè«–æ–‡ã®T: æ¸©åº¦/ä¹±é›‘ã•ï¼‰
        if self.training:
            theta = torch.sigmoid(self.embed_theta) * np.pi / 2
            T_noise = APQBCore.theta_to_T(theta)
            noise = torch.randn_like(x) * T_noise.unsqueeze(0).unsqueeze(0) * 0.1
            x = x + noise
        
        # å› æœãƒã‚¹ã‚¯
        if mask is None:
            mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)
        
        # ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
        for block in self.blocks:
            x = block(x, mask)
        
        # å‡ºåŠ›
        x = self.norm(x)
        return self.output(x)
    
    def get_total_constraint_loss(self) -> torch.Tensor:
        """å…¨å±¤ã®å¹¾ä½•å­¦çš„åˆ¶ç´„æå¤±"""
        loss = torch.tensor(0.0, device=next(self.parameters()).device)
        for block in self.blocks:
            loss = loss + block.get_constraint_loss()
        return loss / len(self.blocks)
    
    @torch.no_grad()
    def generate(self, prompt_ids: torch.Tensor, max_new_tokens: int = 100,
                 temperature: float = 1.0, top_k: int = 50) -> torch.Tensor:
        """APQBé‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        self.eval()
        generated = prompt_ids.clone()
        
        for _ in range(max_new_tokens):
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¶é™
            context = generated[:, -self.max_seq_len:]
            
            # æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬
            logits = self(context)[:, -1, :] / temperature
            
            # Top-k ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_k > 0:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = float('-inf')
            
            probs = F.softmax(logits, dim=-1)
            
            # APQBé‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            next_token = self._quantum_sample(probs)
            generated = torch.cat([generated, next_token], dim=1)
        
        return generated
    
    def _quantum_sample(self, probs: torch.Tensor) -> torch.Tensor:
        """
        APQBé‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        
        è«–æ–‡ã® z = e^{i2Î¸} ã‚’ä½¿ç”¨ã—ãŸç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        """
        B, V = probs.shape
        
        # é‡å­ä¹±æ•°ç”Ÿæˆï¼ˆè¤‡æ•°ã®å¤šä½“ç›¸é–¢ã‚’ä½¿ç”¨ï¼‰
        theta = torch.sigmoid(self.sampling_theta) * np.pi / 2
        quantum_rand = torch.zeros(B, device=probs.device)
        
        for k in range(1, self.max_order + 1):
            # Q_k ç›¸é–¢ã‚’ä½¿ç”¨
            Q_k = APQBCore.Q_k(theta, k)
            rand_phase = torch.rand(B, device=probs.device) * 2 * np.pi
            quantum_rand += torch.sin(rand_phase + k * theta.item()) / k
        
        quantum_rand = (quantum_rand - quantum_rand.min()) / (quantum_rand.max() - quantum_rand.min() + 1e-8)
        
        # ç´¯ç©ç¢ºç‡ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        cumsum = probs.cumsum(dim=-1)
        next_token = (cumsum < quantum_rand.unsqueeze(1)).sum(dim=-1, keepdim=True)
        next_token = next_token.clamp(0, V - 1)
        
        return next_token


# ========================================================================
# 5. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
# ========================================================================

class SimpleTokenizer:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"""
    
    def __init__(self):
        self.special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']
        self.token_to_idx = {}
        self.idx_to_token = {}
    
    def train(self, texts: List[str]):
        chars = set()
        for text in texts:
            chars.update(text)
        
        vocab = self.special_tokens + sorted(chars)
        self.token_to_idx = {t: i for i, t in enumerate(vocab)}
        self.idx_to_token = {i: t for t, i in self.token_to_idx.items()}
        
        print(f"ğŸ“š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: {len(self.token_to_idx)} ãƒˆãƒ¼ã‚¯ãƒ³")
        return self
    
    def encode(self, text: str) -> List[int]:
        return [self.token_to_idx.get(c, self.token_to_idx['<UNK>']) for c in text]
    
    def decode(self, ids) -> str:
        result = []
        for idx in ids:
            if isinstance(idx, torch.Tensor):
                idx = idx.item()
            token = self.idx_to_token.get(idx, '')
            if token not in self.special_tokens:
                result.append(token)
        return ''.join(result)
    
    @property
    def vocab_size(self) -> int:
        return len(self.token_to_idx)


class TextDataset(Dataset):
    def __init__(self, text: str, tokenizer: SimpleTokenizer, seq_len: int = 128):
        self.data = torch.tensor(tokenizer.encode(text), dtype=torch.long)
        self.seq_len = seq_len
        self.vocab_size = tokenizer.vocab_size
    
    def __len__(self):
        return max(0, len(self.data) - self.seq_len - 1)
    
    def __getitem__(self, idx):
        return self.data[idx:idx+self.seq_len], self.data[idx+1:idx+self.seq_len+1]


# ========================================================================
# 6. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
# ========================================================================

def get_training_data():
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚½ãƒ¼ã‚¹ï¼‰"""
    
    # ã¾ãšHugging Faceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è©¦ã™
    try:
        from datasets import load_dataset
        print("ğŸ“¥ Hugging Face ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—ä¸­...")
        
        # WikiTextãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
        texts = [item['text'] for item in dataset if len(item['text']) > 50][:1000]
        
        if texts:
            text = '\n'.join(texts)
            print(f"âœ… WikiText-2: {len(text):,} æ–‡å­—å–å¾—")
            return text
    except Exception as e:
        print(f"âš ï¸ Hugging Faceå–å¾—å¤±æ•—: {e}")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: çµ„ã¿è¾¼ã¿ãƒ†ã‚­ã‚¹ãƒˆ
    print("ğŸ“ çµ„ã¿è¾¼ã¿å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨...")
    
    training_text = """
    The field of artificial intelligence has seen remarkable progress in recent years.
    Machine learning algorithms can now process vast amounts of data and identify complex patterns.
    Neural networks have revolutionized many areas of computer science and technology.
    Deep learning models have achieved superhuman performance in various tasks.
    Natural language processing enables computers to understand and generate human language.
    Computer vision systems can recognize objects, faces, and scenes with high accuracy.
    Reinforcement learning allows agents to learn optimal strategies through trial and error.
    
    Quantum computing represents a fundamental shift in computational paradigms.
    Quantum bits or qubits can exist in superposition states unlike classical bits.
    Entanglement allows quantum systems to exhibit correlations impossible in classical physics.
    Quantum algorithms like Shor's can factor large numbers exponentially faster.
    Quantum machine learning combines quantum computing with artificial intelligence.
    The APQB framework bridges neural networks and quantum many-body systems.
    
    Mathematics provides the foundation for both artificial intelligence and physics.
    Linear algebra describes transformations in high-dimensional vector spaces.
    Probability theory quantifies uncertainty and enables statistical inference.
    Calculus allows optimization of continuous functions and gradient descent.
    Complex numbers and their geometry connect algebra with trigonometry.
    The exponential map links the real line to the unit circle in the complex plane.
    
    The universe operates according to fundamental physical laws.
    Quantum mechanics describes the behavior of particles at atomic scales.
    General relativity explains gravity as the curvature of spacetime.
    Statistical mechanics connects microscopic dynamics to macroscopic properties.
    Information theory quantifies the fundamental limits of communication.
    Thermodynamics governs the flow of energy and the arrow of time.
    
    Technology continues to transform human society in profound ways.
    The internet connects billions of people across the globe.
    Smartphones have become ubiquitous tools for communication and information.
    Social media platforms enable new forms of human interaction.
    Autonomous vehicles promise to revolutionize transportation.
    Renewable energy technologies offer hope for sustainable development.
    
    Science advances through observation, hypothesis, and experimentation.
    The scientific method provides a systematic approach to understanding nature.
    Peer review ensures the quality and reliability of scientific findings.
    Interdisciplinary research combines insights from multiple fields.
    Open science promotes transparency and reproducibility in research.
    Collaboration accelerates the pace of scientific discovery.
    """ * 10  # ãƒ‡ãƒ¼ã‚¿é‡ã‚’å¢—ã‚„ã™
    
    print(f"âœ… çµ„ã¿è¾¼ã¿ãƒ†ã‚­ã‚¹ãƒˆ: {len(training_text):,} æ–‡å­—")
    return training_text


# ========================================================================
# 7. APQBç”ŸæˆAI ã‚¯ãƒ©ã‚¹
# ========================================================================

class APQBGenerativeAI:
    """è«–æ–‡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ãAPQBç”ŸæˆAI"""
    
    def __init__(self, embed_dim: int = 128, num_heads: int = 4,
                 num_layers: int = 4, max_order: int = 4):
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.max_order = max_order
        
        self.model = None
        self.tokenizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def train(self, text: str, epochs: int = 30, batch_size: int = 16,
              lr: float = 0.001, seq_len: int = 64, constraint_weight: float = 0.1):
        """å­¦ç¿’"""
        print(f"\nğŸ§ âš›ï¸ APQBç”ŸæˆAI å­¦ç¿’é–‹å§‹")
        print(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        print(f"   è«–æ–‡ç‰¹æ€§: è¤‡ç´ è§’åº¦ç©ºé–“ + å¹¾ä½•å­¦çš„åˆ¶ç´„ + å¤šä½“ç›¸é–¢")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        self.tokenizer = SimpleTokenizer()
        self.tokenizer.train([text])
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        dataset = TextDataset(text, self.tokenizer, seq_len)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
        
        print(f"   èªå½™ã‚µã‚¤ã‚º: {self.tokenizer.vocab_size}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(dataset):,} ã‚µãƒ³ãƒ—ãƒ«")
        print(f"   å¤šä½“ç›¸é–¢æ¬¡æ•°: {self.max_order}")
        
        # ãƒ¢ãƒ‡ãƒ«
        self.model = APQBGenerativeModel(
            vocab_size=self.tokenizer.vocab_size,
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            num_layers=self.num_layers,
            max_seq_len=seq_len,
            max_order=self.max_order
        ).to(self.device)
        
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)
        criterion = nn.CrossEntropyLoss()
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        self.model.train()
        losses = []
        
        print("\nğŸ“š å­¦ç¿’ä¸­...")
        for epoch in range(epochs):
            epoch_loss = 0
            constraint_loss_total = 0
            
            for x, y in dataloader:
                x, y = x.to(self.device), y.to(self.device)
                
                optimizer.zero_grad()
                
                # é †ä¼æ’­
                logits = self.model(x)
                ce_loss = criterion(logits.view(-1, self.tokenizer.vocab_size), y.view(-1))
                
                # å¹¾ä½•å­¦çš„åˆ¶ç´„æå¤±ï¼ˆè«–æ–‡: rÂ² + TÂ² = 1ï¼‰
                constraint_loss = self.model.get_total_constraint_loss()
                
                # ç·æå¤±
                loss = ce_loss + constraint_weight * constraint_loss
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                epoch_loss += ce_loss.item()
                constraint_loss_total += constraint_loss.item()
            
            scheduler.step()
            
            avg_loss = epoch_loss / len(dataloader)
            avg_constraint = constraint_loss_total / len(dataloader)
            losses.append(avg_loss)
            
            if (epoch + 1) % 5 == 0:
                print(f"   Epoch {epoch+1}/{epochs}: CE={avg_loss:.4f}, åˆ¶ç´„={avg_constraint:.4f}")
        
        print("âœ… å­¦ç¿’å®Œäº†ï¼")
        return losses
    
    def generate(self, prompt: str = "", max_length: int = 200,
                 temperature: float = 1.0, top_k: int = 50) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if self.model is None:
            raise ValueError("å…ˆã«train()ã§å­¦ç¿’ã—ã¦ãã ã•ã„")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        if prompt:
            prompt_ids = torch.tensor([self.tokenizer.encode(prompt)], dtype=torch.long)
        else:
            prompt_ids = torch.tensor([[self.tokenizer.token_to_idx['<BOS>']]], dtype=torch.long)
        
        prompt_ids = prompt_ids.to(self.device)
        
        # ç”Ÿæˆ
        generated = self.model.generate(
            prompt_ids,
            max_new_tokens=max_length,
            temperature=temperature,
            top_k=top_k
        )
        
        return self.tokenizer.decode(generated[0])
    
    def get_quantum_stats(self) -> dict:
        """é‡å­çµ±è¨ˆæƒ…å ±"""
        if self.model is None:
            return {}
        
        stats = {'layers': []}
        for i, block in enumerate(self.model.blocks):
            theta = torch.sigmoid(block.theta_ff) * np.pi / 2
            r = APQBCore.theta_to_r(theta)
            T = APQBCore.theta_to_T(theta)
            
            stats['layers'].append({
                'r_mean': r.mean().item(),
                'T_mean': T.mean().item(),
                'constraint': (r**2 + T**2).mean().item()
            })
        
        return stats


# ========================================================================
# 8. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================================================

def main():
    print("\nğŸ“¥ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    training_text = get_training_data()
    
    print("\nğŸ—ï¸ APQBç”ŸæˆAI ã‚’æ§‹ç¯‰ä¸­...")
    ai = APQBGenerativeAI(
        embed_dim=96,
        num_heads=4,
        num_layers=3,
        max_order=4
    )
    
    # å­¦ç¿’
    ai.train(
        training_text,
        epochs=25,
        batch_size=16,
        seq_len=64,
        constraint_weight=0.1
    )
    
    # é‡å­çµ±è¨ˆ
    stats = ai.get_quantum_stats()
    print("\nâš›ï¸ é‡å­çµ±è¨ˆ (è«–æ–‡: rÂ² + TÂ² = 1):")
    for i, layer_stats in enumerate(stats['layers']):
        print(f"   Layer {i}: r={layer_stats['r_mean']:.3f}, T={layer_stats['T_mean']:.3f}, rÂ²+TÂ²={layer_stats['constraint']:.4f}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    print("\n" + "=" * 70)
    print("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆçµæœ")
    print("=" * 70)
    
    prompts = ["The ", "Quantum ", "Neural ", "Science ", "Technology "]
    
    for prompt in prompts:
        print(f"\nğŸ”® ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ã€Œ{prompt}ã€")
        print("-" * 50)
        text = ai.generate(prompt, max_length=100, temperature=0.8)
        print(f"{text}")
    
    # æ¸©åº¦ã«ã‚ˆã‚‹é•ã„
    print("\n" + "=" * 70)
    print("ğŸŒ¡ï¸ æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹é•ã„")
    print("=" * 70)
    
    for temp in [0.5, 0.8, 1.0, 1.3]:
        text = ai.generate("The future ", max_length=80, temperature=temp)
        print(f"\næ¸©åº¦ {temp}: {text}")
    
    print("\nâœ… å®Œäº†ï¼")
    return ai


if __name__ == "__main__":
    main()

