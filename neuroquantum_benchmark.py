#!/usr/bin/env python3
"""
NeuroQuantum ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
================================
QBNNï¼ˆQuantum-Bit Neural Networkï¼‰ã®æ€§èƒ½è©•ä¾¡

ãƒ†ã‚¹ãƒˆé …ç›®ï¼š
1. å­¦ç¿’é€Ÿåº¦
2. æ¨è«–é€Ÿåº¦
3. ç”Ÿæˆå“è³ª
4. QBNNã‚‚ã¤ã‚ŒåŠ¹æœ
5. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
"""

import torch
import torch.nn as nn
import time
import math
import psutil
import os
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt
import numpy as np

# NeuroQuantumã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from neuroquantum import (
    NeuroQuantumAI, QBNNLayer, QBNNAttention, 
    QBNNTransformerBlock, NeuroQuantum, NeuroQuantumTokenizer
)

plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']


def get_memory_usage():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024


def benchmark_qbnn_layer():
    """QBNNLayerå˜ä½“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("\n" + "=" * 60)
    print("ğŸ”¬ ãƒ†ã‚¹ãƒˆ1: QBNNLayer ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    results = []
    
    # ç•°ãªã‚‹ã‚µã‚¤ã‚ºã§ãƒ†ã‚¹ãƒˆ
    sizes = [(64, 64), (128, 128), (256, 256), (512, 512)]
    
    for in_dim, out_dim in sizes:
        layer = QBNNLayer(in_dim, out_dim, lambda_min=0.2, lambda_max=0.5).to(device)
        x = torch.randn(32, 64, in_dim).to(device)  # (batch, seq, dim)
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(10):
            _ = layer(x)
        
        # è¨ˆæ¸¬
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        start = time.perf_counter()
        iterations = 100
        for _ in range(iterations):
            _ = layer(x)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        elapsed = (time.perf_counter() - start) / iterations * 1000  # ms
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        params = sum(p.numel() for p in layer.parameters())
        
        results.append({
            'size': f'{in_dim}x{out_dim}',
            'time_ms': elapsed,
            'params': params,
            'throughput': 32 * 64 / elapsed * 1000  # tokens/sec
        })
        
        print(f"   ã‚µã‚¤ã‚º {in_dim}x{out_dim}: {elapsed:.3f}ms, ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params:,}, ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {results[-1]['throughput']:.0f} tokens/s")
    
    return results


def benchmark_attention_comparison():
    """QBNN Attention vs é€šå¸¸Attentionã®æ¯”è¼ƒ"""
    print("\n" + "=" * 60)
    print("ğŸ”¬ ãƒ†ã‚¹ãƒˆ2: QBNN Attention vs æ¨™æº– Attention")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    embed_dim = 128
    num_heads = 4
    batch_size = 16
    seq_len = 64
    
    # QBNN Attention
    qbnn_attn = QBNNAttention(embed_dim, num_heads, lambda_val=0.35).to(device)
    
    # æ¨™æº– Attention
    std_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True).to(device)
    
    x = torch.randn(batch_size, seq_len, embed_dim).to(device)
    
    results = {}
    
    # QBNN Attentionè¨ˆæ¸¬
    for _ in range(10):
        _ = qbnn_attn(x)
    
    start = time.perf_counter()
    for _ in range(50):
        _ = qbnn_attn(x)
    qbnn_time = (time.perf_counter() - start) / 50 * 1000
    
    # æ¨™æº–Attentionè¨ˆæ¸¬
    for _ in range(10):
        _, _ = std_attn(x, x, x)
    
    start = time.perf_counter()
    for _ in range(50):
        _, _ = std_attn(x, x, x)
    std_time = (time.perf_counter() - start) / 50 * 1000
    
    results['qbnn_attention'] = qbnn_time
    results['standard_attention'] = std_time
    results['overhead'] = (qbnn_time / std_time - 1) * 100
    
    print(f"   QBNN Attention:    {qbnn_time:.3f} ms")
    print(f"   æ¨™æº– Attention:    {std_time:.3f} ms")
    print(f"   ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰:    {results['overhead']:.1f}%")
    
    return results


def benchmark_training_speed():
    """å­¦ç¿’é€Ÿåº¦ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("\n" + "=" * 60)
    print("ğŸ”¬ ãƒ†ã‚¹ãƒˆ3: å­¦ç¿’é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 60)
    
    # å°ã•ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_data = [
        ("ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã¯ï¼ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ"),
        ("AIã¨ã¯", "äººå·¥çŸ¥èƒ½ã¯æ©Ÿæ¢°å­¦ç¿’ã‚’ä½¿ã£ãŸæŠ€è¡“ã§ã™ã€‚"),
        ("é‡å­ã¨ã¯", "é‡å­ã¯ç‰©è³ªã®æœ€å°å˜ä½ã§ã™ã€‚"),
        ("Hello", "Hello! How can I help you?"),
        ("What is AI", "AI is artificial intelligence."),
    ] * 10  # 50ãƒšã‚¢
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ç•°ãªã‚‹è¨­å®šã§ãƒ†ã‚¹ãƒˆ
    configs = [
        {'embed_dim': 64, 'hidden_dim': 128, 'num_layers': 2, 'name': 'Small'},
        {'embed_dim': 128, 'hidden_dim': 256, 'num_layers': 4, 'name': 'Medium'},
    ]
    
    results = []
    
    for config in configs:
        print(f"\n   è¨­å®š: {config['name']} (embed={config['embed_dim']}, layers={config['num_layers']})")
        
        ai = NeuroQuantumAI(
            embed_dim=config['embed_dim'],
            hidden_dim=config['hidden_dim'],
            num_heads=4,
            num_layers=config['num_layers'],
            max_seq_len=64,
            dropout=0.1,
            lambda_entangle=0.35,
        )
        
        mem_before = get_memory_usage()
        
        start = time.perf_counter()
        ai.train(test_data, epochs=5, batch_size=8, lr=0.001, seq_len=32)
        train_time = time.perf_counter() - start
        
        mem_after = get_memory_usage()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        total_params = ai.model.num_params
        
        results.append({
            'name': config['name'],
            'train_time': train_time,
            'params': total_params,
            'memory_mb': mem_after - mem_before,
            'time_per_epoch': train_time / 5,
            'tokens_per_sec': len(test_data) * 32 * 5 / train_time
        })
        
        print(f"      å­¦ç¿’æ™‚é–“: {train_time:.2f}ç§’ ({train_time/5:.3f}ç§’/ã‚¨ãƒãƒƒã‚¯)")
        print(f"      ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        print(f"      ãƒ¡ãƒ¢ãƒªå¢—åŠ : {mem_after - mem_before:.1f} MB")
        print(f"      ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {results[-1]['tokens_per_sec']:.0f} tokens/s")
    
    return results


def benchmark_generation_quality():
    """ç”Ÿæˆå“è³ªã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("\n" + "=" * 60)
    print("ğŸ”¬ ãƒ†ã‚¹ãƒˆ4: ç”Ÿæˆå“è³ªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 60)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
    test_data = [
        ("ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã¯ï¼å…ƒæ°—ã§ã™ã‹ï¼Ÿä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
        ("AIã¨ã¯", "AIã¯äººå·¥çŸ¥èƒ½ã®ç•¥ã§ã€æ©Ÿæ¢°å­¦ç¿’ãªã©ã®æŠ€è¡“ã‚’ä½¿ã£ã¦çŸ¥çš„ãªå‡¦ç†ã‚’è¡Œã†ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚"),
        ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é‡å­åŠ›å­¦ã®åŸç†ã‚’ä½¿ã£ãŸæ¬¡ä¸–ä»£ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚"),
        ("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨ã¯", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«æŒ‡ç¤ºã‚’ä¸ãˆã‚‹ãŸã‚ã®è¨€èªã‚’æ›¸ãä½œæ¥­ã§ã™ã€‚"),
        ("Hello", "Hello! How are you today? I'm here to help!"),
        ("What is machine learning", "Machine learning is a type of AI that enables computers to learn from data."),
        ("Explain quantum computing", "Quantum computing uses quantum bits to perform calculations in parallel."),
    ] * 20
    
    ai = NeuroQuantumAI(
        embed_dim=128,
        hidden_dim=256,
        num_heads=4,
        num_layers=4,
        max_seq_len=128,
        dropout=0.1,
        lambda_entangle=0.35,
    )
    
    print("\n   å­¦ç¿’ä¸­...")
    ai.train(test_data, epochs=30, batch_size=16, lr=0.001, seq_len=64)
    
    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    test_prompts = [
        "ã“ã‚“ã«ã¡ã¯",
        "AIã¨ã¯",
        "Hello",
        "What is",
    ]
    
    print("\n   ç”Ÿæˆãƒ†ã‚¹ãƒˆ:")
    results = []
    
    for prompt in test_prompts:
        # ç•°ãªã‚‹æ¸©åº¦ç¯„å›²ã§ãƒ†ã‚¹ãƒˆ
        temp_configs = [
            (0.3, 0.5, "ä½"),
            (0.4, 0.8, "ä¸­"),
            (0.6, 1.0, "é«˜"),
        ]
        
        print(f"\n   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'")
        
        for temp_min, temp_max, label in temp_configs:
            start = time.perf_counter()
            response = ai.generate(prompt, max_length=50, temp_min=temp_min, temp_max=temp_max)
            gen_time = time.perf_counter() - start
            
            # å“è³ªæŒ‡æ¨™
            unique_chars = len(set(response))
            repetition = 1 - (unique_chars / max(len(response), 1))
            
            results.append({
                'prompt': prompt,
                'temp_range': f'{temp_min}-{temp_max}',
                'response': response[:60],
                'gen_time': gen_time,
                'length': len(response),
                'unique_ratio': unique_chars / max(len(response), 1),
                'repetition': repetition
            })
            
            print(f"      æ¸©åº¦{label}({temp_min}-{temp_max}): {response[:40]}...")
    
    return results


def benchmark_quantum_entanglement():
    """é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("\n" + "=" * 60)
    print("ğŸ”¬ ãƒ†ã‚¹ãƒˆ5: é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœåˆ†æ")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ç•°ãªã‚‹Î»ç¯„å›²ã§ã®QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼
    lambda_configs = [
        (0.0, 0.1, "å¾®å¼±"),
        (0.2, 0.5, "ä¸­ç¨‹åº¦"),
        (0.5, 0.9, "å¼·åŠ›"),
    ]
    
    results = []
    
    for lambda_min, lambda_max, label in lambda_configs:
        layer = QBNNLayer(128, 128, lambda_min=lambda_min, lambda_max=lambda_max).to(device)
        x = torch.randn(16, 32, 128).to(device)
        
        # Forwardé€šé
        output = layer(x)
        
        # é‡å­æƒ…å ±å–å¾—
        q_info = layer.get_quantum_info()
        
        # å‡ºåŠ›ã®çµ±è¨ˆ
        output_mean = output.mean().item()
        output_std = output.std().item()
        
        results.append({
            'label': label,
            'lambda_range': f'{lambda_min}-{lambda_max}',
            'lambda_eff': q_info['lambda_eff'],
            'J_mean': q_info['J_mean'],
            'J_std': q_info['J_std'],
            'output_mean': output_mean,
            'output_std': output_std,
        })
        
        print(f"\n   {label}ã‚‚ã¤ã‚Œ (Î»={lambda_min}-{lambda_max}):")
        print(f"      æœ‰åŠ¹Î»: {q_info['lambda_eff']:.4f}")
        print(f"      Jå¹³å‡: {q_info['J_mean']:.6f}, Jæ¨™æº–åå·®: {q_info['J_std']:.4f}")
        print(f"      å‡ºåŠ›å¹³å‡: {output_mean:.4f}, å‡ºåŠ›æ¨™æº–åå·®: {output_std:.4f}")
    
    return results


def benchmark_memory_efficiency():
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("\n" + "=" * 60)
    print("ğŸ”¬ ãƒ†ã‚¹ãƒˆ6: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åˆ†æ")
    print("=" * 60)
    
    results = []
    
    configs = [
        {'embed_dim': 64, 'layers': 2, 'name': 'Tiny'},
        {'embed_dim': 128, 'layers': 4, 'name': 'Small'},
        {'embed_dim': 256, 'layers': 6, 'name': 'Medium'},
    ]
    
    base_memory = get_memory_usage()
    
    for config in configs:
        # GC
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        mem_before = get_memory_usage()
        
        ai = NeuroQuantumAI(
            embed_dim=config['embed_dim'],
            hidden_dim=config['embed_dim'] * 2,
            num_heads=4,
            num_layers=config['layers'],
            max_seq_len=128,
            dropout=0.1,
            lambda_entangle=0.35,
        )
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã®ãŸã‚ã«ãƒ€ãƒŸãƒ¼å­¦ç¿’
        dummy_data = [("test", "test response")] * 5
        ai.train(dummy_data, epochs=1, batch_size=4, lr=0.001, seq_len=32)
        
        mem_after = get_memory_usage()
        
        params = ai.model.num_params
        memory_used = mem_after - mem_before
        
        results.append({
            'name': config['name'],
            'embed_dim': config['embed_dim'],
            'layers': config['layers'],
            'params': params,
            'memory_mb': memory_used,
            'bytes_per_param': memory_used * 1024 * 1024 / params if params > 0 else 0
        })
        
        print(f"   {config['name']}: {params:,} params, {memory_used:.1f} MB ({results[-1]['bytes_per_param']:.1f} bytes/param)")
        
        del ai
    
    return results


def create_benchmark_report(all_results: Dict):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('NeuroQuantum (QBNN) ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ', fontsize=14, fontweight='bold')
    
    # 1. QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼é€Ÿåº¦
    ax1 = axes[0, 0]
    if 'qbnn_layer' in all_results:
        sizes = [r['size'] for r in all_results['qbnn_layer']]
        times = [r['time_ms'] for r in all_results['qbnn_layer']]
        ax1.bar(sizes, times, color='#4CAF50')
        ax1.set_xlabel('Layer Size')
        ax1.set_ylabel('Time (ms)')
        ax1.set_title('QBNNLayer å‡¦ç†æ™‚é–“')
    
    # 2. Attentionæ¯”è¼ƒ
    ax2 = axes[0, 1]
    if 'attention' in all_results:
        labels = ['QBNN\nAttention', 'Standard\nAttention']
        times = [all_results['attention']['qbnn_attention'], 
                 all_results['attention']['standard_attention']]
        colors = ['#2196F3', '#FF9800']
        ax2.bar(labels, times, color=colors)
        ax2.set_ylabel('Time (ms)')
        ax2.set_title('Attention æ¯”è¼ƒ')
        ax2.text(0.5, max(times) * 0.8, f"+{all_results['attention']['overhead']:.1f}%", 
                ha='center', fontsize=10, color='red')
    
    # 3. å­¦ç¿’é€Ÿåº¦
    ax3 = axes[0, 2]
    if 'training' in all_results:
        names = [r['name'] for r in all_results['training']]
        throughput = [r['tokens_per_sec'] for r in all_results['training']]
        ax3.bar(names, throughput, color='#9C27B0')
        ax3.set_ylabel('Tokens/sec')
        ax3.set_title('å­¦ç¿’ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ')
    
    # 4. é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœ
    ax4 = axes[1, 0]
    if 'entanglement' in all_results:
        labels = [r['label'] for r in all_results['entanglement']]
        lambda_eff = [r['lambda_eff'] for r in all_results['entanglement']]
        output_std = [r['output_std'] for r in all_results['entanglement']]
        
        x = np.arange(len(labels))
        width = 0.35
        ax4.bar(x - width/2, lambda_eff, width, label='Î»_eff', color='#00BCD4')
        ax4.bar(x + width/2, output_std, width, label='å‡ºåŠ›Ïƒ', color='#E91E63')
        ax4.set_xticks(x)
        ax4.set_xticklabels(labels)
        ax4.set_title('é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœ')
        ax4.legend()
    
    # 5. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
    ax5 = axes[1, 1]
    if 'memory' in all_results:
        names = [r['name'] for r in all_results['memory']]
        memory = [r['memory_mb'] for r in all_results['memory']]
        ax5.bar(names, memory, color='#FF5722')
        ax5.set_ylabel('Memory (MB)')
        ax5.set_title('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡')
    
    # 6. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° vs æ€§èƒ½
    ax6 = axes[1, 2]
    if 'memory' in all_results and 'training' in all_results:
        for m, t in zip(all_results['memory'], all_results['training']):
            ax6.scatter(m['params'] / 1000, t['tokens_per_sec'], s=100, alpha=0.7)
            ax6.annotate(m['name'], (m['params'] / 1000, t['tokens_per_sec']),
                        xytext=(5, 5), textcoords='offset points')
        ax6.set_xlabel('Parameters (K)')
        ax6.set_ylabel('Throughput (tokens/s)')
        ax6.set_title('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° vs å­¦ç¿’é€Ÿåº¦')
    
    plt.tight_layout()
    plt.savefig('/Users/yuyahiguchi/Program/Qubit/neuroquantum_benchmark.png', dpi=150, bbox_inches='tight')
    print("\n   ğŸ“ˆ ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: neuroquantum_benchmark.png")
    
    return fig


def main():
    print("=" * 60)
    print("ğŸš€ NeuroQuantum (QBNN) ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    print(f"   ãƒ‡ãƒã‚¤ã‚¹: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    print(f"   PyTorch: {torch.__version__}")
    
    all_results = {}
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    all_results['qbnn_layer'] = benchmark_qbnn_layer()
    all_results['attention'] = benchmark_attention_comparison()
    all_results['training'] = benchmark_training_speed()
    all_results['entanglement'] = benchmark_quantum_entanglement()
    all_results['memory'] = benchmark_memory_efficiency()
    
    # ç”Ÿæˆå“è³ªï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§æœ€å¾Œï¼‰
    all_results['generation'] = benchmark_generation_quality()
    
    # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    create_benchmark_report(all_results)
    
    print("\n" + "=" * 60)
    print("âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ï¼")
    print("=" * 60)
    
    return all_results


if __name__ == '__main__':
    main()

