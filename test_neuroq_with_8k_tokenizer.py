#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NeuroQ Brain ã‚’ vocab_size=8000 ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ†ã‚¹ãƒˆ

æ”¹å–„ã‚’ç¢ºèªã—ã¾ã™ï¼š
- å…ƒã® vocab ~300 â†’ æ–°ã—ã„ vocab 8000ï¼ˆ27å€æ”¹å–„ï¼‰
- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å“è³ªå‘ä¸Š
- ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®æ”¹å–„
"""

import torch
import sys
import os

# neuroquantum_brain.py ã‹ã‚‰å¿…è¦ãªã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from neuroquantum_brain import (
    NeuroQuantumBrainAI,
    BrainTokenizer,
)


def test_tokenizer():
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å˜ä½“ãƒ†ã‚¹ãƒˆ"""
    print("=" * 70)
    print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    # å­¦ç¿’æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
    tokenizer = BrainTokenizer(
        vocab_size=8000,
        model_file="neuroq_tokenizer_8k.model"
    )

    print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
    print(f"   èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}")
    print(f"   å®Ÿéš›ã®èªå½™ã‚µã‚¤ã‚º: {tokenizer.actual_vocab_size:,}")

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ:")
    print("-" * 70)

    test_texts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é©æ–°çš„ãªæŠ€è¡“ã§ã™ã€‚",
        "äººå·¥çŸ¥èƒ½ãŒæœªæ¥ã‚’å¤‰ãˆã¦ã„ãã¾ã™ã€‚",
        "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯å‰µé€ çš„ãªæ´»å‹•ã§ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯è¤‡é›‘ãªå•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚",
    ]

    for text in test_texts:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        tokens = tokenizer.encode(text)

        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        decoded = tokenizer.decode(tokens)

        print(f"\nåŸæ–‡: {text}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³ID: {tokens[:15]}{'...' if len(tokens) > 15 else ''}")
        print(f"ãƒ‡ã‚³ãƒ¼ãƒ‰: {decoded}")

    print("\n" + "=" * 70)


def test_neuroq_brain_small():
    """NeuroQ Brain ã§å°è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼ˆè»½é‡ç‰ˆï¼‰"""
    print("\n" + "=" * 70)
    print("ğŸ§  NeuroQ Brain å°è¦æ¨¡ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆå°‘é‡ã§ãƒ†ã‚¹ãƒˆï¼‰
    texts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é©æ–°çš„ãªæŠ€è¡“ã§ã™ã€‚",
        "äººå·¥çŸ¥èƒ½ãŒæœªæ¥ã‚’å¤‰ãˆã¦ã„ãã¾ã™ã€‚",
        "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯å‰µé€ çš„ãªæ´»å‹•ã§ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯è¤‡é›‘ãªå•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯è„³ã‚’æ¨¡å€£ã—ã¾ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã¾ã™ã€‚",
        "è‡ªç„¶è¨€èªå‡¦ç†ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ç†è§£ã—ã¾ã™ã€‚",
    ] * 50  # 400ã‚µãƒ³ãƒ—ãƒ«

    print(f"ğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«")

    # AIæ§‹ç¯‰ï¼ˆè»½é‡è¨­å®šï¼‰
    ai = NeuroQuantumBrainAI(
        embed_dim=64,      # å°ã•ãè¨­å®š
        num_heads=2,       # å°ã•ãè¨­å®š
        num_layers=2,      # å°ã•ãè¨­å®š
        num_neurons=30,    # å°ã•ãè¨­å®š
        max_vocab=8000,    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«åˆã‚ã›ã‚‹
    )

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ‰‹å‹•è¨­å®šï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
    ai.tokenizer = BrainTokenizer(
        vocab_size=8000,
        model_file="neuroq_tokenizer_8k.model"
    )

    print(f"âœ… AIæ§‹ç¯‰å®Œäº†")
    print(f"   èªå½™ã‚µã‚¤ã‚º: {ai.tokenizer.vocab_size:,}")

    # è»½é‡å­¦ç¿’ï¼ˆã‚¨ãƒãƒƒã‚¯æ•°ã‚’æ¸›ã‚‰ã™ï¼‰
    print("\nğŸ“ å­¦ç¿’é–‹å§‹...")
    ai.train(
        texts=texts,
        epochs=10,          # å°‘ãªãè¨­å®š
        batch_size=8,       # å°ã•ãè¨­å®š
        lr=0.001,
        seq_length=32,      # çŸ­ãè¨­å®š
    )

    print("\nâœ… å­¦ç¿’å®Œäº†ï¼")

    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    print("\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ:")
    print("-" * 70)

    prompts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿",
        "äººå·¥çŸ¥èƒ½",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°",
        "ã“ã‚“ã«ã¡ã¯",
    ]

    for prompt in prompts:
        generated = ai.generate(
            prompt,
            max_length=30,
            temperature_min=0.5,
            temperature_max=0.8
        )
        print(f"\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
        print(f"ç”Ÿæˆçµæœ: {generated}")

    print("\n" + "=" * 70)
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 70)
    print("ğŸš€ NeuroQ Brain with vocab_size=8000 ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists("neuroq_tokenizer_8k.model"):
        print("âŒ ã‚¨ãƒ©ãƒ¼: neuroq_tokenizer_8k.model ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   python train_tokenizer_32k.py --vocab-size 8000 --prefix neuroq_tokenizer_8k ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        sys.exit(1)

    # 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å˜ä½“ãƒ†ã‚¹ãƒˆ
    test_tokenizer()

    # 2. NeuroQ Brain å°è¦æ¨¡ãƒ†ã‚¹ãƒˆ
    response = input("\n\nNeuroQ Brain ã®å­¦ç¿’ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
    if response == 'y':
        test_neuroq_brain_small()
    else:
        print("\nğŸ‘‹ ãƒ†ã‚¹ãƒˆçµ‚äº†")


if __name__ == '__main__':
    main()
