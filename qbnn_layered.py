#!/usr/bin/env python3
"""
Entangled Quantum Bit Neural Network (E-QBNN)
å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã‚’æŒã¤é‡å­ãƒ“ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

è«–æ–‡ + ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãå®Ÿè£…:
1. å„å±¤ã«é‡å­çŠ¶æ…‹ |Ïˆ^(l)âŸ© ã‚’æŒãŸã›ã‚‹
2. å±¤é–“ã®ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«æ¼”ç®— U ã‚’å®šç¾©
3. ã‚‚ã¤ã‚Œé …: e^(l) = f_entangle(q^(l), q^(l-1))
4. æ¬¡å±¤å…¥åŠ›: h^(l+1) = Ïƒ(W^(l) h^(l) + B^(l) + G(e^(l)))
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import requests
import gzip
import io
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("ğŸ§ âš›ï¸ Entangled Quantum Bit Neural Network (E-QBNN)")
print("   å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã‚’æŒã¤é‡å­ç”ŸæˆAI")
print("=" * 70)

# ========================================================================
# 1. APQB ã‚³ã‚¢ï¼ˆè«–æ–‡ã®æ•°å­¦çš„å®šç¾©ï¼‰
# ========================================================================

class APQB:
    """Adjustable Pseudo Quantum Bit - è«–æ–‡ã®æ ¸å¿ƒ"""
    
    @staticmethod
    def theta_to_state(theta):
        """Î¸ â†’ é‡å­çŠ¶æ…‹ [cos(Î¸), sin(Î¸)]"""
        return torch.stack([torch.cos(theta), torch.sin(theta)], dim=-1)
    
    @staticmethod
    def theta_to_r(theta):
        """Î¸ â†’ ç›¸é–¢ä¿‚æ•° r = cos(2Î¸)"""
        return torch.cos(2 * theta)
    
    @staticmethod
    def theta_to_T(theta):
        """Î¸ â†’ æ¸©åº¦ T = |sin(2Î¸)|"""
        return torch.abs(torch.sin(2 * theta))
    
    @staticmethod
    def theta_to_z(theta):
        """Î¸ â†’ è¤‡ç´ æ•° z = e^{i2Î¸} (å®Ÿéƒ¨, è™šéƒ¨)"""
        return torch.stack([torch.cos(2 * theta), torch.sin(2 * theta)], dim=-1)
    
    @staticmethod
    def constraint(theta):
        """rÂ² + TÂ² = 1 ã®æ¤œè¨¼"""
        r = APQB.theta_to_r(theta)
        T = APQB.theta_to_T(theta)
        return r**2 + T**2
    
    @staticmethod
    def Q_k(theta, k):
        """kä½“ç›¸é–¢ Q_k(Î¸)"""
        if k % 2 == 0:
            return torch.cos(2 * k * theta)
        else:
            return torch.sin(2 * k * theta)


# ========================================================================
# 2. å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
# ========================================================================

class EntanglementOperator(nn.Module):
    """
    å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«æ¼”ç®— U
    
    e^(l) = f_entangle(q^(l), q^(l-1))
    
    å®Ÿè£…:
    - ç›¸é–¢è¡Œåˆ—ãƒ™ãƒ¼ã‚¹ã®ã‚‚ã¤ã‚Œ
    - CNOTãƒ©ã‚¤ã‚¯ãªç›¸äº’ä½œç”¨
    - ä½ç›¸ã‚­ãƒƒã‚¯ãƒãƒƒã‚¯
    """
    
    def __init__(self, current_dim, prev_dim=None, entangle_strength=0.5):
        super().__init__()
        self.current_dim = current_dim
        self.prev_dim = prev_dim if prev_dim else current_dim
        self.entangle_strength = nn.Parameter(torch.tensor(entangle_strength))
        
        # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«é‡ã¿ï¼ˆç•°ãªã‚‹æ¬¡å…ƒé–“ã‚’æ¥ç¶šï¼‰
        self.W_entangle = nn.Linear(self.prev_dim, current_dim)
        
        # ä½ç›¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.phase = nn.Parameter(torch.rand(current_dim) * np.pi / 2)
    
    def forward(self, q_current, q_prev):
        """
        å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«è¨ˆç®—
        
        Args:
            q_current: ç¾åœ¨ã®å±¤ã®é‡å­çŠ¶æ…‹ [batch, current_dim]
            q_prev: å‰ã®å±¤ã®é‡å­çŠ¶æ…‹ [batch, prev_dim]
        
        Returns:
            ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆé … e [batch, current_dim]
        """
        # 1. å‰ã®å±¤ã®çŠ¶æ…‹ã‚’ç¾åœ¨ã®æ¬¡å…ƒã«å¤‰æ›
        q_prev_transformed = self.W_entangle(q_prev)
        
        # 2. è¦ç´ ã”ã¨ã®ç›¸é–¢ï¼ˆCNOTãƒ©ã‚¤ã‚¯ï¼‰
        # åˆ¶å¾¡ãƒ“ãƒƒãƒˆï¼ˆå‰ã®å±¤ï¼‰ã¨ç¾åœ¨ã®å±¤ã®ç›¸äº’ä½œç”¨
        correlation = q_current * torch.tanh(q_prev_transformed)
        
        # 3. ä½ç›¸ã‚­ãƒƒã‚¯ãƒãƒƒã‚¯
        phase_factor = torch.cos(self.phase.unsqueeze(0))
        
        # 4. æœ€çµ‚ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«é …
        e = self.entangle_strength * correlation * phase_factor
        
        return e


class QuantumCorrelationMatrix(nn.Module):
    """
    é‡å­ç›¸é–¢è¡Œåˆ—ã®è¨ˆç®—
    
    h^(l) ã‹ã‚‰ç›¸é–¢è¡Œåˆ— R^(l) ã‚’ä½œæˆã—ã€APQBçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« q^(l) ã‚’ç”Ÿæˆ
    """
    
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.theta_proj = nn.Linear(dim, dim)
    
    def forward(self, h):
        """
        éš ã‚ŒçŠ¶æ…‹ â†’ é‡å­çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«
        
        Args:
            h: éš ã‚ŒçŠ¶æ…‹ [batch, dim]
        
        Returns:
            q: é‡å­çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« [batch, dim]
            theta: å†…éƒ¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ [batch, dim]
        """
        # 1. Î¸ ã®è¨ˆç®—ï¼ˆ0ã€œÏ€/2 ã«åˆ¶é™ï¼‰
        theta = torch.sigmoid(self.theta_proj(h)) * np.pi / 2
        
        # 2. é‡å­çŠ¶æ…‹ï¼ˆr æˆåˆ†ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ä½¿ç”¨ï¼‰
        r = APQB.theta_to_r(theta)
        
        return r, theta


# ========================================================================
# 3. E-QBNN ãƒ¬ã‚¤ãƒ¤ãƒ¼
# ========================================================================

class EQBNNLayer(nn.Module):
    """
    Entangled QBNN Layer
    
    h^(l+1) = Ïƒ(W^(l) h^(l) + B^(l) + G(e^(l)))
    
    - é€šå¸¸ã®ç·šå½¢å¤‰æ›
    - é‡å­ã‚‚ã¤ã‚Œã‹ã‚‰ã®è£œæ­£
    - å¹¾ä½•å­¦çš„åˆ¶ç´„ã®æ­£å‰‡åŒ–
    """
    
    def __init__(self, input_dim, output_dim, prev_output_dim=None, entangle_strength=0.5):
        super().__init__()
        
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.prev_output_dim = prev_output_dim if prev_output_dim else input_dim
        
        # ç·šå½¢å¤‰æ›
        self.linear = nn.Linear(input_dim, output_dim)
        
        # é‡å­ç›¸é–¢
        self.quantum_corr = QuantumCorrelationMatrix(output_dim)
        
        # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆæ¼”ç®—å­ï¼ˆç•°ãªã‚‹æ¬¡å…ƒé–“ï¼‰
        self.entangle_op = EntanglementOperator(output_dim, self.prev_output_dim, entangle_strength)
        
        # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«è£œæ­£ã®å¤‰æ› G
        self.G = nn.Linear(output_dim, output_dim)
        
        # é‡å­çŠ¶æ…‹ä¿æŒ
        self.q = None
        self.theta = None
    
    def forward(self, h, q_prev=None):
        """
        é †ä¼æ’­
        
        Args:
            h: å…¥åŠ› [batch, input_dim]
            q_prev: å‰ã®å±¤ã®é‡å­çŠ¶æ…‹ [batch, output_dim] or None
        
        Returns:
            h_out: å‡ºåŠ› [batch, output_dim]
            q: ã“ã®å±¤ã®é‡å­çŠ¶æ…‹ [batch, output_dim]
        """
        # 1. é€šå¸¸ã®ç·šå½¢å¤‰æ›
        h_linear = self.linear(h)
        
        # 2. é‡å­çŠ¶æ…‹ã®è¨ˆç®—
        self.q, self.theta = self.quantum_corr(h_linear)
        
        # 3. ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆè£œæ­£
        if q_prev is not None:
            e = self.entangle_op(self.q, q_prev)
            entangle_correction = self.G(e)
        else:
            entangle_correction = 0
        
        # 4. å‡ºåŠ› = ç·šå½¢å¤‰æ› + ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«è£œæ­£
        h_out = torch.tanh(h_linear + entangle_correction)
        
        return h_out, self.q
    
    def get_constraint_loss(self):
        """å¹¾ä½•å­¦çš„åˆ¶ç´„ rÂ² + TÂ² = 1 ã®æå¤±"""
        if self.theta is None:
            return 0
        constraint = APQB.constraint(self.theta)
        return ((constraint - 1) ** 2).mean()


# ========================================================================
# 4. E-QBNN ç”Ÿæˆãƒ¢ãƒ‡ãƒ«
# ========================================================================

class EQBNNGenerativeModel(nn.Module):
    """
    Entangled QBNN ç”Ÿæˆãƒ¢ãƒ‡ãƒ«
    
    ç‰¹å¾´:
    - å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
    - é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    - å¹¾ä½•å­¦çš„åˆ¶ç´„ã«ã‚ˆã‚‹æ­£å‰‡åŒ–
    """
    
    def __init__(self, vocab_size, embed_dim=128, hidden_dims=[256, 256, 256], 
                 entangle_strength=0.5, dropout=0.1):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.entangle_strength = entangle_strength
        
        # åŸ‹ã‚è¾¼ã¿
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoding = nn.Parameter(torch.randn(512, embed_dim) * 0.02)
        
        # E-QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.layers = nn.ModuleList()
        dims = [embed_dim] + hidden_dims
        
        # å‰ã®å±¤ã®Qæ¬¡å…ƒã‚’è¿½è·¡ï¼ˆQã¯å„å±¤ã®output_dimã¨åŒã˜ï¼‰
        prev_q_dim = embed_dim  # æœ€åˆã®å±¤ã¯å…¥åŠ›æ¬¡å…ƒ
        
        for i in range(len(dims) - 1):
            current_output_dim = dims[i+1]
            # 2å±¤ç›®ä»¥é™ã¯å‰ã®å±¤ã®å‡ºåŠ›æ¬¡å…ƒã‚’prev_q_dimã¨ã—ã¦ä½¿ç”¨
            layer = EQBNNLayer(dims[i], current_output_dim, prev_q_dim, entangle_strength)
            self.layers.append(layer)
            prev_q_dim = current_output_dim  # æ¬¡ã®å±¤ã®ãŸã‚ã«æ›´æ–°
        
        # å‡ºåŠ›å±¤
        self.output_proj = nn.Linear(hidden_dims[-1], vocab_size)
        
        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        """
        é †ä¼æ’­
        
        Args:
            x: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ [batch, seq_len]
        
        Returns:
            logits: å‡ºåŠ›ãƒ­ã‚¸ãƒƒãƒˆ [batch, seq_len, vocab_size]
        """
        batch_size, seq_len = x.shape
        
        # åŸ‹ã‚è¾¼ã¿ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        h = self.embedding(x) + self.pos_encoding[:seq_len].unsqueeze(0)
        h = self.dropout(h)
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å¹³å‡åŒ–ï¼ˆç°¡ç•¥åŒ–ï¼‰
        h = h.mean(dim=1)  # [batch, embed_dim]
        
        # E-QBNNãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é€šé
        q_prev = None
        for layer in self.layers:
            h, q = layer(h, q_prev)
            q_prev = q
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ¬¡å…ƒã‚’å¾©å…ƒ
        h = h.unsqueeze(1).expand(-1, seq_len, -1)
        
        # å‡ºåŠ›
        logits = self.output_proj(h)
        
        return logits
    
    def get_total_constraint_loss(self):
        """å…¨å±¤ã®å¹¾ä½•å­¦çš„åˆ¶ç´„æå¤±"""
        loss = 0
        for layer in self.layers:
            loss += layer.get_constraint_loss()
        return loss / len(self.layers)
    
    def get_entanglement_stats(self):
        """ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆçµ±è¨ˆ"""
        stats = []
        for i, layer in enumerate(self.layers):
            if layer.q is not None:
                r_mean = APQB.theta_to_r(layer.theta).mean().item()
                T_mean = APQB.theta_to_T(layer.theta).mean().item()
                constraint = APQB.constraint(layer.theta).mean().item()
                stats.append({
                    'layer': i,
                    'r_mean': r_mean,
                    'T_mean': T_mean,
                    'constraint': constraint,
                    'entangle_strength': layer.entangle_op.entangle_strength.item()
                })
        return stats
    
    @torch.no_grad()
    def generate(self, start_tokens, max_length=50, temperature=1.0, 
                 use_quantum_sampling=True, top_k=40, top_p=0.9, 
                 repetition_penalty=1.2):
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        
        Args:
            start_tokens: é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ [seq_len]
            max_length: æœ€å¤§ç”Ÿæˆé•·
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            use_quantum_sampling: é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            top_k: ãƒˆãƒƒãƒ—Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            top_p: ãƒ‹ãƒ¥ãƒ¼ã‚¯ãƒ¬ã‚ªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆtop-pï¼‰
            repetition_penalty: ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
        """
        self.eval()
        
        tokens = start_tokens.clone()
        generated_tokens = []
        
        for _ in range(max_length):
            # å…¥åŠ›æº–å‚™
            x = tokens.unsqueeze(0)
            if x.size(1) > 512:
                x = x[:, -512:]
            
            # é †ä¼æ’­
            logits = self(x)
            next_logits = logits[0, -1] / temperature
            
            # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
            if len(generated_tokens) > 0:
                for prev_token in set(generated_tokens[-20:]):  # ç›´è¿‘20ãƒˆãƒ¼ã‚¯ãƒ³
                    next_logits[prev_token] /= repetition_penalty
            
            # é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if use_quantum_sampling and len(self.layers) > 0:
                last_layer = self.layers[-1]
                if last_layer.theta is not None:
                    T = APQB.theta_to_T(last_layer.theta).mean()
                    quantum_noise = torch.randn_like(next_logits) * T * 0.3
                    next_logits = next_logits + quantum_noise
            
            # Top-K ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_k > 0:
                indices_to_remove = next_logits < torch.topk(next_logits, top_k)[0][..., -1, None]
                next_logits[indices_to_remove] = float('-inf')
            
            # Top-P (Nucleus) ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                
                # top_p ã‚’è¶…ãˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_logits[indices_to_remove] = float('-inf')
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, 1)
            
            tokens = torch.cat([tokens, next_token], dim=0)
            generated_tokens.append(next_token.item())
        
        return tokens


# ========================================================================
# 5. ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆCommon Crawl ã‚µãƒ³ãƒ—ãƒ«ï¼‰
# ========================================================================

def fetch_common_crawl_sample(max_samples=1000, min_length=50, lang='en'):
    """
    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Args:
        max_samples: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        min_length: æœ€å°ãƒ†ã‚­ã‚¹ãƒˆé•·
        lang: 'en' (è‹±èª) or 'ja' (æ—¥æœ¬èª)
    """
    print(f"\nğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­... (è¨€èª: {lang})")
    
    texts = []
    
    if lang == 'ja':
        # æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿
        texts = fetch_japanese_data(max_samples, min_length)
    else:
        # è‹±èªãƒ‡ãƒ¼ã‚¿
        texts = fetch_english_data(max_samples, min_length)
    
    print(f"   å–å¾—ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(texts)}")
    
    return texts[:max_samples]


def fetch_japanese_data(max_samples=1000, min_length=30):
    """æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    texts = []
    
    # 1. Wikipediaæ—¥æœ¬èªç‰ˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«å–å¾—
    try:
        wiki_titles = [
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "æ©Ÿæ¢°å­¦ç¿’",
            "äººå·¥çŸ¥èƒ½", "æ·±å±¤å­¦ç¿’", "è‡ªç„¶è¨€èªå‡¦ç†",
            "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿", "ç‰©ç†å­¦", "æ•°å­¦", "æŠ€è¡“"
        ]
        
        for title in wiki_titles:
            url = f"https://ja.wikipedia.org/w/api.php?action=query&titles={title}&prop=extracts&explaintext&format=json"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                pages = data.get('query', {}).get('pages', {})
                for page in pages.values():
                    extract = page.get('extract', '')
                    if len(extract) > min_length:
                        paragraphs = extract.split('\n\n')
                        for p in paragraphs:
                            if len(p) > min_length:
                                texts.append(p.strip())
            
            if len(texts) >= max_samples:
                break
    except Exception as e:
        print(f"   Wikipediaæ—¥æœ¬èªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 2. è¿½åŠ ã®æ—¥æœ¬èªã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
    japanese_texts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ã¦æƒ…å ±ã‚’å‡¦ç†ã™ã‚‹é©æ–°çš„ãªè¨ˆç®—æ©Ÿã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒ0ã¨1ã®ãƒ“ãƒƒãƒˆã‚’ä½¿ã†ã®ã«å¯¾ã—ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æŒã¤é‡å­ãƒ“ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€äººé–“ã®è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚å…¥åŠ›å±¤ã€éš ã‚Œå±¤ã€å‡ºåŠ›å±¤ã‹ã‚‰æ§‹æˆã•ã‚Œã€å­¦ç¿’ã«ã‚ˆã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¾ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã¯ã€æ˜ç¤ºçš„ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã•ã‚Œã‚‹ã“ã¨ãªãã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã¦æ”¹å–„ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã™ã‚‹äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã™ã€‚",
        "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é«˜åº¦ãªç‰¹å¾´ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ç”»åƒèªè­˜ã‚„éŸ³å£°èªè­˜ã§å¤§ããªæˆåŠŸã‚’åã‚ã¦ã„ã¾ã™ã€‚",
        "è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®è¨€èªã‚’ç†è§£ã—ã€ç”Ÿæˆã™ã‚‹ãŸã‚ã®æŠ€è¡“ã§ã™ã€‚æ©Ÿæ¢°ç¿»è¨³ã€æ„Ÿæƒ…åˆ†æã€è³ªå•å¿œç­”ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "äººå·¥çŸ¥èƒ½ã¯ã€äººé–“ã®çŸ¥çš„æ´»å‹•ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å®Ÿç¾ã—ã‚ˆã†ã¨ã™ã‚‹æŠ€è¡“ã®ç·ç§°ã§ã™ã€‚ç¾åœ¨ã¯æ©Ÿæ¢°å­¦ç¿’ãŒä¸»æµã¨ãªã£ã¦ã„ã¾ã™ã€‚",
        "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã‚’ç”¨ã„ãŸé©æ–°çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚GPTã‚„BERTãªã©ã€å¤šãã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚",
        "å¼·åŒ–å­¦ç¿’ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦ã€å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹è¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ã‚²ãƒ¼ãƒ ã‚„ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "è»¢ç§»å­¦ç¿’ã¯ã€ã‚ã‚‹èª²é¡Œã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã€åˆ¥ã®èª²é¡Œã«é©ç”¨ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ã‚‚åŠ¹æœçš„ã«å­¦ç¿’ã§ãã¾ã™ã€‚",
        "ç”ŸæˆAIã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãªã©ã‚’ç”Ÿæˆã§ãã‚‹äººå·¥çŸ¥èƒ½ã§ã™ã€‚å‰µé€ çš„ãªã‚¿ã‚¹ã‚¯ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚",
        "é‡å­ãƒ“ãƒƒãƒˆã¯ã€0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®æ€§è³ªã«ã‚ˆã‚Šã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ä¸¦åˆ—è¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚",
        "ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã¯ã€è¤‡æ•°ã®é‡å­ãƒ“ãƒƒãƒˆãŒå¼·ãç›¸é–¢ã—ãŸçŠ¶æ…‹ã§ã™ã€‚é‡å­é€šä¿¡ã‚„é‡å­æš—å·ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "æ³¨æ„æ©Ÿæ§‹ã¯ã€å…¥åŠ›ã®é‡è¦ãªéƒ¨åˆ†ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹æŠ€è¡“ã§ã™ã€‚æ©Ÿæ¢°ç¿»è¨³ã®å“è³ªã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã—ãŸã€‚",
        "ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ç”»åƒèªè­˜ã«ç‰¹åŒ–ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ã¦ç‰¹å¾´ã‚’æŠ½å‡ºã—ã¾ã™ã€‚",
        "å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ã®ã«é©ã—ã¦ã„ã¾ã™ã€‚éå»ã®æƒ…å ±ã‚’è¨˜æ†¶ã—ã¦åˆ©ç”¨ã§ãã¾ã™ã€‚",
    ] * 70
    
    texts.extend(japanese_texts)
    
    return texts


def fetch_english_data(max_samples=1000, min_length=50):
    """è‹±èªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    texts = []
    
    # 1. Wikipediaã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«å–å¾—
    try:
        wiki_titles = [
            "Quantum_computing", "Neural_network", "Machine_learning",
            "Artificial_intelligence", "Deep_learning", "Natural_language_processing",
            "Computer_science", "Physics", "Mathematics", "Technology"
        ]
        
        for title in wiki_titles:
            url = f"https://en.wikipedia.org/w/api.php?action=query&titles={title}&prop=extracts&explaintext&format=json"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                pages = data.get('query', {}).get('pages', {})
                for page in pages.values():
                    extract = page.get('extract', '')
                    if len(extract) > min_length:
                        paragraphs = extract.split('\n\n')
                        for p in paragraphs:
                            if len(p) > min_length:
                                texts.append(p.strip())
            
            if len(texts) >= max_samples:
                break
    except Exception as e:
        print(f"   Wikipediaå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 2. è¿½åŠ ã®è‹±èªã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
    english_texts = [
        "Quantum computing harnesses the principles of quantum mechanics to process information in fundamentally new ways. Unlike classical computers that use bits representing 0 or 1, quantum computers use qubits that can exist in superposition of both states simultaneously.",
        "Neural networks are computing systems inspired by biological neural networks in the brain. They consist of interconnected nodes or neurons that process information using connectionist approaches to computation.",
        "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.",
        "Deep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher-level features from raw input. In image recognition, lower layers may identify edges, while higher layers may identify human-relevant concepts.",
        "Natural language processing combines computational linguistics with statistical, machine learning, and deep learning models. It enables computers to process and analyze large amounts of natural language data.",
        "The transformer architecture has revolutionized natural language processing since its introduction. It relies entirely on self-attention mechanisms to compute representations of its input and output without using sequence-aligned RNNs or convolution.",
        "Attention mechanisms allow neural networks to focus on specific parts of the input when producing output. This is particularly useful for tasks like machine translation where the relationship between input and output elements is not strictly sequential.",
        "Generative models learn the underlying distribution of the training data and can generate new samples from that distribution. Examples include variational autoencoders and generative adversarial networks.",
        "Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward. It differs from supervised learning in not requiring labeled input-output pairs.",
        "Transfer learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task. This is popular in deep learning because it can train models with comparatively little data.",
    ] * 100
    
    texts.extend(english_texts)
    
    return texts


# ========================================================================
# 6. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
# ========================================================================

class SimpleTokenizer:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆæ—¥æœ¬èª/è‹±èªå¯¾å¿œï¼‰"""
    
    def __init__(self, max_vocab_size=5000, use_char=False):
        self.max_vocab_size = max_vocab_size
        self.use_char = use_char  # æ–‡å­—å˜ä½ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<BOS>': 2, '<EOS>': 3}
        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<BOS>', 3: '<EOS>'}
        self.vocab_size = 4
        self.is_japanese = False
    
    def fit(self, texts):
        """èªå½™ã‚’æ§‹ç¯‰"""
        word_counts = Counter()
        
        # æ—¥æœ¬èªæ¤œå‡º
        sample_text = ' '.join(texts[:10])
        self.is_japanese = any('\u3040' <= c <= '\u309F' or '\u30A0' <= c <= '\u30FF' or '\u4E00' <= c <= '\u9FFF' for c in sample_text)
        
        if self.is_japanese:
            print("   æ—¥æœ¬èªãƒ¢ãƒ¼ãƒ‰ã‚’æ¤œå‡º")
            self.use_char = True  # æ—¥æœ¬èªã¯æ–‡å­—å˜ä½
        
        for text in texts:
            words = self._tokenize(text)
            word_counts.update(words)
        
        # é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        most_common = word_counts.most_common(self.max_vocab_size - self.vocab_size)
        
        for word, _ in most_common:
            if word not in self.word2idx:
                idx = len(self.word2idx)
                self.word2idx[word] = idx
                self.idx2word[idx] = word
        
        self.vocab_size = len(self.word2idx)
        print(f"   èªå½™ã‚µã‚¤ã‚º: {self.vocab_size}")
    
    def _tokenize(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²"""
        if self.use_char or self.is_japanese:
            # æ–‡å­—å˜ä½ï¼ˆæ—¥æœ¬èªå‘ã‘ï¼‰
            return list(text)
        else:
            # å˜èªå˜ä½ï¼ˆè‹±èªå‘ã‘ï¼‰
            text = text.lower()
            text = re.sub(r'[^\w\s]', ' ', text)
            words = text.split()
            return words
    
    def encode(self, text, max_length=None):
        """ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒˆãƒ¼ã‚¯ãƒ³ID"""
        words = self._tokenize(text)
        tokens = [self.word2idx.get(w, 1) for w in words]  # 1 = <UNK>
        
        if max_length:
            tokens = tokens[:max_length]
        
        return tokens
    
    def decode(self, tokens):
        """ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆ"""
        words = [self.idx2word.get(t, '') for t in tokens if t not in [0, 1, 2, 3]]
        if self.is_japanese or self.use_char:
            return ''.join(words)  # æ—¥æœ¬èªã¯çµåˆ
        return ' '.join(words)


# ========================================================================
# 7. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
# ========================================================================

class TextDataset(Dataset):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, texts, tokenizer, seq_length=64):
        self.tokenizer = tokenizer
        self.seq_length = seq_length
        
        # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        self.all_tokens = []
        for text in texts:
            tokens = tokenizer.encode(text)
            if len(tokens) > 2:
                self.all_tokens.extend(tokens)
        
        print(f"   ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(self.all_tokens)}")
    
    def __len__(self):
        return max(0, len(self.all_tokens) - self.seq_length - 1)
    
    def __getitem__(self, idx):
        tokens = self.all_tokens[idx:idx + self.seq_length + 1]
        x = torch.tensor(tokens[:-1], dtype=torch.long)
        y = torch.tensor(tokens[1:], dtype=torch.long)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if len(x) < self.seq_length:
            pad_len = self.seq_length - len(x)
            x = F.pad(x, (0, pad_len), value=0)
            y = F.pad(y, (0, pad_len), value=0)
        
        return x, y


# ========================================================================
# 8. ç”ŸæˆAI ã‚¯ãƒ©ã‚¹
# ========================================================================

class EQBNNGenerativeAI:
    """Entangled QBNN ç”ŸæˆAI"""
    
    def __init__(self, embed_dim=128, hidden_dims=[256, 256], 
                 entangle_strength=0.5, max_vocab_size=3000,
                 num_neurons: int = None):  # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°æŒ‡å®šå¯èƒ½
        self.embed_dim = embed_dim
        # num_neuronsãŒæŒ‡å®šã•ã‚ŒãŸã‚‰hidden_dimsã‚’ä¸Šæ›¸ã
        if num_neurons is not None:
            self.hidden_dims = [num_neurons, num_neurons]
        else:
            self.hidden_dims = hidden_dims
        self.entangle_strength = entangle_strength
        self.max_vocab_size = max_vocab_size
        
        self.tokenizer = SimpleTokenizer(max_vocab_size)
        self.model = None
        
        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ: MPS (Apple Silicon) > CUDA > CPU
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print("ğŸ® NVIDIA GPU (CUDA) ã‚’ä½¿ç”¨")
        else:
            self.device = torch.device("cpu")
            print("ğŸ’» CPU ã‚’ä½¿ç”¨")
    
    def train(self, texts, epochs=10, batch_size=32, lr=0.001, seq_length=64):
        """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        print("\nğŸ“ å­¦ç¿’é–‹å§‹...")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ§‹ç¯‰
        self.tokenizer.fit(texts)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        dataset = TextDataset(texts, self.tokenizer, seq_length)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        self.model = EQBNNGenerativeModel(
            vocab_size=self.tokenizer.vocab_size,
            embed_dim=self.embed_dim,
            hidden_dims=self.hidden_dims,
            entangle_strength=self.entangle_strength
        ).to(self.device)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        
        # æå¤±é–¢æ•°
        criterion = nn.CrossEntropyLoss(ignore_index=0)
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            total_constraint = 0
            num_batches = 0
            
            for batch_x, batch_y in dataloader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                
                # é †ä¼æ’­
                logits = self.model(batch_x)
                
                # æå¤±è¨ˆç®—
                ce_loss = criterion(logits.view(-1, self.tokenizer.vocab_size), batch_y.view(-1))
                constraint_loss = self.model.get_total_constraint_loss()
                
                # ç·æå¤± = CEæå¤± + å¹¾ä½•å­¦çš„åˆ¶ç´„
                loss = ce_loss + 0.01 * constraint_loss
                
                # é€†ä¼æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += ce_loss.item()
                total_constraint += constraint_loss.item() if isinstance(constraint_loss, torch.Tensor) else constraint_loss
                num_batches += 1
            
            avg_loss = total_loss / max(num_batches, 1)
            avg_constraint = total_constraint / max(num_batches, 1)
            
            if (epoch + 1) % 2 == 0 or epoch == 0:
                print(f"   Epoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, rÂ²+TÂ²={1+avg_constraint:.4f}")
        
        print("   å­¦ç¿’å®Œäº†ï¼")
    
    def generate(self, prompt="The quantum", max_length=50, temperature=1.0, 
                 use_quantum=True, top_k=40, top_p=0.9, repetition_penalty=1.2):
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if self.model is None:
            return "ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        self.model.eval()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokens = self.tokenizer.encode(prompt)
        if len(tokens) == 0:
            tokens = [2]  # <BOS>
        
        tokens = torch.tensor(tokens, dtype=torch.long).to(self.device)
        
        # ç”Ÿæˆ
        generated = self.model.generate(
            tokens, 
            max_length=max_length, 
            temperature=temperature,
            use_quantum_sampling=use_quantum,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty
        )
        
        return self.tokenizer.decode(generated.cpu().tolist())
    
    def get_entanglement_report(self):
        """ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆ"""
        if self.model is None:
            return "ãƒ¢ãƒ‡ãƒ«ãªã—"
        
        # ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§çµ±è¨ˆã‚’å–å¾—
        dummy_input = torch.randint(0, self.tokenizer.vocab_size, (1, 10)).to(self.device)
        with torch.no_grad():
            _ = self.model(dummy_input)
        
        stats = self.model.get_entanglement_stats()
        
        report = "\nğŸ“Š ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆ\n" + "-" * 50 + "\n"
        for s in stats:
            report += f"   Layer {s['layer']}: r={s['r_mean']:.3f}, T={s['T_mean']:.3f}, "
            report += f"rÂ²+TÂ²={s['constraint']:.4f}, Î»={s['entangle_strength']:.3f}\n"
        
        return report


# ========================================================================
# 9. å¯è¦–åŒ–
# ========================================================================

def visualize_entanglement(ai, save_path=None):
    """ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã®å¯è¦–åŒ–"""
    import matplotlib.pyplot as plt
    
    if ai.model is None:
        print("ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§çµ±è¨ˆã‚’å–å¾—
    dummy_input = torch.randint(0, ai.tokenizer.vocab_size, (1, 10)).to(ai.device)
    with torch.no_grad():
        _ = ai.model(dummy_input)
    
    stats = ai.model.get_entanglement_stats()
    
    if not stats:
        print("çµ±è¨ˆãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. å„å±¤ã® r ã¨ T
    ax = axes[0, 0]
    layers = [s['layer'] for s in stats]
    r_values = [s['r_mean'] for s in stats]
    T_values = [s['T_mean'] for s in stats]
    
    x = np.arange(len(layers))
    width = 0.35
    ax.bar(x - width/2, r_values, width, label='r (correlation)', color='blue', alpha=0.7)
    ax.bar(x + width/2, T_values, width, label='T (temperature)', color='red', alpha=0.7)
    ax.set_xlabel('Layer')
    ax.set_ylabel('Value')
    ax.set_title('APQB Parameters per Layer')
    ax.set_xticks(x)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. å¹¾ä½•å­¦çš„åˆ¶ç´„
    ax = axes[0, 1]
    constraints = [s['constraint'] for s in stats]
    ax.bar(layers, constraints, color='green', alpha=0.7)
    ax.axhline(1.0, color='red', linestyle='--', label='Target: 1.0')
    ax.set_xlabel('Layer')
    ax.set_ylabel('rÂ² + TÂ²')
    ax.set_title('Geometric Constraint')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆå¼·åº¦
    ax = axes[1, 0]
    entangle_strengths = [s['entangle_strength'] for s in stats]
    ax.plot(layers, entangle_strengths, 'o-', color='purple', linewidth=2, markersize=10)
    ax.set_xlabel('Layer')
    ax.set_ylabel('Entanglement Strength (Î»)')
    ax.set_title('Inter-layer Entanglement')
    ax.grid(True, alpha=0.3)
    
    # 4. r-T å¹³é¢ä¸Šã®ãƒ—ãƒ­ãƒƒãƒˆ
    ax = axes[1, 1]
    
    # åˆ¶ç´„æ›²ç·š
    theta_range = np.linspace(0, np.pi/2, 100)
    r_curve = np.cos(2 * theta_range)
    T_curve = np.abs(np.sin(2 * theta_range))
    ax.plot(r_curve, T_curve, 'b-', linewidth=2, label='rÂ² + TÂ² = 1')
    
    # å„å±¤ã®ãƒ—ãƒ­ãƒƒãƒˆ
    colors = plt.cm.viridis(np.linspace(0, 1, len(stats)))
    for i, s in enumerate(stats):
        ax.scatter([s['r_mean']], [s['T_mean']], s=200, c=[colors[i]], 
                   label=f'Layer {s["layer"]}', zorder=5, edgecolors='black')
    
    ax.set_xlabel('r (Correlation)')
    ax.set_ylabel('T (Temperature)')
    ax.set_title('Layer States on r-T Plane')
    ax.legend()
    ax.set_xlim(-1.2, 1.2)
    ax.set_ylim(-0.1, 1.2)
    ax.grid(True, alpha=0.3)
    
    plt.suptitle('Entangled Quantum Bit Neural Network (E-QBNN)\nInter-layer Entanglement Visualization', fontsize=14)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"   ä¿å­˜: {save_path}")
    
    plt.close()


# ========================================================================
# 10. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================================================

def main(lang='en', num_neurons: int = 128):
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    
    Args:
        lang: è¨€èª ('en' or 'ja')
        num_neurons: ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128)
    """
    print("\nğŸ”§ E-QBNN ç”ŸæˆAI ã‚’æ§‹ç¯‰ä¸­...")
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {num_neurons}")
    
    # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—
    texts = fetch_common_crawl_sample(max_samples=500, min_length=30, lang=lang)
    
    # 2. AIæ§‹ç¯‰ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã‚’æŒ‡å®šï¼‰
    ai = EQBNNGenerativeAI(
        embed_dim=64,
        num_neurons=num_neurons,  # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã‚’æŒ‡å®š
        entangle_strength=0.5,
        max_vocab_size=2000
    )
    
    # 3. å­¦ç¿’
    ai.train(texts, epochs=10, batch_size=16, lr=0.002, seq_length=32)
    
    # 4. ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
    print(ai.get_entanglement_report())
    
    # 5. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    print("\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ:")
    print("-" * 50)
    
    if lang == 'ja':
        prompts = [
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯",
            "äººå·¥çŸ¥èƒ½ã¨ã¯",
            "æ©Ÿæ¢°å­¦ç¿’ã¯",
            "æœªæ¥ã®æŠ€è¡“"
        ]
    else:
        prompts = [
            "Quantum computing",
            "Neural networks",
            "Machine learning",
            "The future of"
        ]
    
    for prompt in prompts:
        generated = ai.generate(
            prompt, 
            max_length=40, 
            temperature=1.0, 
            use_quantum=True,
            top_k=50,
            top_p=0.9,
            repetition_penalty=1.5
        )
        print(f"   Prompt: '{prompt}'")
        print(f"   â†’ {generated}\n")
    
    # 6. å¯è¦–åŒ–
    print("\nğŸ“Š å¯è¦–åŒ–ã‚’ç”Ÿæˆä¸­...")
    visualize_entanglement(ai, '/Users/yuyahiguchi/Program/Qubit/eqbnn_entanglement.png')
    
    # 7. è«–æ–‡ã¨ã®å¯¾å¿œ
    print("\n" + "=" * 70)
    print("ğŸ“š è«–æ–‡ã¨ã®å¯¾å¿œï¼ˆå±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆç‰ˆï¼‰")
    print("=" * 70)
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Entangled Quantum Bit Neural Network (E-QBNN)                  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                 â”‚
    â”‚  1. å„å±¤ã«é‡å­çŠ¶æ…‹ã‚’æŒãŸã›ã‚‹                                   â”‚
    â”‚     h^(l) â†’ R^(l) â†’ |Ïˆ^(l)âŸ© (APQB)                            â”‚
    â”‚                                                                 â”‚
    â”‚  2. å±¤é–“ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ                                     â”‚
    â”‚     e^(l) = f_entangle(q^(l), q^(l-1))                         â”‚
    â”‚     - ç›¸é–¢è¡Œåˆ—ãƒ™ãƒ¼ã‚¹ã®ã‚‚ã¤ã‚Œ                                   â”‚
    â”‚     - CNOTãƒ©ã‚¤ã‚¯ãªç›¸äº’ä½œç”¨                                     â”‚
    â”‚     - ä½ç›¸ã‚­ãƒƒã‚¯ãƒãƒƒã‚¯                                         â”‚
    â”‚                                                                 â”‚
    â”‚  3. æ¬¡å±¤ã¸ã®ä¼æ’­                                               â”‚
    â”‚     h^(l+1) = Ïƒ(W^(l) h^(l) + B^(l) + G(e^(l)))               â”‚
    â”‚     - é€šå¸¸ã®ç·šå½¢å¤‰æ› + ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«è£œæ­£                        â”‚
    â”‚                                                                 â”‚
    â”‚  4. å¹¾ä½•å­¦çš„åˆ¶ç´„                                               â”‚
    â”‚     rÂ² + TÂ² = 1 ã‚’æå¤±é–¢æ•°ã«è¿½åŠ                                â”‚
    â”‚                                                                 â”‚
    â”‚  5. é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°                                           â”‚
    â”‚     ç”Ÿæˆæ™‚ã« Tï¼ˆæ¸©åº¦ï¼‰ã§é‡å­ãƒã‚¤ã‚ºã‚’è¿½åŠ                        â”‚
    â”‚                                                                 â”‚
    â”‚  åˆ©ç‚¹:                                                          â”‚
    â”‚     - æ·±ã„ä¾å­˜é–¢ä¿‚ã‚’è¡¨ç¾                                       â”‚
    â”‚     - è‡ªç„¶ãªæ­£å‰‡åŒ–ï¼ˆå¹¾ä½•å­¦çš„åˆ¶ç´„ï¼‰                             â”‚
    â”‚     - ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆå¼·åº¦ Î» ã§åˆ¶å¾¡å¯èƒ½                      â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    print("\nâœ… E-QBNN ç”ŸæˆAI å®Œæˆï¼")
    print("   ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:")
    print("   - eqbnn_entanglement.png (ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆå¯è¦–åŒ–)")


def chat_mode(lang='en'):
    """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
    lang_name = "æ—¥æœ¬èª" if lang == 'ja' else "English"
    print(f"\nğŸ”§ E-QBNN ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’èµ·å‹•ä¸­... ({lang_name})")
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    texts = fetch_common_crawl_sample(max_samples=500, min_length=30, lang=lang)
    
    # AIæ§‹ç¯‰
    ai = EQBNNGenerativeAI(
        embed_dim=64,
        hidden_dims=[128, 128, 64],
        entangle_strength=0.5,
        max_vocab_size=2000
    )
    
    # å­¦ç¿’
    ai.train(texts, epochs=15, batch_size=16, lr=0.002, seq_length=32)
    
    print("\n" + "=" * 60)
    print("ğŸ’¬ E-QBNN ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰")
    print("=" * 60)
    print("ã‚³ãƒãƒ³ãƒ‰:")
    print("  /quit, /exit  - çµ‚äº†")
    print("  /temp <å€¤>    - æ¸©åº¦è¨­å®š (0.1-2.0)")
    print("  /len <å€¤>     - ç”Ÿæˆé•·ã• (10-100)")
    print("  /topk <å€¤>    - Top-K (1-100)")
    print("  /topp <å€¤>    - Top-P (0.1-1.0)")
    print("  /rep <å€¤>     - ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£ (1.0-2.0)")
    print("  /quantum      - é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ON/OFF")
    print("  /stats        - ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆçµ±è¨ˆ")
    print("  /help         - ãƒ˜ãƒ«ãƒ—è¡¨ç¤º")
    print("-" * 60)
    
    temperature = 1.0
    max_length = 30
    use_quantum = True
    top_k = 40
    top_p = 0.9
    repetition_penalty = 1.5
    
    while True:
        try:
            user_input = input("\nğŸ§‘ You: ").strip()
            
            if not user_input:
                continue
            
            # ã‚³ãƒãƒ³ãƒ‰å‡¦ç†
            if user_input.lower() in ['/quit', '/exit', '/q']:
                print("\nğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼")
                break
            
            elif user_input.lower() == '/help':
                print("ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§:")
                print("  /quit, /exit  - çµ‚äº†")
                print("  /temp <å€¤>    - æ¸©åº¦è¨­å®š (0.1-2.0)")
                print("  /len <å€¤>     - ç”Ÿæˆé•·ã• (10-100)")
                print("  /quantum      - é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ON/OFF")
                print("  /stats        - ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆçµ±è¨ˆ")
                continue
            
            elif user_input.lower().startswith('/temp'):
                try:
                    val = float(user_input.split()[1])
                    temperature = max(0.1, min(2.0, val))
                    print(f"   æ¸©åº¦ã‚’ {temperature} ã«è¨­å®šã—ã¾ã—ãŸ")
                except:
                    print("   ä½¿ç”¨æ³•: /temp <0.1-2.0>")
                continue
            
            elif user_input.lower().startswith('/len'):
                try:
                    val = int(user_input.split()[1])
                    max_length = max(10, min(100, val))
                    print(f"   ç”Ÿæˆé•·ã•ã‚’ {max_length} ã«è¨­å®šã—ã¾ã—ãŸ")
                except:
                    print("   ä½¿ç”¨æ³•: /len <10-100>")
                continue
            
            elif user_input.lower() == '/quantum':
                use_quantum = not use_quantum
                status = "ON" if use_quantum else "OFF"
                print(f"   é‡å­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {status}")
                continue
            
            elif user_input.lower().startswith('/topk'):
                try:
                    val = int(user_input.split()[1])
                    top_k = max(1, min(100, val))
                    print(f"   Top-K ã‚’ {top_k} ã«è¨­å®šã—ã¾ã—ãŸ")
                except:
                    print("   ä½¿ç”¨æ³•: /topk <1-100>")
                continue
            
            elif user_input.lower().startswith('/topp'):
                try:
                    val = float(user_input.split()[1])
                    top_p = max(0.1, min(1.0, val))
                    print(f"   Top-P ã‚’ {top_p} ã«è¨­å®šã—ã¾ã—ãŸ")
                except:
                    print("   ä½¿ç”¨æ³•: /topp <0.1-1.0>")
                continue
            
            elif user_input.lower().startswith('/rep'):
                try:
                    val = float(user_input.split()[1])
                    repetition_penalty = max(1.0, min(2.0, val))
                    print(f"   ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ {repetition_penalty} ã«è¨­å®šã—ã¾ã—ãŸ")
                except:
                    print("   ä½¿ç”¨æ³•: /rep <1.0-2.0>")
                continue
            
            elif user_input.lower() == '/stats':
                print(ai.get_entanglement_report())
                continue
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            response = ai.generate(
                user_input, 
                max_length=max_length, 
                temperature=temperature,
                use_quantum=use_quantum,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty
            )
            
            print(f"\nğŸ¤– E-QBNN: {response}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼")
            break
        except Exception as e:
            print(f"   ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description='E-QBNN ç”ŸæˆAI')
    parser.add_argument('--neurons', type=int, default=128, help='ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128)')
    parser.add_argument('--ja', action='store_true', help='æ—¥æœ¬èªãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--chat', action='store_true', help='ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰')
    args = parser.parse_args()
    
    lang = 'ja' if args.ja else 'en'
    
    if args.chat:
        chat_mode(lang=lang)
    else:
        main(lang=lang, num_neurons=args.neurons)

