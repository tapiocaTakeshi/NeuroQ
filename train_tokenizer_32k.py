#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NeuroQ ç”¨ SentencePiece ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

vocab_size = 32000 ã®é«˜å“è³ªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    python train_tokenizer_32k.py
"""

import os
import sys

try:
    import sentencepiece as spm
    SENTENCEPIECE_AVAILABLE = True
except ImportError:
    SENTENCEPIECE_AVAILABLE = False
    print("âŒ sentencepiece ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼")
    print("   pip install sentencepiece ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)


def train_sentencepiece_tokenizer(
    input_file: str = "data/training_data.txt",
    model_prefix: str = "neuroq_tokenizer_32k",
    vocab_size: int = 32000,
    character_coverage: float = 0.9995,
    model_type: str = "bpe",
):
    """
    SentencePiece ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å­¦ç¿’

    Args:
        input_file: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        model_prefix: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        vocab_size: èªå½™ã‚µã‚¤ã‚ºï¼ˆ32000æ¨å¥¨ï¼‰
        character_coverage: æ–‡å­—ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆæ—¥æœ¬èªã§ã¯0.9995æ¨å¥¨ï¼‰
        model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆbpe or unigramï¼‰
    """

    print("=" * 70)
    print("ğŸ”¤ NeuroQ SentencePiece ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å­¦ç¿’")
    print("=" * 70)
    print(f"ğŸ“‚ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {input_file}")
    print(f"ğŸ“Š èªå½™ã‚µã‚¤ã‚º: {vocab_size:,}")
    print(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
    print(f"ğŸ“ˆ æ–‡å­—ã‚«ãƒãƒ¬ãƒƒã‚¸: {character_coverage}")
    print("-" * 70)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(input_file):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {input_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª
    file_size = os.path.getsize(input_file)
    print(f"ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {file_size:,} ãƒã‚¤ãƒˆ ({file_size / 1024 / 1024:.2f} MB)")

    # SentencePiece å­¦ç¿’
    print("\nğŸš€ å­¦ç¿’é–‹å§‹...")

    try:
        spm.SentencePieceTrainer.train(
            input=input_file,
            model_prefix=model_prefix,
            vocab_size=vocab_size,
            character_coverage=character_coverage,
            model_type=model_type,
            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            pad_id=0,
            unk_id=1,
            bos_id=2,
            eos_id=3,
            pad_piece='<pad>',
            unk_piece='<unk>',
            bos_piece='<s>',
            eos_piece='</s>',
            # è¿½åŠ è¨­å®š
            user_defined_symbols=[],
            max_sentence_length=4192,  # é•·ã„æ–‡ã«å¯¾å¿œ
            num_threads=os.cpu_count(),  # ä¸¦åˆ—åŒ–
            train_extremely_large_corpus=False,
            # ãƒ­ã‚°è¨­å®š
            minloglevel=1,  # INFO ãƒ¬ãƒ™ãƒ«
        )
        print("âœ… å­¦ç¿’å®Œäº†ï¼")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
    model_file = f"{model_prefix}.model"
    vocab_file = f"{model_prefix}.vocab"

    if os.path.exists(model_file):
        model_size = os.path.getsize(model_file)
        print(f"\nğŸ“¦ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {model_file} ({model_size:,} ãƒã‚¤ãƒˆ)")
    else:
        print(f"\nâŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« {model_file} ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        return

    if os.path.exists(vocab_file):
        vocab_size_actual = sum(1 for _ in open(vocab_file, 'r', encoding='utf-8'))
        print(f"ğŸ“– èªå½™ãƒ•ã‚¡ã‚¤ãƒ«: {vocab_file} ({vocab_size_actual:,} ã‚¨ãƒ³ãƒˆãƒª)")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
    print("\nğŸ§ª ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆä¸­...")
    sp = spm.SentencePieceProcessor()
    sp.load(model_file)

    actual_vocab_size = sp.get_piece_size()
    print(f"   èªå½™ã‚µã‚¤ã‚º: {actual_vocab_size:,}")
    print(f"   PAD ID: {sp.pad_id()}")
    print(f"   UNK ID: {sp.unk_id()}")
    print(f"   BOS ID: {sp.bos_id()}")
    print(f"   EOS ID: {sp.eos_id()}")

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ:")
    print("-" * 70)

    test_texts = [
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é©æ–°çš„ãªæŠ€è¡“ã§ã™ã€‚",
        "äººå·¥çŸ¥èƒ½ãŒæœªæ¥ã‚’å¤‰ãˆã¦ã„ãã¾ã™ã€‚",
        "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯å‰µé€ çš„ãªæ´»å‹•ã§ã™ã€‚",
    ]

    for text in test_texts:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        encoded = sp.encode(text, out_type=int)
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        decoded = sp.decode(encoded)

        print(f"\nåŸæ–‡: {text}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(encoded)}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³ID: {encoded[:20]}{'...' if len(encoded) > 20 else ''}")
        print(f"ãƒ‡ã‚³ãƒ¼ãƒ‰: {decoded}")

        # ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã‚’è¡¨ç¤º
        pieces = sp.encode(text, out_type=str)
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³: {pieces}")

    print("\n" + "=" * 70)
    print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å­¦ç¿’å®Œäº†ï¼")
    print(f"   ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {model_file}")
    print(f"   èªå½™ã‚µã‚¤ã‚º: {actual_vocab_size:,}")
    print("=" * 70)

    return model_file


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='NeuroQ SentencePiece ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å­¦ç¿’')
    parser.add_argument(
        '--input',
        type=str,
        default='data/training_data.txt',
        help='å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/training_data.txt)'
    )
    parser.add_argument(
        '--prefix',
        type=str,
        default='neuroq_tokenizer_32k',
        help='ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: neuroq_tokenizer_32k)'
    )
    parser.add_argument(
        '--vocab-size',
        type=int,
        default=32000,
        help='èªå½™ã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 32000)'
    )
    parser.add_argument(
        '--coverage',
        type=float,
        default=0.9995,
        help='æ–‡å­—ã‚«ãƒãƒ¬ãƒƒã‚¸ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.9995)'
    )
    parser.add_argument(
        '--model-type',
        type=str,
        default='bpe',
        choices=['bpe', 'unigram'],
        help='ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: bpe)'
    )

    args = parser.parse_args()

    train_sentencepiece_tokenizer(
        input_file=args.input,
        model_prefix=args.prefix,
        vocab_size=args.vocab_size,
        character_coverage=args.coverage,
        model_type=args.model_type,
    )
