#!/usr/bin/env python3
"""
QBNN Brain å‹•çš„å…¥å‡ºåŠ›ãƒ†ã‚¹ãƒˆ
============================
æ•°å­¦å•é¡Œãƒ»å›å¸°åˆ†æãƒ»ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ€ãƒ¼ãªã©
å‹•çš„å…¥å‡ºåŠ›ãŒæœ‰åŠ¹ã«æ©Ÿèƒ½ã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Tuple
import time

from qbnn_brain import QBNNBrain, QBNNBrainTorch


# ========================================
# 1. æ•°å­¦å•é¡Œãƒ†ã‚¹ãƒˆ
# ========================================

def test_math_problems():
    """æ•°å­¦å•é¡Œãƒ†ã‚¹ãƒˆï¼ˆå‹•çš„å…¥å‡ºåŠ›ï¼‰"""
    print("=" * 60)
    print("ğŸ“ æ•°å­¦å•é¡Œãƒ†ã‚¹ãƒˆï¼ˆå‹•çš„å…¥å‡ºåŠ› QBNN Brainï¼‰")
    print("=" * 60)
    
    results = {}
    
    # --- 1.1 XORå•é¡Œ ---
    print("\nğŸ”¹ 1.1 XORå•é¡Œ")
    print("-" * 40)
    
    model = QBNNBrainTorch(
        num_neurons=40,
        max_input_size=10,
        max_output_size=5,
        connection_density=0.25
    )
    
    # XORãƒ‡ãƒ¼ã‚¿
    X_xor = torch.tensor([
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
    ], dtype=torch.float32)
    
    y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)
    criterion = nn.MSELoss()
    
    print("   å­¦ç¿’ä¸­...")
    for epoch in range(200):
        optimizer.zero_grad()
        # dynamic_selection=False ã§å­¦ç¿’ï¼ˆå®‰å®šæ€§ã®ãŸã‚ï¼‰
        pred, in_idx, out_idx = model(X_xor, input_size=2, output_size=1, 
                                       time_steps=3, dynamic_selection=False)
        loss = criterion(pred[:, :1], y_xor)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 50 == 0:
            print(f"   Epoch {epoch+1}: Loss = {loss.item():.4f}")
    
    # ãƒ†ã‚¹ãƒˆï¼ˆå‹•çš„å…¥å‡ºåŠ›ã§ï¼‰
    model.eval()
    correct = 0
    print("\n   å‹•çš„å…¥å‡ºåŠ›ã§ãƒ†ã‚¹ãƒˆ:")
    with torch.no_grad():
        for i in range(4):
            # å‹•çš„é¸æŠã§æ¨è«–
            pred, in_idx, out_idx = model(X_xor[i:i+1], input_size=2, output_size=1, 
                                          time_steps=3, dynamic_selection=True)
            pred_val = pred[0, 0].item()
            expected = y_xor[i, 0].item()
            correct += 1 if abs(pred_val - expected) < 0.5 else 0
            status = "âœ…" if abs(pred_val - expected) < 0.5 else "âŒ"
            print(f"   {status} ({int(X_xor[i,0])},{int(X_xor[i,1])}) â†’ äºˆæ¸¬:{pred_val:.3f}, æ­£è§£:{expected:.0f}")
            print(f"      å…¥åŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: {in_idx.tolist()}, å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: {out_idx.tolist()}")
    
    results['XOR'] = correct / 4.0
    print(f"\n   XORæ­£ç­”ç‡: {correct}/4 = {results['XOR']*100:.1f}%")
    
    # --- 1.2 åŠ ç®—å•é¡Œ ---
    print("\nğŸ”¹ 1.2 åŠ ç®—å•é¡Œ (a + b)")
    print("-" * 40)
    
    model_add = QBNNBrainTorch(
        num_neurons=50,
        max_input_size=10,
        max_output_size=5,
        connection_density=0.2
    )
    
    # åŠ ç®—ãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£è¦åŒ–ï¼‰
    np.random.seed(42)
    X_add = []
    y_add = []
    for _ in range(100):
        a = np.random.uniform(0, 1)
        b = np.random.uniform(0, 1)
        X_add.append([a, b, 0, 0, 0, 0, 0, 0, 0, 0])
        y_add.append([(a + b) / 2])  # 0-1ã«æ­£è¦åŒ–
    
    X_add = torch.tensor(X_add, dtype=torch.float32)
    y_add = torch.tensor(y_add, dtype=torch.float32)
    
    optimizer = torch.optim.Adam(model_add.parameters(), lr=0.01)
    
    print("   å­¦ç¿’ä¸­...")
    for epoch in range(200):
        optimizer.zero_grad()
        pred, _, _ = model_add(X_add, input_size=2, output_size=1, 
                               time_steps=3, dynamic_selection=False)
        loss = criterion(pred[:, :1], y_add)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 50 == 0:
            print(f"   Epoch {epoch+1}: Loss = {loss.item():.4f}")
    
    # ãƒ†ã‚¹ãƒˆ
    model_add.eval()
    errors = []
    print("\n   å‹•çš„å…¥å‡ºåŠ›ã§ãƒ†ã‚¹ãƒˆï¼ˆ5ã‚µãƒ³ãƒ—ãƒ«ï¼‰:")
    with torch.no_grad():
        for i in range(5):
            pred, in_idx, out_idx = model_add(X_add[i:i+1], input_size=2, output_size=1,
                                               time_steps=3, dynamic_selection=True)
            pred_val = pred[0, 0].item() * 2  # é€†æ­£è¦åŒ–
            expected = y_add[i, 0].item() * 2
            error = abs(pred_val - expected)
            errors.append(error)
            print(f"   {X_add[i,0]:.2f} + {X_add[i,1]:.2f} = äºˆæ¸¬:{pred_val:.3f}, æ­£è§£:{expected:.3f}, èª¤å·®:{error:.3f}")
            print(f"      å…¥åŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: {in_idx.tolist()[:3]}..., å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: {out_idx.tolist()}")
    
    results['åŠ ç®—_MAE'] = np.mean(errors)
    print(f"\n   åŠ ç®—MAE: {results['åŠ ç®—_MAE']:.4f}")
    
    # --- 1.3 ä¹—ç®—å•é¡Œ ---
    print("\nğŸ”¹ 1.3 ä¹—ç®—å•é¡Œ (a Ã— b)")
    print("-" * 40)
    
    model_mul = QBNNBrainTorch(
        num_neurons=60,
        max_input_size=10,
        max_output_size=5,
        connection_density=0.25
    )
    
    # ä¹—ç®—ãƒ‡ãƒ¼ã‚¿
    X_mul = []
    y_mul = []
    for _ in range(100):
        a = np.random.uniform(0, 1)
        b = np.random.uniform(0, 1)
        X_mul.append([a, b, 0, 0, 0, 0, 0, 0, 0, 0])
        y_mul.append([a * b])
    
    X_mul = torch.tensor(X_mul, dtype=torch.float32)
    y_mul = torch.tensor(y_mul, dtype=torch.float32)
    
    optimizer = torch.optim.Adam(model_mul.parameters(), lr=0.01)
    
    print("   å­¦ç¿’ä¸­...")
    for epoch in range(200):
        optimizer.zero_grad()
        pred, _, _ = model_mul(X_mul, input_size=2, output_size=1, 
                               time_steps=4, dynamic_selection=False)
        loss = criterion(pred[:, :1], y_mul)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 50 == 0:
            print(f"   Epoch {epoch+1}: Loss = {loss.item():.4f}")
    
    model_mul.eval()
    errors = []
    print("\n   å‹•çš„å…¥å‡ºåŠ›ã§ãƒ†ã‚¹ãƒˆï¼ˆ5ã‚µãƒ³ãƒ—ãƒ«ï¼‰:")
    with torch.no_grad():
        for i in range(5):
            pred, in_idx, out_idx = model_mul(X_mul[i:i+1], input_size=2, output_size=1,
                                               time_steps=4, dynamic_selection=True)
            pred_val = pred[0, 0].item()
            expected = y_mul[i, 0].item()
            error = abs(pred_val - expected)
            errors.append(error)
            print(f"   {X_mul[i,0]:.2f} Ã— {X_mul[i,1]:.2f} = äºˆæ¸¬:{pred_val:.3f}, æ­£è§£:{expected:.3f}")
    
    results['ä¹—ç®—_MAE'] = np.mean(errors)
    print(f"\n   ä¹—ç®—MAE: {results['ä¹—ç®—_MAE']:.4f}")
    
    return results


# ========================================
# 2. å›å¸°åˆ†æãƒ†ã‚¹ãƒˆ
# ========================================

def test_regression():
    """å›å¸°åˆ†æãƒ†ã‚¹ãƒˆï¼ˆå‹•çš„å…¥å‡ºåŠ›ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ å›å¸°åˆ†æãƒ†ã‚¹ãƒˆï¼ˆå‹•çš„å…¥å‡ºåŠ› QBNN Brainï¼‰")
    print("=" * 60)
    
    results = {}
    
    # --- 2.1 ç·šå½¢å›å¸° ---
    print("\nğŸ”¹ 2.1 ç·šå½¢å›å¸° (y = 2x + 1)")
    print("-" * 40)
    
    model_lin = QBNNBrainTorch(
        num_neurons=30,
        max_input_size=10,
        max_output_size=5,
        connection_density=0.2
    )
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    X_lin = np.random.uniform(-1, 1, (100, 1))
    y_lin = 2 * X_lin + 1 + np.random.normal(0, 0.1, (100, 1))
    
    # æ­£è¦åŒ–
    y_lin_norm = (y_lin - y_lin.min()) / (y_lin.max() - y_lin.min())
    
    X_lin_pad = np.pad(X_lin, ((0, 0), (0, 9)), 'constant')
    X_lin_t = torch.tensor(X_lin_pad, dtype=torch.float32)
    y_lin_t = torch.tensor(y_lin_norm, dtype=torch.float32)
    
    optimizer = torch.optim.Adam(model_lin.parameters(), lr=0.01)
    criterion = nn.MSELoss()
    
    print("   å­¦ç¿’ä¸­...")
    losses = []
    for epoch in range(300):
        optimizer.zero_grad()
        pred, _, _ = model_lin(X_lin_t, input_size=1, output_size=1, 
                               time_steps=3, dynamic_selection=False)
        loss = criterion(pred[:, :1], y_lin_t)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        
        if (epoch + 1) % 100 == 0:
            print(f"   Epoch {epoch+1}: Loss = {loss.item():.6f}")
    
    # ãƒ†ã‚¹ãƒˆï¼ˆå‹•çš„å…¥å‡ºåŠ›ï¼‰
    model_lin.eval()
    with torch.no_grad():
        # è¤‡æ•°å›ãƒ†ã‚¹ãƒˆï¼ˆå‹•çš„é¸æŠã®å½±éŸ¿ã‚’è¦‹ã‚‹ï¼‰
        preds_all = []
        for trial in range(5):
            pred, in_idx, out_idx = model_lin(X_lin_t[:20], input_size=1, output_size=1,
                                              time_steps=3, dynamic_selection=True)
            preds_all.append(pred[:, 0].numpy())
        
        preds_all = np.array(preds_all)
        pred_mean = preds_all.mean(axis=0)
        pred_std = preds_all.std(axis=0)
    
    # RÂ²ã‚¹ã‚³ã‚¢
    ss_res = np.sum((y_lin_t[:20, 0].numpy() - pred_mean) ** 2)
    ss_tot = np.sum((y_lin_t[:20, 0].numpy() - y_lin_t[:20, 0].numpy().mean()) ** 2)
    r2 = 1 - ss_res / ss_tot
    
    results['ç·šå½¢å›å¸°_R2'] = r2
    results['ç·šå½¢å›å¸°_äºˆæ¸¬std'] = pred_std.mean()
    print(f"\n   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    print(f"   å‹•çš„é¸æŠã«ã‚ˆã‚‹äºˆæ¸¬ã®æ¨™æº–åå·®: {pred_std.mean():.4f}")
    
    # --- 2.2 äºŒæ¬¡é–¢æ•°å›å¸° ---
    print("\nğŸ”¹ 2.2 äºŒæ¬¡é–¢æ•°å›å¸° (y = xÂ² + 0.5)")
    print("-" * 40)
    
    model_quad = QBNNBrainTorch(
        num_neurons=50,
        max_input_size=10,
        max_output_size=5,
        connection_density=0.25
    )
    
    X_quad = np.random.uniform(-1, 1, (100, 1))
    y_quad = X_quad ** 2 + 0.5 + np.random.normal(0, 0.05, (100, 1))
    
    # æ­£è¦åŒ–
    y_quad_norm = (y_quad - y_quad.min()) / (y_quad.max() - y_quad.min())
    
    X_quad_pad = np.pad(X_quad, ((0, 0), (0, 9)), 'constant')
    X_quad_t = torch.tensor(X_quad_pad, dtype=torch.float32)
    y_quad_t = torch.tensor(y_quad_norm, dtype=torch.float32)
    
    optimizer = torch.optim.Adam(model_quad.parameters(), lr=0.01)
    
    print("   å­¦ç¿’ä¸­...")
    for epoch in range(300):
        optimizer.zero_grad()
        pred, _, _ = model_quad(X_quad_t, input_size=1, output_size=1, 
                                time_steps=4, dynamic_selection=False)
        loss = criterion(pred[:, :1], y_quad_t)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            print(f"   Epoch {epoch+1}: Loss = {loss.item():.6f}")
    
    model_quad.eval()
    with torch.no_grad():
        pred, _, _ = model_quad(X_quad_t[:20], input_size=1, output_size=1,
                                time_steps=4, dynamic_selection=True)
    
    ss_res = np.sum((y_quad_t[:20, 0].numpy() - pred[:, 0].numpy()) ** 2)
    ss_tot = np.sum((y_quad_t[:20, 0].numpy() - y_quad_t[:20, 0].numpy().mean()) ** 2)
    r2 = 1 - ss_res / ss_tot
    
    results['äºŒæ¬¡é–¢æ•°_R2'] = r2
    print(f"\n   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    
    # --- 2.3 siné–¢æ•°å›å¸° ---
    print("\nğŸ”¹ 2.3 siné–¢æ•°å›å¸° (y = sin(Ï€x))")
    print("-" * 40)
    
    model_sin = QBNNBrainTorch(
        num_neurons=60,
        max_input_size=10,
        max_output_size=5,
        connection_density=0.3
    )
    
    X_sin = np.random.uniform(-1, 1, (100, 1))
    y_sin = np.sin(np.pi * X_sin) + np.random.normal(0, 0.05, (100, 1))
    
    # æ­£è¦åŒ–
    y_sin_norm = (y_sin - y_sin.min()) / (y_sin.max() - y_sin.min())
    
    X_sin_pad = np.pad(X_sin, ((0, 0), (0, 9)), 'constant')
    X_sin_t = torch.tensor(X_sin_pad, dtype=torch.float32)
    y_sin_t = torch.tensor(y_sin_norm, dtype=torch.float32)
    
    optimizer = torch.optim.Adam(model_sin.parameters(), lr=0.01)
    
    print("   å­¦ç¿’ä¸­...")
    for epoch in range(400):
        optimizer.zero_grad()
        pred, _, _ = model_sin(X_sin_t, input_size=1, output_size=1, 
                               time_steps=5, dynamic_selection=False)
        loss = criterion(pred[:, :1], y_sin_t)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            print(f"   Epoch {epoch+1}: Loss = {loss.item():.6f}")
    
    model_sin.eval()
    with torch.no_grad():
        pred, _, _ = model_sin(X_sin_t[:20], input_size=1, output_size=1,
                               time_steps=5, dynamic_selection=True)
    
    ss_res = np.sum((y_sin_t[:20, 0].numpy() - pred[:, 0].numpy()) ** 2)
    ss_tot = np.sum((y_sin_t[:20, 0].numpy() - y_sin_t[:20, 0].numpy().mean()) ** 2)
    r2 = 1 - ss_res / ss_tot
    
    results['siné–¢æ•°_R2'] = r2
    print(f"\n   RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
    
    return results


# ========================================
# 3. å‹•çš„å…¥å‡ºåŠ›ã®æ¤œè¨¼
# ========================================

def test_dynamic_io():
    """å‹•çš„å…¥å‡ºåŠ›ã®æ¤œè¨¼"""
    print("\n" + "=" * 60)
    print("ğŸ”„ å‹•çš„å…¥å‡ºåŠ›ã®æ¤œè¨¼")
    print("=" * 60)
    
    # ç´”ç²‹Pythonç‰ˆ
    print("\nğŸ”¹ ç´”ç²‹Pythonç‰ˆï¼ˆQBNNBrainï¼‰")
    print("-" * 40)
    
    brain = QBNNBrain(num_neurons=50, connection_density=0.15, plasticity=0.1)
    
    inputs = [0.5, -0.3, 0.8, -0.1, 0.6]
    
    # åŒã˜å…¥åŠ›ã§3å›å®Ÿè¡Œ
    io_records = []
    for trial in range(3):
        outputs, in_neurons, out_neurons = brain.forward(
            inputs, 
            output_size=3,
            time_steps=3,
            input_type='visual',
            output_type='motor'
        )
        io_records.append({
            'input_neurons': set(in_neurons),
            'output_neurons': set(out_neurons),
            'outputs': outputs
        })
        print(f"\n   è©¦è¡Œ{trial+1}:")
        print(f"   å…¥åŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: {in_neurons}")
        print(f"   å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: {out_neurons}")
        print(f"   å‡ºåŠ›: {[f'{o:.4f}' for o in outputs]}")
    
    # å…¥å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å¤‰åŒ–ã‚’ç¢ºèª
    in_changes = 0
    out_changes = 0
    for i in range(1, 3):
        if io_records[i]['input_neurons'] != io_records[i-1]['input_neurons']:
            in_changes += 1
        if io_records[i]['output_neurons'] != io_records[i-1]['output_neurons']:
            out_changes += 1
    
    print(f"\n   ğŸ“Š å‹•çš„é¸æŠã®ç¢ºèª:")
    print(f"   å…¥åŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå¤‰åŒ–ã—ãŸå›æ•°: {in_changes}/2")
    print(f"   å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå¤‰åŒ–ã—ãŸå›æ•°: {out_changes}/2")
    
    # PyTorchç‰ˆ
    print("\nğŸ”¹ PyTorchç‰ˆï¼ˆQBNNBrainTorchï¼‰")
    print("-" * 40)
    
    model = QBNNBrainTorch(
        num_neurons=40,
        max_input_size=10,
        max_output_size=5,
        connection_density=0.2
    )
    
    x = torch.tensor([[0.5, -0.3, 0.8, -0.1, 0.6]], dtype=torch.float32)
    
    # åŒã˜å…¥åŠ›ã§3å›å®Ÿè¡Œ
    io_records_torch = []
    for trial in range(3):
        with torch.no_grad():
            output, in_idx, out_idx = model(x, input_size=5, output_size=3, 
                                            time_steps=3, dynamic_selection=True)
        io_records_torch.append({
            'input_neurons': set(in_idx.tolist()),
            'output_neurons': set(out_idx.tolist()),
        })
        print(f"\n   è©¦è¡Œ{trial+1}:")
        print(f"   å…¥åŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: {in_idx.tolist()}")
        print(f"   å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: {out_idx.tolist()}")
    
    in_changes = 0
    out_changes = 0
    for i in range(1, 3):
        if io_records_torch[i]['input_neurons'] != io_records_torch[i-1]['input_neurons']:
            in_changes += 1
        if io_records_torch[i]['output_neurons'] != io_records_torch[i-1]['output_neurons']:
            out_changes += 1
    
    print(f"\n   ğŸ“Š å‹•çš„é¸æŠã®ç¢ºèª:")
    print(f"   å…¥åŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå¤‰åŒ–ã—ãŸå›æ•°: {in_changes}/2")
    print(f"   å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå¤‰åŒ–ã—ãŸå›æ•°: {out_changes}/2")
    
    # é‡å­æƒ…å ±
    info = model.get_quantum_info()
    print(f"\n   âš›ï¸ é‡å­æƒ…å ±:")
    print(f"   Î¸å¹³å‡: {info['theta_mean']:.4f}")
    print(f"   rå¹³å‡ï¼ˆç›¸é–¢ï¼‰: {info['r_mean']:.4f}")
    print(f"   Tå¹³å‡ï¼ˆæ¸©åº¦ï¼‰: {info['T_mean']:.4f}")
    print(f"   Î»ï¼ˆã‚‚ã¤ã‚Œå¼·åº¦ï¼‰: {info['lambda']:.4f}")
    print(f"   æ„Ÿå—æ€§å¹³å‡: {info['sensitivity_mean']:.4f}")
    print(f"   å‡ºåŠ›å‚¾å‘å¹³å‡: {info['output_tendency_mean']:.4f}")


# ========================================
# 4. å¯è¦–åŒ–
# ========================================

def visualize_results(math_results: dict, reg_results: dict):
    """çµæœã®å¯è¦–åŒ–"""
    print("\n" + "=" * 60)
    print("ğŸ“Š çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # æ•°å­¦å•é¡Œ
    ax1 = axes[0]
    ax1.set_title('Math Problem Results', fontsize=12)
    
    labels = ['XOR\nAccuracy', 'Addition\nMAE', 'Multiplication\nMAE']
    values = [
        math_results.get('XOR', 0) * 100,
        (1 - min(math_results.get('åŠ ç®—_MAE', 1), 1)) * 100,
        (1 - min(math_results.get('ä¹—ç®—_MAE', 1), 1)) * 100
    ]
    colors = ['#2ecc71' if v > 50 else '#e74c3c' for v in values]
    
    bars = ax1.bar(labels, values, color=colors, edgecolor='black')
    ax1.set_ylim(0, 100)
    ax1.set_ylabel('Score (%)')
    ax1.axhline(y=50, color='gray', linestyle='--', alpha=0.5)
    
    for bar, val in zip(bars, values):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, 
                f'{val:.1f}%', ha='center', fontsize=10)
    
    # å›å¸°åˆ†æ
    ax2 = axes[1]
    ax2.set_title('Regression RÂ² Scores', fontsize=12)
    
    labels = ['Linear\ny=2x+1', 'Quadratic\ny=xÂ²+0.5', 'Sine\ny=sin(Ï€x)']
    values = [
        max(0, reg_results.get('ç·šå½¢å›å¸°_R2', 0)) * 100,
        max(0, reg_results.get('äºŒæ¬¡é–¢æ•°_R2', 0)) * 100,
        max(0, reg_results.get('siné–¢æ•°_R2', 0)) * 100
    ]
    colors = ['#3498db' if v > 50 else '#e74c3c' for v in values]
    
    bars = ax2.bar(labels, values, color=colors, edgecolor='black')
    ax2.set_ylim(0, 100)
    ax2.set_ylabel('RÂ² Score (%)')
    ax2.axhline(y=50, color='gray', linestyle='--', alpha=0.5)
    
    for bar, val in zip(bars, values):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, 
                f'{val:.1f}%', ha='center', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('/Users/yuyahiguchi/Program/Qubit/qbnn_brain_dynamic_tests.png', 
                dpi=150, bbox_inches='tight')
    print("\nğŸ“Š ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: qbnn_brain_dynamic_tests.png")
    plt.close()
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\n" + "-" * 40)
    print("ğŸ“ æ•°å­¦å•é¡Œ:")
    print(f"   XORæ­£ç­”ç‡: {math_results.get('XOR', 0)*100:.1f}%")
    print(f"   åŠ ç®—MAE: {math_results.get('åŠ ç®—_MAE', 0):.4f}")
    print(f"   ä¹—ç®—MAE: {math_results.get('ä¹—ç®—_MAE', 0):.4f}")
    
    print("\nğŸ“ˆ å›å¸°åˆ†æ:")
    print(f"   ç·šå½¢å›å¸° RÂ²: {reg_results.get('ç·šå½¢å›å¸°_R2', 0):.4f}")
    print(f"   äºŒæ¬¡é–¢æ•° RÂ²: {reg_results.get('äºŒæ¬¡é–¢æ•°_R2', 0):.4f}")
    print(f"   siné–¢æ•° RÂ²: {reg_results.get('siné–¢æ•°_R2', 0):.4f}")


# ========================================
# ãƒ¡ã‚¤ãƒ³
# ========================================

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ§ âš›ï¸ QBNN Brain å‹•çš„å…¥å‡ºåŠ› ç·åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    start_time = time.time()
    
    # å‹•çš„å…¥å‡ºåŠ›ã®æ¤œè¨¼
    test_dynamic_io()
    
    # æ•°å­¦å•é¡Œãƒ†ã‚¹ãƒˆ
    math_results = test_math_problems()
    
    # å›å¸°åˆ†æãƒ†ã‚¹ãƒˆ
    reg_results = test_regression()
    
    # çµæœå¯è¦–åŒ–
    visualize_results(math_results, reg_results)
    
    elapsed = time.time() - start_time
    print(f"\nâ±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {elapsed:.1f}ç§’")
    print("\nâœ… å…¨ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
